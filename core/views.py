from rest_framework import viewsets, status, generics
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework.parsers import MultiPartParser, FormParser
from django.db import transaction
from django.db.models import Q
from django.utils import timezone
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import pandas as pd
import csv
import io
import logging
from datetime import datetime, timedelta
from decimal import Decimal, InvalidOperation
from django.db.models import Sum, Avg, Count, Min, Max
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from django.db.models.manager import Manager
    # Type hints for Django models
    SAPGLPostingManager = Manager
    DataFileManager = Manager
    AnalysisSessionManager = Manager
    TransactionAnalysisManager = Manager

from .models import SAPGLPosting, DataFile, AnalysisSession, TransactionAnalysis, SystemMetrics, GLAccount, FileProcessingJob, MLModelTraining, MLModelProcessingResult, AnalyticsProcessingResult, ProcessingJobTracker
from .serializers import (
    SAPGLPostingSerializer, SAPGLPostingListSerializer,
    DataFileSerializer, DataFileUploadSerializer,
    AnalysisSessionSerializer, AnalysisSessionCreateSerializer,
    TransactionAnalysisSerializer, TransactionAnalysisListSerializer,
    SystemMetricsSerializer, AnalysisRequestSerializer,
    AnalysisSummarySerializer, DataUploadResponseSerializer,
    DashboardStatsSerializer, GLAccountSerializer, GLAccountListSerializer,
    GLAccountAnalysisSerializer, TrialBalanceSerializer, GLAccountChartSerializer,
    GLAccountUploadSerializer, DuplicateAnomalySerializer, DuplicateAnomalyListSerializer,
    DuplicateChartDataSerializer, DuplicateAnalysisRequestSerializer, DuplicateTrainingDataSerializer,
    TargetedAnomalyUploadSerializer, FileProcessingJobSerializer, FileProcessingJobListSerializer,
    TargetedAnomalyResponseSerializer, MLModelTrainingSerializer, MLModelTrainingListSerializer,
    MLModelTrainingRequestSerializer, MLModelInfoSerializer, MLAnomalyResultsSerializer
)
from .analytics import SAPGLAnalyzer
from .ml_models import MLAnomalyDetector
from .enhanced_duplicate_analysis import EnhancedDuplicateAnalyzer
from .analytics_db_saver import get_file_processing_summary

# Import all tasks once at the top to avoid duplicate registrations
from .tasks import (
    process_file_with_anomalies, debug_task, worker_health_check, 
    monitor_worker_performance, train_ml_models, retrain_ml_models,
    _process_file_content, _run_default_analytics, _run_requested_anomalies
)

logger = logging.getLogger(__name__)

class SAPGLPostingViewSet(viewsets.ModelViewSet):
    """ViewSet for SAP GL Posting data"""
    
    queryset = SAPGLPosting.objects.all()
    serializer_class = SAPGLPostingSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return SAPGLPostingListSerializer
        return SAPGLPostingSerializer
    
    def get_queryset(self):
        # type: ignore
        queryset = SAPGLPosting.objects.all()
        
        # Apply filters
        document_number = self.request.query_params.get('document_number', None)
        if document_number:
            queryset = queryset.filter(document_number__icontains=document_number)
        
        document_type = self.request.query_params.get('document_type', None)
        if document_type:
            queryset = queryset.filter(document_type=document_type)
        
        gl_account = self.request.query_params.get('gl_account', None)
        if gl_account:
            queryset = queryset.filter(gl_account__icontains=gl_account)
        
        profit_center = self.request.query_params.get('profit_center', None)
        if profit_center:
            queryset = queryset.filter(profit_center__icontains=profit_center)
        
        user_name = self.request.query_params.get('user_name', None)
        if user_name:
            queryset = queryset.filter(user_name__icontains=user_name)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(posting_date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(posting_date__lte=date_to)
        
        min_amount = self.request.query_params.get('min_amount', None)
        if min_amount:
            queryset = queryset.filter(amount_local_currency__gte=min_amount)
        
        max_amount = self.request.query_params.get('max_amount', None)
        if max_amount:
            queryset = queryset.filter(amount_local_currency__lte=max_amount)
        
        fiscal_year = self.request.query_params.get('fiscal_year', None)
        if fiscal_year:
            queryset = queryset.filter(fiscal_year=fiscal_year)
        
        return queryset
    
    @action(detail=False, methods=['get'])
    def statistics(self, request):
        """Get statistics for SAP GL postings"""
        queryset = self.get_queryset()
        
        stats = {
            'total_transactions': queryset.count(),
            'total_amount': float(queryset.aggregate(total=Sum('amount_local_currency'))['total'] or 0),
            'avg_amount': float(queryset.aggregate(avg=Avg('amount_local_currency'))['avg'] or 0),
            'high_value_transactions': queryset.filter(amount_local_currency__gt=1000000).count(),
            'unique_users': queryset.values('user_name').distinct().count(),
            'unique_accounts': queryset.values('gl_account').distinct().count(),
            'unique_profit_centers': queryset.values('profit_center').distinct().count(),
            'date_range': {
                'min_date': queryset.aggregate(min_date=Min('posting_date'))['min_date'],
                'max_date': queryset.aggregate(max_date=Max('posting_date'))['max_date']
            }
        }
        
        return Response(stats)
    
    @action(detail=False, methods=['get'])
    def top_users(self, request):
        """Get top users by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_users = queryset.values('user_name').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_users))
    
    @action(detail=False, methods=['get'])
    def top_accounts(self, request):
        """Get top G/L accounts by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_accounts = queryset.values('gl_account').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_accounts))

class DataFileViewSet(viewsets.ModelViewSet):
    """ViewSet for uploaded data files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    parser_classes = (MultiPartParser, FormParser)
    
    @action(detail=False, methods=['post'])
    def upload(self, request):
        """Upload and process CSV file"""
        try:
            file_obj = request.FILES.get('file')
            if not file_obj:
                return Response(
                    {'error': 'No file provided'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get additional fields from request data
            engagement_id = request.data.get('engagement_id', '')
            client_name = request.data.get('client_name', '')
            company_name = request.data.get('company_name', '')
            fiscal_year = request.data.get('fiscal_year', 2025)
            audit_start_date = request.data.get('audit_start_date')
            audit_end_date = request.data.get('audit_end_date')
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file in background (for now, process synchronously)
            result = self._process_csv_file(data_file, file_obj)
            
            if result['success']:
                return Response(
                    DataUploadResponseSerializer(data_file).data,
                    status=status.HTTP_201_CREATED
                )
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error uploading file: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_csv_file(self, data_file, file_obj):
        """Process uploaded CSV file"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            
            for row in csv_reader:
                try:
                    # Map CSV columns to model fields
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        
                        # Update date and amount ranges
                        if posting.posting_date:
                            if min_date is None or posting.posting_date < min_date:
                                min_date = posting.posting_date
                            if max_date is None or posting.posting_date > max_date:
                                max_date = posting.posting_date
                        
                        if min_amount is None or posting.amount_local_currency < min_amount:
                            min_amount = posting.amount_local_currency
                        if max_amount is None or posting.amount_local_currency > max_amount:
                            max_amount = posting.amount_local_currency
                except Exception as e:
                    logger.error(f"Error processing row: {e}")
                    failed_count += 1
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            return {'success': True}
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Map CSV columns to model fields
            # This mapping should be adjusted based on actual CSV structure
            posting = SAPGLPosting(
                document_number=row.get('Document', ''),
                document_type=row.get('Document type', ''),
                amount_local_currency=Decimal(row.get('Amount in Local Currency', '0').replace(',', '')),
                local_currency=row.get('Local Currency', 'SAR'),
                gl_account=row.get('G/L Account', ''),
                profit_center=row.get('Profit Center', ''),
                user_name=row.get('User Name', ''),
                fiscal_year=int(row.get('Fiscal Year', '2025')),
                posting_period=int(row.get('Posting period', '1')),
                text=row.get('Text', ''),
                segment=row.get('Segment', ''),
                clearing_document=row.get('Clearing Document', ''),
                offsetting_account=row.get('Offsetting', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                sales_document=row.get('Sales Document', ''),
                assignment=row.get('Assignment', ''),
                year_month=row.get('Year/Month', '')
            )
            
            # Parse dates
            posting_date_str = row.get('Posting Date', '')
            if posting_date_str:
                try:
                    parsed_date = datetime.strptime(posting_date_str, '%m/%d/%Y').date()
                    posting.posting_date = parsed_date
                except:
                    pass
            
            document_date_str = row.get('Document Date', '')
            if document_date_str:
                try:
                    parsed_date = datetime.strptime(document_date_str, '%m/%d/%Y').date()
                    posting.document_date = parsed_date
                except:
                    pass
            
            entry_date_str = row.get('Entry Date', '')
            if entry_date_str:
                try:
                    parsed_date = datetime.strptime(entry_date_str, '%m/%d/%Y').date()
                    posting.entry_date = parsed_date
                except:
                    pass
            
            return posting
            
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None

class AnalysisSessionViewSet(viewsets.ModelViewSet):
    """ViewSet for analysis sessions"""
    
    queryset = AnalysisSession.objects.all()
    serializer_class = AnalysisSessionSerializer
    
    def get_serializer_class(self):
        if self.action == 'create':
            return AnalysisSessionCreateSerializer
        return AnalysisSessionSerializer
    
    @action(detail=True, methods=['post'])
    def run_analysis(self, request, pk=None):
        """Run analysis for a session"""
        try:
            session = self.get_object()
            
            # Check if session is already running or completed
            if session.status in ['RUNNING', 'COMPLETED']:
                return Response(
                    {'error': f'Analysis session is already {session.status.lower()}'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Run analysis
            analyzer = SAPGLAnalyzer()
            result = analyzer.analyze_transactions(session)
            
            if 'error' in result:
                return Response(
                    {'error': result['error']},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"Error running analysis: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['get'])
    def summary(self, request, pk=None):
        """Get analysis summary"""
        try:
            session = self.get_object()
            analyzer = SAPGLAnalyzer()
            summary = analyzer.get_analysis_summary(session)
            
            return Response(AnalysisSummarySerializer(summary).data)
            
        except Exception as e:
            logger.error(f"Error getting summary: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class TransactionAnalysisViewSet(viewsets.ReadOnlyModelViewSet):
    """ViewSet for transaction analysis results"""
    
    queryset = TransactionAnalysis.objects.all()
    serializer_class = TransactionAnalysisSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return TransactionAnalysisListSerializer
        return TransactionAnalysisSerializer
    
    def get_queryset(self):
        queryset = TransactionAnalysis.objects.all()
        
        # Apply filters
        session_id = self.request.query_params.get('session_id', None)
        if session_id:
            queryset = queryset.filter(session_id=session_id)
        
        risk_level = self.request.query_params.get('risk_level', None)
        if risk_level:
            queryset = queryset.filter(risk_level=risk_level)
        
        min_risk_score = self.request.query_params.get('min_risk_score', None)
        if min_risk_score:
            queryset = queryset.filter(risk_score__gte=min_risk_score)
        
        max_risk_score = self.request.query_params.get('max_risk_score', None)
        if max_risk_score:
            queryset = queryset.filter(risk_score__lte=max_risk_score)
        
        # Filter by anomaly types
        amount_anomaly = self.request.query_params.get('amount_anomaly', None)
        if amount_anomaly:
            queryset = queryset.filter(amount_anomaly=amount_anomaly.lower() == 'true')
        
        timing_anomaly = self.request.query_params.get('timing_anomaly', None)
        if timing_anomaly:
            queryset = queryset.filter(timing_anomaly=timing_anomaly.lower() == 'true')
        
        user_anomaly = self.request.query_params.get('user_anomaly', None)
        if user_anomaly:
            queryset = queryset.filter(user_anomaly=user_anomaly.lower() == 'true')
        
        account_anomaly = self.request.query_params.get('account_anomaly', None)
        if account_anomaly:
            queryset = queryset.filter(account_anomaly=account_anomaly.lower() == 'true')
        
        pattern_anomaly = self.request.query_params.get('pattern_anomaly', None)
        if pattern_anomaly:
            queryset = queryset.filter(pattern_anomaly=pattern_anomaly.lower() == 'true')
        
        return queryset.order_by('-risk_score', '-created_at')

class GLAccountViewSet(viewsets.ModelViewSet):
    """ViewSet for GL Account management and analysis"""
    
    queryset = GLAccount.objects.all()
    serializer_class = GLAccountSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return GLAccountListSerializer
        return GLAccountSerializer
    
    def get_queryset(self):
        queryset = GLAccount.objects.all()
        
        # Apply filters
        account_id = self.request.query_params.get('account_id', None)
        if account_id:
            queryset = queryset.filter(account_id__icontains=account_id)
        
        account_name = self.request.query_params.get('account_name', None)
        if account_name:
            queryset = queryset.filter(account_name__icontains=account_name)
        
        account_type = self.request.query_params.get('account_type', None)
        if account_type:
            queryset = queryset.filter(account_type=account_type)
        
        account_category = self.request.query_params.get('account_category', None)
        if account_category:
            queryset = queryset.filter(account_category=account_category)
        
        normal_balance = self.request.query_params.get('normal_balance', None)
        if normal_balance:
            queryset = queryset.filter(normal_balance=normal_balance)
        
        is_active = self.request.query_params.get('is_active', None)
        if is_active:
            queryset = queryset.filter(is_active=is_active.lower() == 'true')
        
        return queryset.order_by('account_id')
    
    @action(detail=False, methods=['get'])
    def analysis(self, request):
        """Get comprehensive analysis for all GL accounts"""
        try:
            accounts = self.get_queryset()
            analysis_results = []
            
            print(f"Debug: Found {accounts.count()} GL accounts")  # Debug log
            
            for account in accounts:
                try:
                    # Get account postings using the gl_account field (not the relationship)
                    postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                    
                    print(f"Debug: Account {account.account_id} has {postings.count()} postings")  # Debug log
                    
                    if not postings.exists():
                        print(f"Debug: Account {account.account_id} has no postings, skipping")  # Debug log
                        continue
                    
                    # Calculate statistics
                    total_debits = postings.filter(transaction_type='DEBIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    total_credits = postings.filter(transaction_type='CREDIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    debit_count = postings.filter(transaction_type='DEBIT').count()
                    credit_count = postings.filter(transaction_type='CREDIT').count()
                    
                    # Risk analysis
                    high_value_count = postings.filter(amount_local_currency__gt=1000000).count()
                    
                    # Get analysis records for this account
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    flagged_count = analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    
                    # Calculate average risk score
                    avg_risk_score = analysis_records.aggregate(
                        avg_score=Avg('risk_score')
                    )['avg_score'] or 0.0
                    
                    # Activity analysis
                    first_transaction = postings.order_by('posting_date').first()
                    last_transaction = postings.order_by('-posting_date').first()
                    
                    avg_amount = postings.aggregate(
                        avg_amount=Avg('amount_local_currency')
                    )['avg_amount'] or Decimal('0.00')
                    
                    max_amount = postings.aggregate(
                        max_amount=Max('amount_local_currency')
                    )['max_amount'] or Decimal('0.00')
                    
                    analysis_results.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'account_type': account.account_type,
                        'account_category': account.account_category,
                        'normal_balance': account.normal_balance,
                        'current_balance': float(account.current_balance),
                        'total_debits': float(total_debits),
                        'total_credits': float(total_credits),
                        'transaction_count': postings.count(),
                        'debit_count': debit_count,
                        'credit_count': credit_count,
                        'high_value_transactions': high_value_count,
                        'flagged_transactions': flagged_count,
                        'risk_score': round(avg_risk_score, 2),
                        'first_transaction_date': first_transaction.posting_date if first_transaction else None,
                        'last_transaction_date': last_transaction.posting_date if last_transaction else None,
                        'avg_transaction_amount': float(avg_amount),
                        'max_transaction_amount': float(max_amount)
                    })
                    
                    print(f"Debug: Added analysis for account {account.account_id}")  # Debug log
                    
                except Exception as account_error:
                    logger.error(f"Error processing account {account.account_id}: {account_error}")
                    print(f"Debug: Error processing account {account.account_id}: {account_error}")  # Debug log
                    continue
            
            print(f"Debug: Total analysis results: {len(analysis_results)}")  # Debug log
            
            # Sort by risk score (highest first)
            analysis_results.sort(key=lambda x: x['risk_score'], reverse=True)
            
            return Response(analysis_results)
            
        except Exception as e:
            logger.error(f"Error in GL account analysis: {e}")
            print(f"Debug: GL account analysis error: {e}")  # Debug log
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def trial_balance(self, request):
        """Generate Trial Balance report"""
        try:
            # Get date range from request
            date_from = request.query_params.get('date_from', None)
            date_to = request.query_params.get('date_to', None)
            
            accounts = self.get_queryset()
            trial_balance_data = []
            
            for account in accounts:
                # Get postings for the account using gl_account field
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                if date_from:
                    postings = postings.filter(posting_date__gte=date_from)
                if date_to:
                    postings = postings.filter(posting_date__lte=date_to)
                
                if not postings.exists():
                    continue
                
                # Calculate period movements
                period_debit = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                period_credit = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                # Calculate closing balances
                if account.normal_balance == 'DEBIT':
                    closing_debit = period_debit - period_credit
                    closing_credit = Decimal('0.00')
                    if closing_debit < 0:
                        closing_credit = abs(closing_debit)
                        closing_debit = Decimal('0.00')
                else:
                    closing_credit = period_credit - period_debit
                    closing_debit = Decimal('0.00')
                    if closing_credit < 0:
                        closing_debit = abs(closing_credit)
                        closing_credit = Decimal('0.00')
                
                # Net balance
                net_balance = closing_debit - closing_credit
                
                trial_balance_data.append({
                    'account_id': account.account_id,
                    'account_name': account.account_name,
                    'account_type': account.account_type,
                    'account_category': account.account_category,
                    'normal_balance': account.normal_balance,
                    'opening_debit': float(Decimal('0.00')),  # Assuming no opening balances for now
                    'opening_credit': float(Decimal('0.00')),
                    'period_debit': float(period_debit),
                    'period_credit': float(period_credit),
                    'closing_debit': float(closing_debit),
                    'closing_credit': float(closing_credit),
                    'net_balance': float(net_balance),
                    'transaction_count': postings.count()
                })
            
            # Sort by account type and account ID
            trial_balance_data.sort(key=lambda x: (x['account_type'], x['account_id']))
            
            return Response(trial_balance_data)
            
        except Exception as e:
            logger.error(f"Error generating trial balance: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def charts(self, request):
        """Get charts data for GL accounts"""
        try:
            accounts = self.get_queryset()
            
            # Account type distribution
            type_distribution = {}
            category_distribution = {}
            
            for account in accounts:
                # Get postings for this account
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                # Type distribution
                if account.account_type not in type_distribution:
                    type_distribution[account.account_type] = {
                        'count': 0,
                        'total_balance': Decimal('0.00'),
                        'total_transactions': 0
                    }
                type_distribution[account.account_type]['count'] += 1
                type_distribution[account.account_type]['total_balance'] += account.current_balance
                type_distribution[account.account_type]['total_transactions'] += postings.count()
                
                # Category distribution
                if account.account_category not in category_distribution:
                    category_distribution[account.account_category] = {
                        'count': 0,
                        'total_balance': Decimal('0.00')
                    }
                category_distribution[account.account_category]['count'] += 1
                category_distribution[account.account_category]['total_balance'] += account.current_balance
            
            # Top accounts by balance
            top_accounts_by_balance = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                if postings.exists():
                    top_accounts_by_balance.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'balance': float(account.current_balance),
                        'account_type': account.account_type
                    })
            
            top_accounts_by_balance.sort(key=lambda x: abs(x['balance']), reverse=True)
            top_accounts_by_balance = top_accounts_by_balance[:10]
            
            # Top accounts by transaction count
            top_accounts_by_transactions = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                transaction_count = postings.count()
                if transaction_count > 0:
                    top_accounts_by_transactions.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'transaction_count': transaction_count,
                        'account_type': account.account_type
                    })
            
            top_accounts_by_transactions.sort(key=lambda x: x['transaction_count'], reverse=True)
            top_accounts_by_transactions = top_accounts_by_transactions[:10]
            
            # Debit vs Credit analysis
            total_debits = Decimal('0.00')
            total_credits = Decimal('0.00')
            debit_count = 0
            credit_count = 0
            
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                account_debits = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                account_credits = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                total_debits += account_debits
                total_credits += account_credits
                debit_count += postings.filter(transaction_type='DEBIT').count()
                credit_count += postings.filter(transaction_type='CREDIT').count()
            
            debit_credit_analysis = {
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'debit_count': debit_count,
                'credit_count': credit_count,
                'net_movement': float(total_debits - total_credits)
            }
            
            # Risk distribution by account type
            risk_distribution = []
            for account_type, data in type_distribution.items():
                accounts_of_type = accounts.filter(account_type=account_type)
                high_risk_count = 0
                total_risk_score = 0
                
                for account in accounts_of_type:
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    high_risk_count += analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    avg_score = analysis_records.aggregate(avg_score=Avg('risk_score'))['avg_score'] or 0
                    total_risk_score += avg_score
                
                avg_risk_score = total_risk_score / len(accounts_of_type) if accounts_of_type.exists() else 0
                
                risk_distribution.append({
                    'account_type': account_type,
                    'account_count': data['count'],
                    'high_risk_count': high_risk_count,
                    'avg_risk_score': round(avg_risk_score, 2)
                })
            
            charts_data = {
                'account_type_distribution': [
                    {
                        'account_type': account_type,
                        'count': data['count'],
                        'total_balance': float(data['total_balance']),
                        'total_transactions': data['total_transactions']
                    }
                    for account_type, data in type_distribution.items()
                ],
                'account_category_distribution': [
                    {
                        'category': category,
                        'count': data['count'],
                        'total_balance': float(data['total_balance'])
                    }
                    for category, data in category_distribution.items()
                ],
                'top_accounts_by_balance': top_accounts_by_balance,
                'top_accounts_by_transactions': top_accounts_by_transactions,
                'debit_credit_analysis': debit_credit_analysis,
                'risk_distribution': risk_distribution
            }
            
            return Response(charts_data)
            
        except Exception as e:
            logger.error(f"Error generating charts data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['post'])
    def upload_master_data(self, request):
        """Upload GL Account master data from CSV"""
        try:
            serializer = GLAccountUploadSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            file_obj = serializer.validated_data['file']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Process CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            created_count = 0
            updated_count = 0
            failed_count = 0
            
            for row in csv_reader:
                try:
                    account_id = row.get('Account ID', '').strip()
                    if not account_id:
                        failed_count += 1
                        continue
                    
                    account_data = {
                        'account_name': row.get('Account Name', '').strip(),
                        'account_type': row.get('Account Type', '').strip(),
                        'account_category': row.get('Account Category', '').strip(),
                        'account_subcategory': row.get('Account Subcategory', '').strip(),
                        'normal_balance': row.get('Normal Balance', 'DEBIT').strip().upper(),
                        'is_active': row.get('Is Active', 'TRUE').strip().upper() == 'TRUE'
                    }
                    
                    # Create or update account
                    account, created = GLAccount.objects.update_or_create(
                        account_id=account_id,
                        defaults=account_data
                    )
                    
                    if created:
                        created_count += 1
                    else:
                        updated_count += 1
                        
                except Exception as e:
                    logger.error(f"Error processing GL account row: {e}")
                    failed_count += 1
            
            return Response({
                'message': 'GL Account master data uploaded successfully',
                'created_count': created_count,
                'updated_count': updated_count,
                'failed_count': failed_count
            })
            
        except Exception as e:
            logger.error(f"Error uploading GL account master data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DashboardView(generics.GenericAPIView):
    """Dashboard view for system overview"""
    
    def get(self, request):
        """Get dashboard statistics"""
        try:
            # Date range (last 30 days)
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=30)
            
            # Overall statistics
            total_transactions = SAPGLPosting.objects.count()
            total_amount = SAPGLPosting.objects.aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0')
            total_files = DataFile.objects.count()
            total_sessions = AnalysisSession.objects.count()
            
            # Recent activity
            recent_transactions = SAPGLPosting.objects.filter(
                created_at__date__gte=start_date
            ).count()
            recent_files = DataFile.objects.filter(
                uploaded_at__date__gte=start_date
            ).count()
            recent_sessions = AnalysisSession.objects.filter(
                created_at__date__gte=start_date
            ).count()
            
            # Risk statistics
            flagged_transactions = TransactionAnalysis.objects.filter(
                risk_level__in=['HIGH', 'CRITICAL']
            ).count()
            high_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='HIGH'
            ).count()
            critical_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='CRITICAL'
            ).count()
            
            # Top statistics
            top_users = SAPGLPosting.objects.values('user_name').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_accounts = SAPGLPosting.objects.values('gl_account').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_profit_centers = SAPGLPosting.objects.values('profit_center').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_document_types = SAPGLPosting.objects.values('document_type').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            stats = {
                'total_transactions': total_transactions,
                'total_amount': float(total_amount),
                'total_files': total_files,
                'total_sessions': total_sessions,
                'recent_transactions': recent_transactions,
                'recent_files': recent_files,
                'recent_sessions': recent_sessions,
                'flagged_transactions': flagged_transactions,
                'high_risk_transactions': high_risk_transactions,
                'critical_risk_transactions': critical_risk_transactions,
                'top_users': list(top_users),
                'top_accounts': list(top_accounts),
                'top_profit_centers': list(top_profit_centers),
                'top_document_types': list(top_document_types),
                'date_from': start_date,
                'date_to': end_date
            }
            
            return Response(DashboardStatsSerializer(stats).data)
            
        except Exception as e:
            logger.error(f"Error getting dashboard stats: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileListView(generics.ListAPIView):
    """View to get list of uploaded files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    
    def get_queryset(self):
        queryset = DataFile.objects.all()
        
        # Apply filters
        status_filter = self.request.query_params.get('status', None)
        if status_filter:
            queryset = queryset.filter(status=status_filter)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(uploaded_at__date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(uploaded_at__date__lte=date_to)
        
        return queryset.order_by('-uploaded_at')
    
    def list(self, request, *args, **kwargs):
        queryset = self.get_queryset()
        serializer = self.get_serializer(queryset, many=True)
        
        # Get file data with session information
        files_data = []
        serializer_data = serializer.data
        
        for i, file_obj in enumerate(queryset):
            file_data = serializer_data[i].copy()  # Make a copy to avoid modifying the original
            
            # Get related analysis sessions for this file
            related_sessions = AnalysisSession.objects.filter(
                created_at__gte=file_obj.uploaded_at
            ).order_by('-created_at')
            
            # Add session information to file data
            file_data['sessions'] = []
            for session in related_sessions:
                session_data = {
                    'id': str(session.id),
                    'session_name': session.session_name,
                    'status': session.status,
                    'created_at': session.created_at,
                    'started_at': session.started_at,
                    'completed_at': session.completed_at,
                    'total_transactions': session.total_transactions,
                    'flagged_transactions': session.flagged_transactions,
                    'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions > 0 else 0
                }
                file_data['sessions'].append(session_data)
            
            files_data.append(file_data)
        
        # Add summary statistics
        total_files = queryset.count()
        total_records = queryset.aggregate(total=Sum('total_records'))['total'] or 0
        total_processed = queryset.aggregate(total=Sum('processed_records'))['total'] or 0
        total_failed = queryset.aggregate(total=Sum('failed_records'))['total'] or 0
        
        # Calculate total sessions across all files
        total_sessions = sum(len(file_data['sessions']) for file_data in files_data)
        
        response_data = {
            'files': files_data,
            'summary': {
                'total_files': total_files,
                'total_records': total_records,
                'total_processed': total_processed,
                'total_failed': total_failed,
                'total_sessions': total_sessions,
                'success_rate': (total_processed / total_records * 100) if total_records > 0 else 0
            }
        }
        
        return Response(response_data)


class FileUploadAnalysisView(generics.CreateAPIView):
    """View to upload file and run analysis"""
    
    parser_classes = (MultiPartParser, FormParser)
    serializer_class = DataFileUploadSerializer
    
    def create(self, request, *args, **kwargs):
        try:
            # Validate the request using serializer
            serializer = self.get_serializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Get the validated data
            file_obj = serializer.validated_data['file']
            description = serializer.validated_data.get('description', '')
            engagement_id = serializer.validated_data['engagement_id']
            client_name = serializer.validated_data['client_name']
            company_name = serializer.validated_data['company_name']
            fiscal_year = serializer.validated_data['fiscal_year']
            audit_start_date = serializer.validated_data['audit_start_date']
            audit_end_date = serializer.validated_data['audit_end_date']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file and run analysis
            result = self._process_and_analyze_file(data_file, file_obj)
            
            if result['success']:
                response_data = {
                        'file': DataFileSerializer(data_file).data,
                        'analysis': result.get('analysis', {}),
                    'processing_summary': result.get('processing_summary', {}),
                        'message': 'File uploaded and analysis completed successfully'
                }
                
                # Add validation errors if any
                if 'validation_errors' in result:
                    response_data['validation_errors'] = result['validation_errors']
                    response_data['validation_errors_summary'] = result.get('validation_errors_summary', '')
                
                return Response(response_data, status=status.HTTP_201_CREATED)
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error in file upload and analysis: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_and_analyze_file(self, data_file, file_obj):
        """Process uploaded CSV file and run analysis"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Keep existing transactions - don't delete them
            logger.info("Processing new file while keeping existing transactions")
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            new_transactions = []
            validation_errors = []
            row_number = 0
            
            for row in csv_reader:
                row_number += 1
                # Map CSV columns to model fields with minimal validation
                posting = self._create_posting_from_row(row)
                if posting:
                    processed_count += 1
                    new_transactions.append(posting)
                    
                    # Update date and amount ranges
                    if posting.posting_date:
                        if min_date is None or posting.posting_date < min_date:
                            min_date = posting.posting_date
                        if max_date is None or posting.posting_date > max_date:
                            max_date = posting.posting_date
                    
                    if min_amount is None or posting.amount_local_currency < min_amount:
                        min_amount = posting.amount_local_currency
                    if max_amount is None or posting.amount_local_currency > max_amount:
                        max_amount = posting.amount_local_currency
                else:
                    failed_count += 1
            
            # Save all transactions in batch
            if new_transactions:
                SAPGLPosting.objects.bulk_create(new_transactions)  # type: ignore
                logger.info(f"Saved {len(new_transactions)} transactions in batch")
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            # Initialize analyzer
            analyzer = SAPGLAnalyzer()
            
            # Run analysis if we have processed transactions
            analysis_result = {}
            if processed_count > 0:
                analysis_result = self._run_file_analysis(analyzer, data_file, new_transactions)
            
            # Prepare response
            response_data = {
                'success': True,
                'analysis': analysis_result,
                'processing_summary': {
                    'total_rows': processed_count + failed_count,
                    'processed_count': processed_count,
                    'failed_count': failed_count,
                    'success_rate': (processed_count / (processed_count + failed_count) * 100) if (processed_count + failed_count) > 0 else 0
                }
            }
            
            # Add validation errors if any
            if validation_errors:
                response_data['validation_errors'] = validation_errors[:10]  # Limit to first 10 errors
                if len(validation_errors) > 10:
                    response_data['validation_errors_summary'] = f"Showing first 10 of {len(validation_errors)} validation errors"
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Define required fields and their display names
            required_fields = {
                'Document Number': row.get('Document Number', '').strip(),
                'G/L Account': row.get('G/L Account', '').strip(),
                'Amount in Local Currency': row.get('Amount in Local Currency', '').strip(),
                'Posting Date': row.get('Posting Date', '').strip(),
                'User Name': row.get('User Name', '').strip()
            }
            
            # Check for missing required fields
            missing_fields = []
            print(required_fields)
            for field_name, field_value in required_fields.items():
                if not field_value:
                    missing_fields.append(field_name)
            
            # If any required fields are missing, raise an error
            if missing_fields:
                error_msg = f"Missing required fields: {', '.join(missing_fields)}"
                logger.error(f"Row validation failed: {error_msg}")
                logger.error(f"Complete row data for debugging: {dict(row)}")
                raise ValueError(error_msg)
            
            # Parse dates with multiple format support
            posting_date = self._parse_date(row.get('Posting Date', ''))
            document_date = self._parse_date(row.get('Document Date', ''))
            entry_date = self._parse_date(row.get('Entry Date', ''))
            
            # Parse amount with proper error handling
            amount_str = row.get('Amount in Local Currency', '0')
            try:
                # Clean the amount string - remove commas, spaces, and currency symbols
                amount_str = str(amount_str).replace(',', '').replace(' ', '').replace('SAR', '').replace('$', '').strip()
                if not amount_str or amount_str == '':
                    raise ValueError("Amount cannot be empty")
                amount_local_currency = Decimal(amount_str)
                if amount_local_currency <= 0:
                    raise ValueError("Amount must be greater than zero")
            except (ValueError, TypeError, InvalidOperation) as e:
                error_msg = f"Invalid amount format: {amount_str}. Error: {str(e)}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Determine transaction type (DEBIT/CREDIT)
            # Check if there's a specific transaction type field
            transaction_type = row.get('Transaction Type', '').strip().upper()
            if not transaction_type:
                # Try to determine from amount (negative = credit, positive = debit)
                # Or use a default based on account type
                if amount_local_currency < 0:
                    transaction_type = 'CREDIT'
                    amount_local_currency = abs(amount_local_currency)  # Make amount positive
                else:
                    transaction_type = 'DEBIT'
            elif transaction_type not in ['DEBIT', 'CREDIT']:
                transaction_type = 'DEBIT'  # Default to debit
            
            # Parse fiscal year and posting period with error handling
            try:
                fiscal_year = int(row.get('Fiscal Year', '0'))
            except (ValueError, TypeError):
                fiscal_year = 2025  # Default value
                
            try:
                posting_period = int(row.get('Posting Period', '0'))
            except (ValueError, TypeError):
                posting_period = 1  # Default value
            
            # Map CSV columns to model fields
            posting = SAPGLPosting(
                document_number=row.get('Document Number', ''),
                posting_date=posting_date,
                gl_account=row.get('G/L Account', ''),
                amount_local_currency=amount_local_currency,
                transaction_type=transaction_type,
                local_currency=row.get('Local Currency', 'SAR'),
                text=row.get('Text', ''),
                document_date=document_date,
                offsetting_account=row.get('Offsetting Account', ''),
                user_name=row.get('User Name', ''),
                entry_date=entry_date,
                document_type=row.get('Document Type', ''),
                profit_center=row.get('Profit Center', ''),
                cost_center=row.get('Cost Center', ''),
                clearing_document=row.get('Clearing Document', ''),
                segment=row.get('Segment', ''),
                wbs_element=row.get('WBS Element', ''),
                plant=row.get('Plant', ''),
                material=row.get('Material', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                billing_document=row.get('Billing Document', ''),
                sales_document=row.get('Sales Document', ''),
                purchasing_document=row.get('Purchasing Document', ''),
                order_number=row.get('Order Number', ''),
                asset_number=row.get('Asset Number', ''),
                network=row.get('Network', ''),
                assignment=row.get('Assignment', ''),
                tax_code=row.get('Tax Code', ''),
                account_assignment=row.get('Account Assignment', ''),
                fiscal_year=fiscal_year,
                posting_period=posting_period,
                year_month=row.get('Year/Month', '')
            )
            return posting
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None
    
    def _parse_date(self, date_str):
        """Parse date string with multiple format support"""
        if not date_str:
            return None
        
        date_formats = [
            '%Y-%m-%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%Y/%m/%d',
            '%m-%d-%Y',
            '%d-%m-%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"Could not parse date: {date_str}")
        return None
    

    
    def _run_file_analysis(self, analyzer, data_file, transactions):
        """Run analysis on the uploaded file data"""
        try:
            # Create analysis session
            session = AnalysisSession.objects.create(
                session_name=f"Analysis for {data_file.file_name}",
                description=f"Automated analysis for uploaded file {data_file.file_name}",
                date_from=data_file.min_date,
                date_to=data_file.max_date,
                min_amount=data_file.min_amount,
                max_amount=data_file.max_amount,
                status='RUNNING'
            )
            
            # Run analysis using the session
            analysis_results = analyzer.analyze_transactions(session)
            
            if 'error' in analysis_results:
                return {
                    'error': analysis_results['error'],
                    'analysis_status': 'FAILED'
                }
            
            # Get analysis summary
            summary = analyzer.get_analysis_summary(session)
            
            return {
                'session_id': str(session.id),
                'total_transactions': summary.get('total_transactions', 0),
                'total_amount': summary.get('total_amount', 0.0),
                'flagged_transactions': summary.get('flagged_transactions', 0),
                'high_value_transactions': summary.get('high_value_transactions', 0),
                'analysis_status': session.status,
                'flag_rate': summary.get('flag_rate', 0.0),
                'anomaly_summary': summary.get('anomaly_summary', {}),
                'risk_distribution': summary.get('risk_distribution', [])
            }
            
        except Exception as e:
            logger.error(f"Error running file analysis: {e}")
            return {
                'error': str(e),
                'analysis_status': 'FAILED'
            }


from rest_framework import viewsets, status, generics
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework.parsers import MultiPartParser, FormParser
from django.db import transaction
from django.db.models import Q
from django.utils import timezone
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import pandas as pd
import csv
import io
import logging
from datetime import datetime, timedelta
from decimal import Decimal, InvalidOperation
from django.db.models import Sum, Avg, Count, Min, Max
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from django.db.models.manager import Manager
    # Type hints for Django models
    SAPGLPostingManager = Manager
    DataFileManager = Manager
    AnalysisSessionManager = Manager
    TransactionAnalysisManager = Manager

from .models import SAPGLPosting, DataFile, AnalysisSession, TransactionAnalysis, SystemMetrics, GLAccount, FileProcessingJob, MLModelTraining
from .serializers import (
    SAPGLPostingSerializer, SAPGLPostingListSerializer,
    DataFileSerializer, DataFileUploadSerializer,
    AnalysisSessionSerializer, AnalysisSessionCreateSerializer,
    TransactionAnalysisSerializer, TransactionAnalysisListSerializer,
    SystemMetricsSerializer, AnalysisRequestSerializer,
    AnalysisSummarySerializer, DataUploadResponseSerializer,
    DashboardStatsSerializer, GLAccountSerializer, GLAccountListSerializer,
    GLAccountAnalysisSerializer, TrialBalanceSerializer, GLAccountChartSerializer,
    GLAccountUploadSerializer, DuplicateAnomalySerializer, DuplicateAnomalyListSerializer,
    DuplicateChartDataSerializer, DuplicateAnalysisRequestSerializer, DuplicateTrainingDataSerializer,
    TargetedAnomalyUploadSerializer, FileProcessingJobSerializer, FileProcessingJobListSerializer,
    TargetedAnomalyResponseSerializer, MLModelTrainingSerializer, MLModelTrainingListSerializer,
    MLModelTrainingRequestSerializer, MLModelInfoSerializer, MLAnomalyResultsSerializer
)
from .analytics import SAPGLAnalyzer
from .ml_models import MLAnomalyDetector

logger = logging.getLogger(__name__)

class SAPGLPostingViewSet(viewsets.ModelViewSet):
    """ViewSet for SAP GL Posting data"""
    
    queryset = SAPGLPosting.objects.all()
    serializer_class = SAPGLPostingSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return SAPGLPostingListSerializer
        return SAPGLPostingSerializer
    
    def get_queryset(self):
        # type: ignore
        queryset = SAPGLPosting.objects.all()
        
        # Apply filters
        document_number = self.request.query_params.get('document_number', None)
        if document_number:
            queryset = queryset.filter(document_number__icontains=document_number)
        
        document_type = self.request.query_params.get('document_type', None)
        if document_type:
            queryset = queryset.filter(document_type=document_type)
        
        gl_account = self.request.query_params.get('gl_account', None)
        if gl_account:
            queryset = queryset.filter(gl_account__icontains=gl_account)
        
        profit_center = self.request.query_params.get('profit_center', None)
        if profit_center:
            queryset = queryset.filter(profit_center__icontains=profit_center)
        
        user_name = self.request.query_params.get('user_name', None)
        if user_name:
            queryset = queryset.filter(user_name__icontains=user_name)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(posting_date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(posting_date__lte=date_to)
        
        min_amount = self.request.query_params.get('min_amount', None)
        if min_amount:
            queryset = queryset.filter(amount_local_currency__gte=min_amount)
        
        max_amount = self.request.query_params.get('max_amount', None)
        if max_amount:
            queryset = queryset.filter(amount_local_currency__lte=max_amount)
        
        fiscal_year = self.request.query_params.get('fiscal_year', None)
        if fiscal_year:
            queryset = queryset.filter(fiscal_year=fiscal_year)
        
        return queryset
    
    @action(detail=False, methods=['get'])
    def statistics(self, request):
        """Get statistics for SAP GL postings"""
        queryset = self.get_queryset()
        
        stats = {
            'total_transactions': queryset.count(),
            'total_amount': float(queryset.aggregate(total=Sum('amount_local_currency'))['total'] or 0),
            'avg_amount': float(queryset.aggregate(avg=Avg('amount_local_currency'))['avg'] or 0),
            'high_value_transactions': queryset.filter(amount_local_currency__gt=1000000).count(),
            'unique_users': queryset.values('user_name').distinct().count(),
            'unique_accounts': queryset.values('gl_account').distinct().count(),
            'unique_profit_centers': queryset.values('profit_center').distinct().count(),
            'date_range': {
                'min_date': queryset.aggregate(min_date=Min('posting_date'))['min_date'],
                'max_date': queryset.aggregate(max_date=Max('posting_date'))['max_date']
            }
        }
        
        return Response(stats)
    
    @action(detail=False, methods=['get'])
    def top_users(self, request):
        """Get top users by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_users = queryset.values('user_name').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_users))
    
    @action(detail=False, methods=['get'])
    def top_accounts(self, request):
        """Get top G/L accounts by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_accounts = queryset.values('gl_account').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_accounts))

class DataFileViewSet(viewsets.ModelViewSet):
    """ViewSet for uploaded data files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    parser_classes = (MultiPartParser, FormParser)
    
    @action(detail=False, methods=['post'])
    def upload(self, request):
        """Upload and process CSV file"""
        try:
            file_obj = request.FILES.get('file')
            if not file_obj:
                return Response(
                    {'error': 'No file provided'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get additional fields from request data
            engagement_id = request.data.get('engagement_id', '')
            client_name = request.data.get('client_name', '')
            company_name = request.data.get('company_name', '')
            fiscal_year = request.data.get('fiscal_year', 2025)
            audit_start_date = request.data.get('audit_start_date')
            audit_end_date = request.data.get('audit_end_date')
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file in background (for now, process synchronously)
            result = self._process_csv_file(data_file, file_obj)
            
            if result['success']:
                return Response(
                    DataUploadResponseSerializer(data_file).data,
                    status=status.HTTP_201_CREATED
                )
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error uploading file: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_csv_file(self, data_file, file_obj):
        """Process uploaded CSV file"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            
            for row in csv_reader:
                try:
                    # Map CSV columns to model fields
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        
                        # Update date and amount ranges
                        if posting.posting_date:
                            if min_date is None or posting.posting_date < min_date:
                                min_date = posting.posting_date
                            if max_date is None or posting.posting_date > max_date:
                                max_date = posting.posting_date
                        
                        if min_amount is None or posting.amount_local_currency < min_amount:
                            min_amount = posting.amount_local_currency
                        if max_amount is None or posting.amount_local_currency > max_amount:
                            max_amount = posting.amount_local_currency
                except Exception as e:
                    logger.error(f"Error processing row: {e}")
                    failed_count += 1
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            return {'success': True}
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Map CSV columns to model fields
            # This mapping should be adjusted based on actual CSV structure
            posting = SAPGLPosting(
                document_number=row.get('Document', ''),
                document_type=row.get('Document type', ''),
                amount_local_currency=Decimal(row.get('Amount in Local Currency', '0').replace(',', '')),
                local_currency=row.get('Local Currency', 'SAR'),
                gl_account=row.get('G/L Account', ''),
                profit_center=row.get('Profit Center', ''),
                user_name=row.get('User Name', ''),
                fiscal_year=int(row.get('Fiscal Year', '2025')),
                posting_period=int(row.get('Posting period', '1')),
                text=row.get('Text', ''),
                segment=row.get('Segment', ''),
                clearing_document=row.get('Clearing Document', ''),
                offsetting_account=row.get('Offsetting', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                sales_document=row.get('Sales Document', ''),
                assignment=row.get('Assignment', ''),
                year_month=row.get('Year/Month', '')
            )
            
            # Parse dates
            posting_date_str = row.get('Posting Date', '')
            if posting_date_str:
                try:
                    parsed_date = datetime.strptime(posting_date_str, '%m/%d/%Y').date()
                    posting.posting_date = parsed_date
                except:
                    pass
            
            document_date_str = row.get('Document Date', '')
            if document_date_str:
                try:
                    parsed_date = datetime.strptime(document_date_str, '%m/%d/%Y').date()
                    posting.document_date = parsed_date
                except:
                    pass
            
            entry_date_str = row.get('Entry Date', '')
            if entry_date_str:
                try:
                    parsed_date = datetime.strptime(entry_date_str, '%m/%d/%Y').date()
                    posting.entry_date = parsed_date
                except:
                    pass
            
            return posting
            
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None

class AnalysisSessionViewSet(viewsets.ModelViewSet):
    """ViewSet for analysis sessions"""
    
    queryset = AnalysisSession.objects.all()
    serializer_class = AnalysisSessionSerializer
    
    def get_serializer_class(self):
        if self.action == 'create':
            return AnalysisSessionCreateSerializer
        return AnalysisSessionSerializer
    
    @action(detail=True, methods=['post'])
    def run_analysis(self, request, pk=None):
        """Run analysis for a session"""
        try:
            session = self.get_object()
            
            # Check if session is already running or completed
            if session.status in ['RUNNING', 'COMPLETED']:
                return Response(
                    {'error': f'Analysis session is already {session.status.lower()}'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Run analysis
            analyzer = SAPGLAnalyzer()
            result = analyzer.analyze_transactions(session)
            
            if 'error' in result:
                return Response(
                    {'error': result['error']},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"Error running analysis: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['get'])
    def summary(self, request, pk=None):
        """Get analysis summary"""
        try:
            session = self.get_object()
            analyzer = SAPGLAnalyzer()
            summary = analyzer.get_analysis_summary(session)
            
            return Response(AnalysisSummarySerializer(summary).data)
            
        except Exception as e:
            logger.error(f"Error getting summary: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class TransactionAnalysisViewSet(viewsets.ReadOnlyModelViewSet):
    """ViewSet for transaction analysis results"""
    
    queryset = TransactionAnalysis.objects.all()
    serializer_class = TransactionAnalysisSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return TransactionAnalysisListSerializer
        return TransactionAnalysisSerializer
    
    def get_queryset(self):
        queryset = TransactionAnalysis.objects.all()
        
        # Apply filters
        session_id = self.request.query_params.get('session_id', None)
        if session_id:
            queryset = queryset.filter(session_id=session_id)
        
        risk_level = self.request.query_params.get('risk_level', None)
        if risk_level:
            queryset = queryset.filter(risk_level=risk_level)
        
        min_risk_score = self.request.query_params.get('min_risk_score', None)
        if min_risk_score:
            queryset = queryset.filter(risk_score__gte=min_risk_score)
        
        max_risk_score = self.request.query_params.get('max_risk_score', None)
        if max_risk_score:
            queryset = queryset.filter(risk_score__lte=max_risk_score)
        
        # Filter by anomaly types
        amount_anomaly = self.request.query_params.get('amount_anomaly', None)
        if amount_anomaly:
            queryset = queryset.filter(amount_anomaly=amount_anomaly.lower() == 'true')
        
        timing_anomaly = self.request.query_params.get('timing_anomaly', None)
        if timing_anomaly:
            queryset = queryset.filter(timing_anomaly=timing_anomaly.lower() == 'true')
        
        user_anomaly = self.request.query_params.get('user_anomaly', None)
        if user_anomaly:
            queryset = queryset.filter(user_anomaly=user_anomaly.lower() == 'true')
        
        account_anomaly = self.request.query_params.get('account_anomaly', None)
        if account_anomaly:
            queryset = queryset.filter(account_anomaly=account_anomaly.lower() == 'true')
        
        pattern_anomaly = self.request.query_params.get('pattern_anomaly', None)
        if pattern_anomaly:
            queryset = queryset.filter(pattern_anomaly=pattern_anomaly.lower() == 'true')
        
        return queryset.order_by('-risk_score', '-created_at')

class GLAccountViewSet(viewsets.ModelViewSet):
    """ViewSet for GL Account management and analysis"""
    
    queryset = GLAccount.objects.all()
    serializer_class = GLAccountSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return GLAccountListSerializer
        return GLAccountSerializer
    
    def get_queryset(self):
        queryset = GLAccount.objects.all()
        
        # Apply filters
        account_id = self.request.query_params.get('account_id', None)
        if account_id:
            queryset = queryset.filter(account_id__icontains=account_id)
        
        account_name = self.request.query_params.get('account_name', None)
        if account_name:
            queryset = queryset.filter(account_name__icontains=account_name)
        
        account_type = self.request.query_params.get('account_type', None)
        if account_type:
            queryset = queryset.filter(account_type=account_type)
        
        account_category = self.request.query_params.get('account_category', None)
        if account_category:
            queryset = queryset.filter(account_category=account_category)
        
        normal_balance = self.request.query_params.get('normal_balance', None)
        if normal_balance:
            queryset = queryset.filter(normal_balance=normal_balance)
        
        is_active = self.request.query_params.get('is_active', None)
        if is_active:
            queryset = queryset.filter(is_active=is_active.lower() == 'true')
        
        return queryset.order_by('account_id')
    
    @action(detail=False, methods=['get'])
    def analysis(self, request):
        """Get comprehensive analysis for all GL accounts"""
        try:
            accounts = self.get_queryset()
            analysis_results = []
            
            print(f"Debug: Found {accounts.count()} GL accounts")  # Debug log
            
            for account in accounts:
                try:
                    # Get account postings using the gl_account field (not the relationship)
                    postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                    
                    print(f"Debug: Account {account.account_id} has {postings.count()} postings")  # Debug log
                    
                    if not postings.exists():
                        print(f"Debug: Account {account.account_id} has no postings, skipping")  # Debug log
                        continue
                    
                    # Calculate statistics
                    total_debits = postings.filter(transaction_type='DEBIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    total_credits = postings.filter(transaction_type='CREDIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    debit_count = postings.filter(transaction_type='DEBIT').count()
                    credit_count = postings.filter(transaction_type='CREDIT').count()
                    
                    # Risk analysis
                    high_value_count = postings.filter(amount_local_currency__gt=1000000).count()
                    
                    # Get analysis records for this account
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    flagged_count = analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    
                    # Calculate average risk score
                    avg_risk_score = analysis_records.aggregate(
                        avg_score=Avg('risk_score')
                    )['avg_score'] or 0.0
                    
                    # Activity analysis
                    first_transaction = postings.order_by('posting_date').first()
                    last_transaction = postings.order_by('-posting_date').first()
                    
                    avg_amount = postings.aggregate(
                        avg_amount=Avg('amount_local_currency')
                    )['avg_amount'] or Decimal('0.00')
                    
                    max_amount = postings.aggregate(
                        max_amount=Max('amount_local_currency')
                    )['max_amount'] or Decimal('0.00')
                    
                    analysis_results.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'account_type': account.account_type,
                        'account_category': account.account_category,
                        'normal_balance': account.normal_balance,
                        'current_balance': float(account.current_balance),
                        'total_debits': float(total_debits),
                        'total_credits': float(total_credits),
                        'transaction_count': postings.count(),
                        'debit_count': debit_count,
                        'credit_count': credit_count,
                        'high_value_transactions': high_value_count,
                        'flagged_transactions': flagged_count,
                        'risk_score': round(avg_risk_score, 2),
                        'first_transaction_date': first_transaction.posting_date if first_transaction else None,
                        'last_transaction_date': last_transaction.posting_date if last_transaction else None,
                        'avg_transaction_amount': float(avg_amount),
                        'max_transaction_amount': float(max_amount)
                    })
                    
                    print(f"Debug: Added analysis for account {account.account_id}")  # Debug log
                    
                except Exception as account_error:
                    logger.error(f"Error processing account {account.account_id}: {account_error}")
                    print(f"Debug: Error processing account {account.account_id}: {account_error}")  # Debug log
                    continue
            
            print(f"Debug: Total analysis results: {len(analysis_results)}")  # Debug log
            
            # Sort by risk score (highest first)
            analysis_results.sort(key=lambda x: x['risk_score'], reverse=True)
            
            return Response(analysis_results)
            
        except Exception as e:
            logger.error(f"Error in GL account analysis: {e}")
            print(f"Debug: GL account analysis error: {e}")  # Debug log
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def trial_balance(self, request):
        """Generate Trial Balance report"""
        try:
            # Get date range from request
            date_from = request.query_params.get('date_from', None)
            date_to = request.query_params.get('date_to', None)
            
            accounts = self.get_queryset()
            trial_balance_data = []
            
            for account in accounts:
                # Get postings for the account using gl_account field
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                if date_from:
                    postings = postings.filter(posting_date__gte=date_from)
                if date_to:
                    postings = postings.filter(posting_date__lte=date_to)
                
                if not postings.exists():
                    continue
                
                # Calculate period movements
                period_debit = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                period_credit = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                # Calculate closing balances
                if account.normal_balance == 'DEBIT':
                    closing_debit = period_debit - period_credit
                    closing_credit = Decimal('0.00')
                    if closing_debit < 0:
                        closing_credit = abs(closing_debit)
                        closing_debit = Decimal('0.00')
                else:
                    closing_credit = period_credit - period_debit
                    closing_debit = Decimal('0.00')
                    if closing_credit < 0:
                        closing_debit = abs(closing_credit)
                        closing_credit = Decimal('0.00')
                
                # Net balance
                net_balance = closing_debit - closing_credit
                
                trial_balance_data.append({
                    'account_id': account.account_id,
                    'account_name': account.account_name,
                    'account_type': account.account_type,
                    'account_category': account.account_category,
                    'normal_balance': account.normal_balance,
                    'opening_debit': float(Decimal('0.00')),  # Assuming no opening balances for now
                    'opening_credit': float(Decimal('0.00')),
                    'period_debit': float(period_debit),
                    'period_credit': float(period_credit),
                    'closing_debit': float(closing_debit),
                    'closing_credit': float(closing_credit),
                    'net_balance': float(net_balance),
                    'transaction_count': postings.count()
                })
            
            # Sort by account type and account ID
            trial_balance_data.sort(key=lambda x: (x['account_type'], x['account_id']))
            
            return Response(trial_balance_data)
            
        except Exception as e:
            logger.error(f"Error generating trial balance: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def charts(self, request):
        """Get charts data for GL accounts"""
        try:
            accounts = self.get_queryset()
            
            # Account type distribution
            type_distribution = {}
            category_distribution = {}
            
            for account in accounts:
                # Get postings for this account
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                # Type distribution
                if account.account_type not in type_distribution:
                    type_distribution[account.account_type] = {
                        'count': 0,
                        'total_balance': Decimal('0.00'),
                        'total_transactions': 0
                    }
                type_distribution[account.account_type]['count'] += 1
                type_distribution[account.account_type]['total_balance'] += account.current_balance
                type_distribution[account.account_type]['total_transactions'] += postings.count()
                
                # Category distribution
                if account.account_category not in category_distribution:
                    category_distribution[account.account_category] = {
                        'count': 0,
                        'total_balance': Decimal('0.00')
                    }
                category_distribution[account.account_category]['count'] += 1
                category_distribution[account.account_category]['total_balance'] += account.current_balance
            
            # Top accounts by balance
            top_accounts_by_balance = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                if postings.exists():
                    top_accounts_by_balance.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'balance': float(account.current_balance),
                        'account_type': account.account_type
                    })
            
            top_accounts_by_balance.sort(key=lambda x: abs(x['balance']), reverse=True)
            top_accounts_by_balance = top_accounts_by_balance[:10]
            
            # Top accounts by transaction count
            top_accounts_by_transactions = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                transaction_count = postings.count()
                if transaction_count > 0:
                    top_accounts_by_transactions.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'transaction_count': transaction_count,
                        'account_type': account.account_type
                    })
            
            top_accounts_by_transactions.sort(key=lambda x: x['transaction_count'], reverse=True)
            top_accounts_by_transactions = top_accounts_by_transactions[:10]
            
            # Debit vs Credit analysis
            total_debits = Decimal('0.00')
            total_credits = Decimal('0.00')
            debit_count = 0
            credit_count = 0
            
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                account_debits = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                account_credits = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                total_debits += account_debits
                total_credits += account_credits
                debit_count += postings.filter(transaction_type='DEBIT').count()
                credit_count += postings.filter(transaction_type='CREDIT').count()
            
            debit_credit_analysis = {
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'debit_count': debit_count,
                'credit_count': credit_count,
                'net_movement': float(total_debits - total_credits)
            }
            
            # Risk distribution by account type
            risk_distribution = []
            for account_type, data in type_distribution.items():
                accounts_of_type = accounts.filter(account_type=account_type)
                high_risk_count = 0
                total_risk_score = 0
                
                for account in accounts_of_type:
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    high_risk_count += analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    avg_score = analysis_records.aggregate(avg_score=Avg('risk_score'))['avg_score'] or 0
                    total_risk_score += avg_score
                
                avg_risk_score = total_risk_score / len(accounts_of_type) if accounts_of_type.exists() else 0
                
                risk_distribution.append({
                    'account_type': account_type,
                    'account_count': data['count'],
                    'high_risk_count': high_risk_count,
                    'avg_risk_score': round(avg_risk_score, 2)
                })
            
            charts_data = {
                'account_type_distribution': [
                    {
                        'account_type': account_type,
                        'count': data['count'],
                        'total_balance': float(data['total_balance']),
                        'total_transactions': data['total_transactions']
                    }
                    for account_type, data in type_distribution.items()
                ],
                'account_category_distribution': [
                    {
                        'category': category,
                        'count': data['count'],
                        'total_balance': float(data['total_balance'])
                    }
                    for category, data in category_distribution.items()
                ],
                'top_accounts_by_balance': top_accounts_by_balance,
                'top_accounts_by_transactions': top_accounts_by_transactions,
                'debit_credit_analysis': debit_credit_analysis,
                'risk_distribution': risk_distribution
            }
            
            return Response(charts_data)
            
        except Exception as e:
            logger.error(f"Error generating charts data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['post'])
    def upload_master_data(self, request):
        """Upload GL Account master data from CSV"""
        try:
            serializer = GLAccountUploadSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            file_obj = serializer.validated_data['file']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Process CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            created_count = 0
            updated_count = 0
            failed_count = 0
            
            for row in csv_reader:
                try:
                    account_id = row.get('Account ID', '').strip()
                    if not account_id:
                        failed_count += 1
                        continue
                    
                    account_data = {
                        'account_name': row.get('Account Name', '').strip(),
                        'account_type': row.get('Account Type', '').strip(),
                        'account_category': row.get('Account Category', '').strip(),
                        'account_subcategory': row.get('Account Subcategory', '').strip(),
                        'normal_balance': row.get('Normal Balance', 'DEBIT').strip().upper(),
                        'is_active': row.get('Is Active', 'TRUE').strip().upper() == 'TRUE'
                    }
                    
                    # Create or update account
                    account, created = GLAccount.objects.update_or_create(
                        account_id=account_id,
                        defaults=account_data
                    )
                    
                    if created:
                        created_count += 1
                    else:
                        updated_count += 1
                        
                except Exception as e:
                    logger.error(f"Error processing GL account row: {e}")
                    failed_count += 1
            
            return Response({
                'message': 'GL Account master data uploaded successfully',
                'created_count': created_count,
                'updated_count': updated_count,
                'failed_count': failed_count
            })
            
        except Exception as e:
            logger.error(f"Error uploading GL account master data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DashboardView(generics.GenericAPIView):
    """Dashboard view for system overview"""
    
    def get(self, request):
        """Get dashboard statistics"""
        try:
            # Date range (last 30 days)
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=30)
            
            # Overall statistics
            total_transactions = SAPGLPosting.objects.count()
            total_amount = SAPGLPosting.objects.aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0')
            total_files = DataFile.objects.count()
            total_sessions = AnalysisSession.objects.count()
            
            # Recent activity
            recent_transactions = SAPGLPosting.objects.filter(
                created_at__date__gte=start_date
            ).count()
            recent_files = DataFile.objects.filter(
                uploaded_at__date__gte=start_date
            ).count()
            recent_sessions = AnalysisSession.objects.filter(
                created_at__date__gte=start_date
            ).count()
            
            # Risk statistics
            flagged_transactions = TransactionAnalysis.objects.filter(
                risk_level__in=['HIGH', 'CRITICAL']
            ).count()
            high_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='HIGH'
            ).count()
            critical_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='CRITICAL'
            ).count()
            
            # Top statistics
            top_users = SAPGLPosting.objects.values('user_name').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_accounts = SAPGLPosting.objects.values('gl_account').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_profit_centers = SAPGLPosting.objects.values('profit_center').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_document_types = SAPGLPosting.objects.values('document_type').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            stats = {
                'total_transactions': total_transactions,
                'total_amount': float(total_amount),
                'total_files': total_files,
                'total_sessions': total_sessions,
                'recent_transactions': recent_transactions,
                'recent_files': recent_files,
                'recent_sessions': recent_sessions,
                'flagged_transactions': flagged_transactions,
                'high_risk_transactions': high_risk_transactions,
                'critical_risk_transactions': critical_risk_transactions,
                'top_users': list(top_users),
                'top_accounts': list(top_accounts),
                'top_profit_centers': list(top_profit_centers),
                'top_document_types': list(top_document_types),
                'date_from': start_date,
                'date_to': end_date
            }
            
            return Response(DashboardStatsSerializer(stats).data)
            
        except Exception as e:
            logger.error(f"Error getting dashboard stats: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileListView(generics.ListAPIView):
    """View to get list of uploaded files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    
    def get_queryset(self):
        queryset = DataFile.objects.all()
        
        # Apply filters
        status_filter = self.request.query_params.get('status', None)
        if status_filter:
            queryset = queryset.filter(status=status_filter)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(uploaded_at__date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(uploaded_at__date__lte=date_to)
        
        return queryset.order_by('-uploaded_at')
    
    def list(self, request, *args, **kwargs):
        queryset = self.get_queryset()
        serializer = self.get_serializer(queryset, many=True)
        
        # Get file data with session information
        files_data = []
        serializer_data = serializer.data
        
        for i, file_obj in enumerate(queryset):
            file_data = serializer_data[i].copy()  # Make a copy to avoid modifying the original
            
            # Get related analysis sessions for this file
            related_sessions = AnalysisSession.objects.filter(
                created_at__gte=file_obj.uploaded_at
            ).order_by('-created_at')
            
            # Add session information to file data
            file_data['sessions'] = []
            for session in related_sessions:
                session_data = {
                    'id': str(session.id),
                    'session_name': session.session_name,
                    'status': session.status,
                    'created_at': session.created_at,
                    'started_at': session.started_at,
                    'completed_at': session.completed_at,
                    'total_transactions': session.total_transactions,
                    'flagged_transactions': session.flagged_transactions,
                    'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions > 0 else 0
                }
                file_data['sessions'].append(session_data)
            
            files_data.append(file_data)
        
        # Add summary statistics
        total_files = queryset.count()
        total_records = queryset.aggregate(total=Sum('total_records'))['total'] or 0
        total_processed = queryset.aggregate(total=Sum('processed_records'))['total'] or 0
        total_failed = queryset.aggregate(total=Sum('failed_records'))['total'] or 0
        
        # Calculate total sessions across all files
        total_sessions = sum(len(file_data['sessions']) for file_data in files_data)
        
        response_data = {
            'files': files_data,
            'summary': {
                'total_files': total_files,
                'total_records': total_records,
                'total_processed': total_processed,
                'total_failed': total_failed,
                'total_sessions': total_sessions,
                'success_rate': (total_processed / total_records * 100) if total_records > 0 else 0
            }
        }
        
        return Response(response_data)


class FileUploadAnalysisView(generics.CreateAPIView):
    """View to upload file and run analysis"""
    
    parser_classes = (MultiPartParser, FormParser)
    serializer_class = DataFileUploadSerializer
    
    def create(self, request, *args, **kwargs):
        try:
            # Validate the request using serializer
            serializer = self.get_serializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Get the validated data
            file_obj = serializer.validated_data['file']
            description = serializer.validated_data.get('description', '')
            engagement_id = serializer.validated_data['engagement_id']
            client_name = serializer.validated_data['client_name']
            company_name = serializer.validated_data['company_name']
            fiscal_year = serializer.validated_data['fiscal_year']
            audit_start_date = serializer.validated_data['audit_start_date']
            audit_end_date = serializer.validated_data['audit_end_date']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file and run analysis
            result = self._process_and_analyze_file(data_file, file_obj)
            
            if result['success']:
                response_data = {
                        'file': DataFileSerializer(data_file).data,
                        'analysis': result.get('analysis', {}),
                    'processing_summary': result.get('processing_summary', {}),
                        'message': 'File uploaded and analysis completed successfully'
                }
                
                # Add validation errors if any
                if 'validation_errors' in result:
                    response_data['validation_errors'] = result['validation_errors']
                    response_data['validation_errors_summary'] = result.get('validation_errors_summary', '')
                
                return Response(response_data, status=status.HTTP_201_CREATED)
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error in file upload and analysis: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_and_analyze_file(self, data_file, file_obj):
        """Process uploaded CSV file and run analysis"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Keep existing transactions - don't delete them
            logger.info("Processing new file while keeping existing transactions")
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            new_transactions = []
            validation_errors = []
            row_number = 0
            
            for row in csv_reader:
                row_number += 1
                try:
                    # Map CSV columns to model fields
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        new_transactions.append(posting)
                        
                        # Update date and amount ranges
                        if posting.posting_date:
                            if min_date is None or posting.posting_date < min_date:
                                min_date = posting.posting_date
                            if max_date is None or posting.posting_date > max_date:
                                max_date = posting.posting_date
                        
                        if min_amount is None or posting.amount_local_currency < min_amount:
                            min_amount = posting.amount_local_currency
                        if max_amount is None or posting.amount_local_currency > max_amount:
                            max_amount = posting.amount_local_currency
                except ValueError as e:
                    # This is a validation error - collect details
                    error_info = {
                        'row_number': row_number,
                        'error': str(e),
                        'complete_row_data': dict(row),  # Include all row data
                        'document_number': row.get('Document Number', 'N/A'),
                        'gl_account': row.get('G/L Account', 'N/A'),
                        'amount': row.get('Amount in Local Currency', 'N/A'),
                        'user_name': row.get('User Name', 'N/A')
                    }
                    validation_errors.append(error_info)
                    logger.error(f"Validation error in row {row_number}: {e}")
                    logger.error(f"Complete row data: {dict(row)}")
                    failed_count += 1
                except Exception as e:
                    # This is a general processing error
                    error_info = {
                        'row_number': row_number,
                        'error': f"Processing error: {str(e)}",
                        'complete_row_data': dict(row),  # Include all row data
                        'document_number': row.get('Document Number', 'N/A'),
                        'gl_account': row.get('G/L Account', 'N/A'),
                        'amount': row.get('Amount in Local Currency', 'N/A'),
                        'user_name': row.get('User Name', 'N/A')
                    }
                    validation_errors.append(error_info)
                    logger.error(f"Error processing row {row_number}: {e}")
                    logger.error(f"Complete row data: {dict(row)}")
                    failed_count += 1
            
            # Save all transactions in batch
            if new_transactions:
                SAPGLPosting.objects.bulk_create(new_transactions)  # type: ignore
                logger.info(f"Saved {len(new_transactions)} transactions in batch")
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            # Initialize analyzer
            analyzer = SAPGLAnalyzer()
            
            # Run analysis if we have processed transactions
            analysis_result = {}
            if processed_count > 0:
                analysis_result = self._run_file_analysis(analyzer, data_file, new_transactions)
            
            # Prepare response
            response_data = {
                'success': True,
                'analysis': analysis_result,
                'processing_summary': {
                    'total_rows': processed_count + failed_count,
                    'processed_count': processed_count,
                    'failed_count': failed_count,
                    'success_rate': (processed_count / (processed_count + failed_count) * 100) if (processed_count + failed_count) > 0 else 0
                }
            }
            
            # Add validation errors if any
            if validation_errors:
                response_data['validation_errors'] = validation_errors[:10]  # Limit to first 10 errors
                if len(validation_errors) > 10:
                    response_data['validation_errors_summary'] = f"Showing first 10 of {len(validation_errors)} validation errors"
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Define required fields and their display names
            required_fields = {
                'Document Number': row.get('Document Number', '').strip(),
                'G/L Account': row.get('G/L Account', '').strip(),
                'Amount in Local Currency': row.get('Amount in Local Currency', '').strip(),
                'Posting Date': row.get('Posting Date', '').strip(),
                'User Name': row.get('User Name', '').strip()
            }
            
            # Check for missing required fields
            missing_fields = []
            print(required_fields)
            for field_name, field_value in required_fields.items():
                if not field_value:
                    missing_fields.append(field_name)
            
            # If any required fields are missing, raise an error
            if missing_fields:
                error_msg = f"Missing required fields: {', '.join(missing_fields)}"
                logger.error(f"Row validation failed: {error_msg}")
                logger.error(f"Complete row data for debugging: {dict(row)}")
                raise ValueError(error_msg)
            
            # Parse dates with multiple format support
            posting_date = self._parse_date(row.get('Posting Date', ''))
            document_date = self._parse_date(row.get('Document Date', ''))
            entry_date = self._parse_date(row.get('Entry Date', ''))
            
            # Parse amount with proper error handling
            amount_str = row.get('Amount in Local Currency', '0')
            try:
                # Clean the amount string - remove commas, spaces, and currency symbols
                amount_str = str(amount_str).replace(',', '').replace(' ', '').replace('SAR', '').replace('$', '').strip()
                if not amount_str or amount_str == '':
                    raise ValueError("Amount cannot be empty")
                amount_local_currency = Decimal(amount_str)
                if amount_local_currency <= 0:
                    raise ValueError("Amount must be greater than zero")
            except (ValueError, TypeError, InvalidOperation) as e:
                error_msg = f"Invalid amount format: {amount_str}. Error: {str(e)}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Determine transaction type (DEBIT/CREDIT)
            # Check if there's a specific transaction type field
            transaction_type = row.get('Transaction Type', '').strip().upper()
            if not transaction_type:
                # Try to determine from amount (negative = credit, positive = debit)
                # Or use a default based on account type
                if amount_local_currency < 0:
                    transaction_type = 'CREDIT'
                    amount_local_currency = abs(amount_local_currency)  # Make amount positive
                else:
                    transaction_type = 'DEBIT'
            elif transaction_type not in ['DEBIT', 'CREDIT']:
                transaction_type = 'DEBIT'  # Default to debit
            
            # Parse fiscal year and posting period with error handling
            try:
                fiscal_year = int(row.get('Fiscal Year', '0'))
            except (ValueError, TypeError):
                fiscal_year = 2025  # Default value
                
            try:
                posting_period = int(row.get('Posting Period', '0'))
            except (ValueError, TypeError):
                posting_period = 1  # Default value
            
            # Map CSV columns to model fields
            posting = SAPGLPosting(
                document_number=row.get('Document Number', ''),
                posting_date=posting_date,
                gl_account=row.get('G/L Account', ''),
                amount_local_currency=amount_local_currency,
                transaction_type=transaction_type,
                local_currency=row.get('Local Currency', 'SAR'),
                text=row.get('Text', ''),
                document_date=document_date,
                offsetting_account=row.get('Offsetting Account', ''),
                user_name=row.get('User Name', ''),
                entry_date=entry_date,
                document_type=row.get('Document Type', ''),
                profit_center=row.get('Profit Center', ''),
                cost_center=row.get('Cost Center', ''),
                clearing_document=row.get('Clearing Document', ''),
                segment=row.get('Segment', ''),
                wbs_element=row.get('WBS Element', ''),
                plant=row.get('Plant', ''),
                material=row.get('Material', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                billing_document=row.get('Billing Document', ''),
                sales_document=row.get('Sales Document', ''),
                purchasing_document=row.get('Purchasing Document', ''),
                order_number=row.get('Order Number', ''),
                asset_number=row.get('Asset Number', ''),
                network=row.get('Network', ''),
                assignment=row.get('Assignment', ''),
                tax_code=row.get('Tax Code', ''),
                account_assignment=row.get('Account Assignment', ''),
                fiscal_year=fiscal_year,
                posting_period=posting_period,
                year_month=row.get('Year/Month', '')
            )
            return posting
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None
    
    def _parse_date(self, date_str):
        """Parse date string with multiple format support"""
        if not date_str:
            return None
        
        date_formats = [
            '%Y-%m-%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%Y/%m/%d',
            '%m-%d-%Y',
            '%d-%m-%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"Could not parse date: {date_str}")
        return None
    

    
    def _run_file_analysis(self, analyzer, data_file, transactions):
        """Run analysis on the uploaded file data"""
        try:
            # Create analysis session
            session = AnalysisSession.objects.create(
                session_name=f"Analysis for {data_file.file_name}",
                description=f"Automated analysis for uploaded file {data_file.file_name}",
                date_from=data_file.min_date,
                date_to=data_file.max_date,
                min_amount=data_file.min_amount,
                max_amount=data_file.max_amount,
                status='RUNNING'
            )
            
            # Run analysis using the session
            analysis_results = analyzer.analyze_transactions(session)
            
            if 'error' in analysis_results:
                return {
                    'error': analysis_results['error'],
                    'analysis_status': 'FAILED'
                }
            
            # Get analysis summary
            summary = analyzer.get_analysis_summary(session)
            
            return {
                'session_id': str(session.id),
                'total_transactions': summary.get('total_transactions', 0),
                'total_amount': summary.get('total_amount', 0.0),
                'flagged_transactions': summary.get('flagged_transactions', 0),
                'high_value_transactions': summary.get('high_value_transactions', 0),
                'analysis_status': session.status,
                'flag_rate': summary.get('flag_rate', 0.0),
                'anomaly_summary': summary.get('anomaly_summary', {}),
                'risk_distribution': summary.get('risk_distribution', [])
            }
            
        except Exception as e:
            logger.error(f"Error running file analysis: {e}")
            return {
                'error': str(e),
                'analysis_status': 'FAILED'
            }


class FileSummaryView(generics.RetrieveAPIView):
    """View to get summary by file ID"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    lookup_field = 'id'
    
    def retrieve(self, request, *args, **kwargs):
        data_file = self.get_object()
        serializer = self.get_serializer(data_file)
        
        # Get related data - get all transactions first for statistics
        all_related_transactions = SAPGLPosting.objects.filter(
            created_at__gte=data_file.uploaded_at
        )
        
        # Get limited transactions for display
        related_transactions = all_related_transactions.order_by('-created_at')[:100]
        
        # Calculate additional statistics using the full dataset
        total_amount = all_related_transactions.aggregate(
            total=Sum('amount_local_currency')
        )['total'] or Decimal('0.00')
        
        # Get currency information from transactions
        currency_info = all_related_transactions.values('local_currency').distinct()
        primary_currency = currency_info.first()['local_currency'] if currency_info.exists() else 'SAR'
        
        unique_users = all_related_transactions.values('user_name').distinct().count()
        unique_accounts = all_related_transactions.values('gl_account').distinct().count()
        unique_profit_centers = all_related_transactions.values('profit_center').distinct().count()
        
        # Get analysis sessions for this file
        analysis_sessions = AnalysisSession.objects.filter(
            created_at__gte=data_file.uploaded_at
        ).order_by('-created_at')
        
        # Get transaction analysis records for this file's transactions
        transaction_analyses = TransactionAnalysis.objects.filter(
            transaction__in=all_related_transactions
        )
        
        # If no analysis records exist, run real-time anomaly detection
        if not transaction_analyses.exists():
            # Run real-time anomaly detection
            analyzer = SAPGLAnalyzer()
            transactions_list = list(all_related_transactions)
            
            # Run all anomaly tests
            duplicate_anomalies = analyzer.detect_duplicate_entries(transactions_list)
            user_anomalies = analyzer.detect_user_anomalies(transactions_list)
            backdated_anomalies = analyzer.detect_backdated_entries(transactions_list)
            closing_anomalies = analyzer.detect_closing_entries(transactions_list)
            unusual_day_anomalies = analyzer.detect_unusual_days(transactions_list)
            holiday_anomalies = analyzer.detect_holiday_entries(transactions_list)
            
            # Count anomalies by type - only the 6 specific types we're implementing
            duplicate_count = 0
            user_anomaly_count = 0
            backdated_count = 0
            closing_count = 0
            unusual_days_count = 0
            holiday_count = 0
            
            # 1) Count Duplicate Entries
            if duplicate_anomalies:
                duplicate_transaction_ids = set()
                for dup in duplicate_anomalies:
                    for transaction in dup.get('transactions', []):
                        duplicate_transaction_ids.add(transaction['id'])
                duplicate_count = len(duplicate_transaction_ids)
            
            # 2) Count User Analysis anomalies
            if user_anomalies:
                user_anomaly_users = set()
                for anomaly in user_anomalies:
                    user_anomaly_users.add(anomaly.get('user_name'))
                user_anomaly_count = len(user_anomaly_users)
            
            # 3) Count Backdated Entries
            if backdated_anomalies:
                backdated_transaction_ids = set()
                for anomaly in backdated_anomalies:
                    backdated_transaction_ids.add(anomaly.get('transaction_id'))
                backdated_count = len(backdated_transaction_ids)
            
            # 4) Count Closing Entries
            if closing_anomalies:
                closing_transaction_ids = set()
                for anomaly in closing_anomalies:
                    closing_transaction_ids.add(anomaly.get('transaction_id'))
                closing_count = len(closing_transaction_ids)
            
            # 5) Count Unusual Days
            if unusual_day_anomalies:
                unusual_days_transaction_ids = set()
                for anomaly in unusual_day_anomalies:
                    unusual_days_transaction_ids.add(anomaly.get('transaction_id'))
                unusual_days_count = len(unusual_days_transaction_ids)
            
            # 6) Count Holiday Entries
            if holiday_anomalies:
                holiday_transaction_ids = set()
                for anomaly in holiday_anomalies:
                    holiday_transaction_ids.add(anomaly.get('transaction_id'))
                holiday_count = len(holiday_transaction_ids)
            
            # Calculate total anomalies (sum of all 6 types)
            total_anomalies = duplicate_count + user_anomaly_count + backdated_count + closing_count + unusual_days_count + holiday_count
            
            # Calculate total anomalies
            total_anomalies = amount_anomaly_count + timing_anomaly_count + user_anomaly_count + account_anomaly_count + pattern_anomaly_count
            
            # Create risk distribution based on anomaly counts
            total_transactions = all_related_transactions.count()
            risk_distribution = [
                {
                    'risk_level': 'LOW',
                    'count': total_transactions - total_anomalies,
                    'percentage': round(((total_transactions - total_anomalies) / total_transactions * 100), 2) if total_transactions > 0 else 0
                },
                {
                    'risk_level': 'MEDIUM',
                    'count': 0,
                    'percentage': 0
                },
                {
                    'risk_level': 'HIGH',
                    'count': total_anomalies,
                    'percentage': round((total_anomalies / total_transactions * 100), 2) if total_transactions > 0 else 0
                },
                {
                    'risk_level': 'CRITICAL',
                    'count': 0,
                    'percentage': 0
                }
            ]
            
            anomaly_summary = {
                'duplicate_entries': duplicate_count,
                'user_anomalies': user_anomaly_count,
                'backdated_entries': backdated_count,
                'closing_entries': closing_count,
                'unusual_days': unusual_days_count,
                'holiday_entries': holiday_count,
                'total_anomalies': total_anomalies
            }
            
        else:
            # Use existing analysis records
            # Calculate risk distribution
            risk_distribution = [
                {
                    'risk_level': 'LOW',
                    'count': transaction_analyses.filter(risk_level='LOW').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='LOW').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                },
                {
                    'risk_level': 'MEDIUM',
                    'count': transaction_analyses.filter(risk_level='MEDIUM').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='MEDIUM').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                },
                {
                    'risk_level': 'HIGH',
                    'count': transaction_analyses.filter(risk_level='HIGH').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='HIGH').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                },
                {
                    'risk_level': 'CRITICAL',
                    'count': transaction_analyses.filter(risk_level='CRITICAL').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='CRITICAL').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                }
            ]
            
            # Calculate anomaly summary - use same structure as real-time detection
            # For existing analysis records, we need to run real-time detection to get accurate counts
            # since the boolean flags don't map directly to our 6 specific types
            
            # Run real-time anomaly detection to get accurate counts
            analyzer = SAPGLAnalyzer()
            transactions_list = list(all_related_transactions)
            
            # Run all anomaly tests
            duplicate_anomalies = analyzer.detect_duplicate_entries(transactions_list)
            user_anomalies = analyzer.detect_user_anomalies(transactions_list)
            backdated_anomalies = analyzer.detect_backdated_entries(transactions_list)
            closing_anomalies = analyzer.detect_closing_entries(transactions_list)
            unusual_day_anomalies = analyzer.detect_unusual_days(transactions_list)
            holiday_anomalies = analyzer.detect_holiday_entries(transactions_list)
            
            # Count anomalies by type - only the 6 specific types we're implementing
            duplicate_count = 0
            user_anomaly_count = 0
            backdated_count = 0
            closing_count = 0
            unusual_days_count = 0
            holiday_count = 0
            
            # 1) Count Duplicate Entries
            if duplicate_anomalies:
                duplicate_transaction_ids = set()
                for dup in duplicate_anomalies:
                    for transaction in dup.get('transactions', []):
                        duplicate_transaction_ids.add(transaction['id'])
                duplicate_count = len(duplicate_transaction_ids)
            
            # 2) Count User Analysis anomalies
            if user_anomalies:
                user_anomaly_users = set()
                for anomaly in user_anomalies:
                    user_anomaly_users.add(anomaly.get('user_name'))
                user_anomaly_count = len(user_anomaly_users)
            
            # 3) Count Backdated Entries
            if backdated_anomalies:
                backdated_transaction_ids = set()
                for anomaly in backdated_anomalies:
                    backdated_transaction_ids.add(anomaly.get('transaction_id'))
                backdated_count = len(backdated_transaction_ids)
            
            # 4) Count Closing Entries
            if closing_anomalies:
                closing_transaction_ids = set()
                for anomaly in closing_anomalies:
                    closing_transaction_ids.add(anomaly.get('transaction_id'))
                closing_count = len(closing_transaction_ids)
            
            # 5) Count Unusual Days
            if unusual_day_anomalies:
                unusual_days_transaction_ids = set()
                for anomaly in unusual_day_anomalies:
                    unusual_days_transaction_ids.add(anomaly.get('transaction_id'))
                unusual_days_count = len(unusual_days_transaction_ids)
            
            # 6) Count Holiday Entries
            if holiday_anomalies:
                holiday_transaction_ids = set()
                for anomaly in holiday_anomalies:
                    holiday_transaction_ids.add(anomaly.get('transaction_id'))
                holiday_count = len(holiday_transaction_ids)
            
            # Calculate total anomalies (sum of all 6 types)
            total_anomalies = duplicate_count + user_anomaly_count + backdated_count + closing_count + unusual_days_count + holiday_count
            
            anomaly_summary = {
                'duplicate_entries': duplicate_count,
                'user_anomalies': user_anomaly_count,
                'backdated_entries': backdated_count,
                'closing_entries': closing_count,
                'unusual_days': unusual_days_count,
                'holiday_entries': holiday_count,
                'total_anomalies': total_anomalies
            }
        
        # Get detailed anomaly data
        duplicate_entries = []
        user_anomalies = []
        backdated_entries = []
        closing_entries = []
        unusual_days = []
        holiday_entries = []
        
        # Use the anomaly detection results from above if available, otherwise run detection
        if not transaction_analyses.exists():
            # Use the results from real-time detection above
            duplicate_entries = duplicate_anomalies[:10]  # Limit to top 10
            
            # Convert user anomalies to the expected format
            for anomaly in user_anomalies:
                user_anomalies.append({
                    'user_name': anomaly.get('user_name'),
                    'anomaly_count': anomaly.get('statistics', {}).get('transaction_count', 0),
                    'avg_risk_score': anomaly.get('risk_score', 0)
                })
            
            # Convert timing anomalies to the expected format
            for anomaly in backdated_anomalies:
                backdated_entries.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
            
            for anomaly in closing_anomalies:
                closing_entries.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
            
            for anomaly in unusual_day_anomalies:
                unusual_days.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
            
            for anomaly in holiday_anomalies:
                holiday_entries.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
        else:
            # Use existing analysis records for detailed data
            if all_related_transactions.exists():
                analyzer = SAPGLAnalyzer()
                duplicates = analyzer.detect_duplicate_entries(list(all_related_transactions))
                duplicate_entries = duplicates[:10]  # Limit to top 10
            
            # Get user anomalies (transactions by users with unusual patterns)
            high_risk_users = transaction_analyses.filter(
                user_anomaly=True, risk_level__in=['HIGH', 'CRITICAL']
            ).values('transaction__user_name').annotate(
                count=Count('id'),
                avg_risk_score=Avg('risk_score')
            ).order_by('-avg_risk_score')[:10]
            
            for user_data in high_risk_users:
                user_anomalies.append({
                    'user_name': user_data['transaction__user_name'],
                    'anomaly_count': user_data['count'],
                    'avg_risk_score': round(user_data['avg_risk_score'], 2)
                })
            
            # Get backdated entries (transactions posted on weekends/holidays)
            backdated_entries = all_related_transactions.filter(
                posting_date__week_day__in=[1, 7]  # Monday=1, Sunday=7
            ).values('document_number', 'posting_date', 'amount_local_currency', 'local_currency', 'user_name')[:10]
            
            # Get closing entries (transactions at month-end)
            closing_entries = all_related_transactions.filter(
                posting_date__day__gte=25  # Last week of month
            ).values('document_number', 'posting_date', 'amount_local_currency', 'local_currency', 'user_name')[:10]
        
        # Generate charts data
        charts_data = {
            'monthly_transaction_volume': self._generate_monthly_transaction_volume(all_related_transactions),
            'risk_distribution_chart': {
                'labels': [item['risk_level'] for item in risk_distribution],
                'data': [item['count'] for item in risk_distribution],
                'percentages': [item['percentage'] for item in risk_distribution]
            },
            'top_users_by_amount': self._generate_top_users_by_amount(all_related_transactions),
            'top_accounts_by_transactions': self._generate_top_accounts_by_transactions(all_related_transactions),
            'anomaly_breakdown': {
                'labels': ['Duplicate Entries', 'User Anomalies', 'Backdated Entries', 'Closing Entries', 'Unusual Days', 'Holiday Entries'],
                'data': [
                    anomaly_summary['duplicate_entries'],
                    anomaly_summary['user_anomalies'],
                    anomaly_summary['backdated_entries'],
                    anomaly_summary['closing_entries'],
                    anomaly_summary['unusual_days'],
                    anomaly_summary['holiday_entries']
                ]
            }
        }
        
        # Calculate flag rate
        flagged_transactions = transaction_analyses.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
        flag_rate = round((flagged_transactions / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
        
        # Prepare statistics-only response
        response_data = {
            'file_info': {
                'id': str(data_file.id),
                'file_name': data_file.file_name,
                'status': data_file.status,
                'uploaded_at': data_file.uploaded_at,
                'processed_at': data_file.processed_at,
                'total_records': data_file.total_records,
                'processed_records': data_file.processed_records,
                'failed_records': data_file.failed_records,
                'currency': primary_currency
            },
            'summary_statistics': {
                'total_transactions': all_related_transactions.count(),
                'total_amount': float(total_amount),
                'currency': primary_currency,
                'flagged_transactions': flagged_transactions,
                'high_value_transactions': all_related_transactions.filter(amount_local_currency__gt=1000000).count(),
                'flag_rate': flag_rate,
                'unique_users': unique_users,
                'unique_accounts': unique_accounts,
                'unique_profit_centers': unique_profit_centers,
                'avg_amount': float(total_amount / all_related_transactions.count()) if all_related_transactions.count() > 0 else 0,
                'min_amount': float(all_related_transactions.aggregate(Min('amount_local_currency'))['amount_local_currency__min'] or 0),
                'max_amount': float(all_related_transactions.aggregate(Max('amount_local_currency'))['amount_local_currency__max'] or 0),
                'date_range': {
                    'start_date': all_related_transactions.aggregate(Min('posting_date'))['posting_date__min'],
                    'end_date': all_related_transactions.aggregate(Max('posting_date'))['posting_date__max']
                }
            },
            'risk_distribution': risk_distribution,
            'anomaly_summary': anomaly_summary,
            'charts_data': {
                'risk_distribution_chart': charts_data['risk_distribution_chart'],
                'anomaly_breakdown': charts_data['anomaly_breakdown'],
                'top_users_by_amount': charts_data['top_users_by_amount'][:5],  # Limit to top 5
                'top_accounts_by_transactions': charts_data['top_accounts_by_transactions'][:5],  # Limit to top 5
                'monthly_transaction_volume': charts_data['monthly_transaction_volume']
            },
            'analysis_sessions_summary': {
                'total_sessions': analysis_sessions.count(),
                'latest_session': {
                    'id': str(analysis_sessions.first().id),
                    'session_name': analysis_sessions.first().session_name,
                    'status': analysis_sessions.first().status,
                    'created_at': analysis_sessions.first().created_at
                } if analysis_sessions.exists() else None
            },
            'gl_charts_data': self._generate_gl_charts_data(all_related_transactions),
            'gl_account_summary': self._generate_gl_account_summary(all_related_transactions)
        }
        
        return Response(response_data)
    
    def _generate_monthly_transaction_volume(self, transactions):
        """Generate monthly transaction volume data"""
        monthly_data = {}
        
        # Get the primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        for transaction in transactions:
            if transaction.posting_date:
                month_key = transaction.posting_date.strftime('%Y-%m')
                if month_key not in monthly_data:
                    monthly_data[month_key] = {
                        'month': month_key,
                        'transaction_count': 0,
                        'total_amount': Decimal('0.00'),
                        'debit_amount': Decimal('0.00'),
                        'credit_amount': Decimal('0.00')
                    }
                
                monthly_data[month_key]['transaction_count'] += 1
                monthly_data[month_key]['total_amount'] += transaction.amount_local_currency
                
                if transaction.transaction_type == 'DEBIT':
                    monthly_data[month_key]['debit_amount'] += transaction.amount_local_currency
                else:
                    monthly_data[month_key]['credit_amount'] += transaction.amount_local_currency
        
        return [
            {
                'month': data['month'],
                'transaction_count': data['transaction_count'],
                'total_amount': float(data['total_amount']),
                'debit_amount': float(data['debit_amount']),
                'credit_amount': float(data['credit_amount']),
                'currency': currency
            }
            for data in sorted(monthly_data.values(), key=lambda x: x['month'])
        ]
    
    def _generate_top_users_by_amount(self, transactions):
        """Generate top users by transaction amount"""
        user_stats = transactions.values('user_name', 'local_currency').annotate(
            total_amount=Sum('amount_local_currency'),
            transaction_count=Count('id'),
            avg_amount=Avg('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return [
            {
                'user_name': stat['user_name'],
                'total_amount': float(stat['total_amount']),
                'currency': stat['local_currency'],
                'transaction_count': stat['transaction_count'],
                'avg_amount': float(stat['avg_amount'])
            }
            for stat in user_stats
        ]
    
    def _generate_top_accounts_by_transactions(self, transactions):
        """Generate top accounts by transaction count"""
        account_stats = transactions.values('gl_account', 'local_currency').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency'),
            avg_amount=Avg('amount_local_currency')
        ).order_by('-transaction_count')[:10]
        
        return [
            {
                'gl_account': stat['gl_account'],
                'transaction_count': stat['transaction_count'],
                'total_amount': float(stat['total_amount']),
                'currency': stat['local_currency'],
                'avg_amount': float(stat['avg_amount'])
            }
            for stat in account_stats
        ]
    
    def _generate_gl_account_data(self, transactions):
        """Generate GL account data including Trial Balance and Trading Equity"""
        # Get unique GL accounts from transactions
        gl_accounts = transactions.values('gl_account').distinct()
        
        gl_account_data = []
        
        for account_data in gl_accounts:
            account_id = account_data['gl_account']
            
            # Get transactions for this account
            account_transactions = transactions.filter(gl_account=account_id)
            
            # Calculate Trial Balance (TB) - Debits minus Credits
            total_debits = account_transactions.filter(transaction_type='DEBIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            total_credits = account_transactions.filter(transaction_type='CREDIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            # Trial Balance = Debits - Credits
            trial_balance = total_debits - total_credits
            
            # Calculate Trading Equity (TE) - Net change in equity accounts
            # For equity accounts (typically 3xxxx series), TE = Credits - Debits
            # For other accounts, TE = Debits - Credits (same as TB)
            if account_id.startswith('3'):  # Equity accounts
                trading_equity = total_credits - total_debits
            else:
                trading_equity = trial_balance
            
            # Get account details from GLAccount model if available
            try:
                gl_account_obj = GLAccount.objects.get(account_id=account_id)
                account_name = gl_account_obj.account_name
                account_type = gl_account_obj.account_type
                account_category = gl_account_obj.account_category
                normal_balance = gl_account_obj.normal_balance
            except GLAccount.DoesNotExist:
                account_name = f"Account {account_id}"
                account_type = "Unknown"
                account_category = "Unknown"
                normal_balance = "DEBIT"
            
            # Calculate additional statistics
            transaction_count = account_transactions.count()
            debit_count = account_transactions.filter(transaction_type='DEBIT').count()
            credit_count = account_transactions.filter(transaction_type='CREDIT').count()
            avg_amount = account_transactions.aggregate(avg=Avg('amount_local_currency'))['avg'] or Decimal('0.00')
            
            # Get currency for this account (use the most common currency)
            account_currency = account_transactions.values('local_currency').annotate(
                count=Count('id')
            ).order_by('-count').first()
            currency = account_currency['local_currency'] if account_currency else 'SAR'
            
            gl_account_data.append({
                'account_id': account_id,
                'account_name': account_name,
                'account_type': account_type,
                'account_category': account_category,
                'normal_balance': normal_balance,
                'trial_balance': float(trial_balance),
                'trading_equity': float(trading_equity),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'transaction_count': transaction_count,
                'debit_count': debit_count,
                'credit_count': credit_count,
                'avg_amount': float(avg_amount),
                'currency': currency,
                'balance_type': 'CREDIT' if trial_balance < 0 else 'DEBIT' if trial_balance > 0 else 'ZERO'
            })
        
        # Sort by absolute trial balance (highest first)
        gl_account_data.sort(key=lambda x: abs(x['trial_balance']), reverse=True)
        
        return gl_account_data
    
    def _generate_gl_charts_data(self, transactions):
        """Generate charts data for GL accounts"""
        # Get GL account statistics
        account_stats = transactions.values('gl_account', 'local_currency').annotate(
            total_amount=Sum('amount_local_currency'),
            transaction_count=Count('id'),
            total_debits=Sum('amount_local_currency', filter=Q(transaction_type='DEBIT')),
            total_credits=Sum('amount_local_currency', filter=Q(transaction_type='CREDIT'))
        ).order_by('-total_amount')[:20]  # Top 20 accounts by amount
        
        # Get primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        # Prepare chart data
        chart_data = {
            'top_accounts_by_amount': [
                {
                    'account_id': stat['gl_account'],
                    'total_amount': float(stat['total_amount']),
                    'currency': stat['local_currency'],
                    'transaction_count': stat['transaction_count'],
                    'trial_balance': float((stat['total_debits'] or 0) - (stat['total_credits'] or 0))
                }
                for stat in account_stats
            ],
            'account_type_distribution': self._get_account_type_distribution(transactions),
            'balance_distribution': self._get_balance_distribution(transactions),
            'monthly_account_activity': self._get_monthly_account_activity(transactions),
            'currency': currency
        }
        
        return chart_data
    
    def _generate_gl_account_summary(self, transactions):
        """Generate comprehensive GL account summary with TB, TE, credits, and debits"""
        # Get all unique GL accounts from transactions
        gl_accounts = transactions.values('gl_account').distinct()
        
        # Get primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        gl_account_summary = []
        
        for account_data in gl_accounts:
            account_id = account_data['gl_account']
            
            # Get transactions for this account
            account_transactions = transactions.filter(gl_account=account_id)
            
            # Calculate Trial Balance (TB) - Debits minus Credits
            total_debits = account_transactions.filter(transaction_type='DEBIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            total_credits = account_transactions.filter(transaction_type='CREDIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            # Trial Balance = Debits - Credits
            trial_balance = total_debits - total_credits
            
            # Calculate Trading Equity (TE) - Net change in equity accounts
            # For equity accounts (typically 3xxxx series), TE = Credits - Debits
            # For other accounts, TE = Debits - Credits (same as TB)
            if account_id.startswith('3'):  # Equity accounts
                trading_equity = total_credits - total_debits
            else:
                trading_equity = trial_balance
            
            # Get account details from GLAccount model if available
            try:
                gl_account_obj = GLAccount.objects.get(account_id=account_id)
                account_name = gl_account_obj.account_name
                account_type = gl_account_obj.account_type
                account_category = gl_account_obj.account_category
                normal_balance = gl_account_obj.normal_balance
            except GLAccount.DoesNotExist:
                account_name = f"Account {account_id}"
                account_type = "Unknown"
                account_category = "Unknown"
                normal_balance = "DEBIT"
            
            # Calculate additional statistics
            transaction_count = account_transactions.count()
            debit_count = account_transactions.filter(transaction_type='DEBIT').count()
            credit_count = account_transactions.filter(transaction_type='CREDIT').count()
            avg_amount = account_transactions.aggregate(avg=Avg('amount_local_currency'))['avg'] or Decimal('0.00')
            
            # Get currency for this account (use the most common currency)
            account_currency = account_transactions.values('local_currency').annotate(
                count=Count('id')
            ).order_by('-count').first()
            account_currency_code = account_currency['local_currency'] if account_currency else currency
            
            # Determine balance type
            if trial_balance > 0:
                balance_type = 'DEBIT'
            elif trial_balance < 0:
                balance_type = 'CREDIT'
            else:
                balance_type = 'ZERO'
            
            # Check if balance is normal (matches normal_balance)
            is_normal_balance = (
                (normal_balance == 'DEBIT' and trial_balance >= 0) or
                (normal_balance == 'CREDIT' and trial_balance <= 0) or
                trial_balance == 0
            )
            
            gl_account_summary.append({
                'account_id': account_id,
                'account_name': account_name,
                'account_type': account_type,
                'account_category': account_category,
                'normal_balance': normal_balance,
                'currency': account_currency_code,
                
                # Balance information
                'trial_balance': float(trial_balance),
                'trading_equity': float(trading_equity),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'balance_type': balance_type,
                'is_normal_balance': is_normal_balance,
                
                # Transaction statistics
                'transaction_count': transaction_count,
                'debit_count': debit_count,
                'credit_count': credit_count,
                'avg_amount': float(avg_amount),
                
                # Additional metrics
                'debit_credit_ratio': float(total_debits / total_credits) if total_credits > 0 else None,
                'avg_debit_amount': float(total_debits / debit_count) if debit_count > 0 else 0,
                'avg_credit_amount': float(total_credits / credit_count) if credit_count > 0 else 0,
                'credit_debit_ratio': float(total_credits / total_debits) if total_debits > 0 else None
            })
        
        # Sort by absolute trial balance (highest first)
        gl_account_summary.sort(key=lambda x: abs(x['trial_balance']), reverse=True)
        
        # Calculate summary statistics
        total_accounts = len(gl_account_summary)
        total_trial_balance = sum(account['trial_balance'] for account in gl_account_summary)
        total_trading_equity = sum(account['trading_equity'] for account in gl_account_summary)
        total_debits = sum(account['total_debits'] for account in gl_account_summary)
        total_credits = sum(account['total_credits'] for account in gl_account_summary)
        
        # Count accounts by balance type
        debit_balance_accounts = sum(1 for account in gl_account_summary if account['balance_type'] == 'DEBIT')
        credit_balance_accounts = sum(1 for account in gl_account_summary if account['balance_type'] == 'CREDIT')
        zero_balance_accounts = sum(1 for account in gl_account_summary if account['balance_type'] == 'ZERO')
        
        # Count accounts with normal vs abnormal balances
        normal_balance_accounts = sum(1 for account in gl_account_summary if account['is_normal_balance'])
        abnormal_balance_accounts = total_accounts - normal_balance_accounts
        
        return {
            'summary_statistics': {
                'total_accounts': total_accounts,
                'total_trial_balance': float(total_trial_balance),
                'total_trading_equity': float(total_trading_equity),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'currency': currency,
                'debit_balance_accounts': debit_balance_accounts,
                'credit_balance_accounts': credit_balance_accounts,
                'zero_balance_accounts': zero_balance_accounts,
                'normal_balance_accounts': normal_balance_accounts,
                'abnormal_balance_accounts': abnormal_balance_accounts
            },
            'accounts': gl_account_summary
        }
    
from rest_framework import viewsets, status, generics
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework.parsers import MultiPartParser, FormParser
from django.db import transaction
from django.db.models import Q
from django.utils import timezone
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import pandas as pd
import csv
import io
import logging
from datetime import datetime, timedelta
from decimal import Decimal, InvalidOperation
from django.db.models import Sum, Avg, Count, Min, Max
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from django.db.models.manager import Manager
    # Type hints for Django models
    SAPGLPostingManager = Manager
    DataFileManager = Manager
    AnalysisSessionManager = Manager
    TransactionAnalysisManager = Manager

from .models import SAPGLPosting, DataFile, AnalysisSession, TransactionAnalysis, SystemMetrics, GLAccount, FileProcessingJob, MLModelTraining
from .serializers import (
    SAPGLPostingSerializer, SAPGLPostingListSerializer,
    DataFileSerializer, DataFileUploadSerializer,
    AnalysisSessionSerializer, AnalysisSessionCreateSerializer,
    TransactionAnalysisSerializer, TransactionAnalysisListSerializer,
    SystemMetricsSerializer, AnalysisRequestSerializer,
    AnalysisSummarySerializer, DataUploadResponseSerializer,
    DashboardStatsSerializer, GLAccountSerializer, GLAccountListSerializer,
    GLAccountAnalysisSerializer, TrialBalanceSerializer, GLAccountChartSerializer,
    GLAccountUploadSerializer, DuplicateAnomalySerializer, DuplicateAnomalyListSerializer,
    DuplicateChartDataSerializer, DuplicateAnalysisRequestSerializer, DuplicateTrainingDataSerializer,
    TargetedAnomalyUploadSerializer, FileProcessingJobSerializer, FileProcessingJobListSerializer,
    TargetedAnomalyResponseSerializer, MLModelTrainingSerializer, MLModelTrainingListSerializer,
    MLModelTrainingRequestSerializer, MLModelInfoSerializer, MLAnomalyResultsSerializer
)
from .analytics import SAPGLAnalyzer
from .ml_models import MLAnomalyDetector
from .enhanced_duplicate_analysis import EnhancedDuplicateAnalyzer

# Import all tasks once at the top to avoid duplicate registrations
from .tasks import (
    process_file_with_anomalies, debug_task, worker_health_check, 
    monitor_worker_performance, train_ml_models, retrain_ml_models,
    _process_file_content, _run_default_analytics, _run_requested_anomalies
)

logger = logging.getLogger(__name__)

class SAPGLPostingViewSet(viewsets.ModelViewSet):
    """ViewSet for SAP GL Posting data"""
    
    queryset = SAPGLPosting.objects.all()
    serializer_class = SAPGLPostingSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return SAPGLPostingListSerializer
        return SAPGLPostingSerializer
    
    def get_queryset(self):
        # type: ignore
        queryset = SAPGLPosting.objects.all()
        
        # Apply filters
        document_number = self.request.query_params.get('document_number', None)
        if document_number:
            queryset = queryset.filter(document_number__icontains=document_number)
        
        document_type = self.request.query_params.get('document_type', None)
        if document_type:
            queryset = queryset.filter(document_type=document_type)
        
        gl_account = self.request.query_params.get('gl_account', None)
        if gl_account:
            queryset = queryset.filter(gl_account__icontains=gl_account)
        
        profit_center = self.request.query_params.get('profit_center', None)
        if profit_center:
            queryset = queryset.filter(profit_center__icontains=profit_center)
        
        user_name = self.request.query_params.get('user_name', None)
        if user_name:
            queryset = queryset.filter(user_name__icontains=user_name)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(posting_date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(posting_date__lte=date_to)
        
        min_amount = self.request.query_params.get('min_amount', None)
        if min_amount:
            queryset = queryset.filter(amount_local_currency__gte=min_amount)
        
        max_amount = self.request.query_params.get('max_amount', None)
        if max_amount:
            queryset = queryset.filter(amount_local_currency__lte=max_amount)
        
        fiscal_year = self.request.query_params.get('fiscal_year', None)
        if fiscal_year:
            queryset = queryset.filter(fiscal_year=fiscal_year)
        
        return queryset
    
    @action(detail=False, methods=['get'])
    def statistics(self, request):
        """Get statistics for SAP GL postings"""
        queryset = self.get_queryset()
        
        stats = {
            'total_transactions': queryset.count(),
            'total_amount': float(queryset.aggregate(total=Sum('amount_local_currency'))['total'] or 0),
            'avg_amount': float(queryset.aggregate(avg=Avg('amount_local_currency'))['avg'] or 0),
            'high_value_transactions': queryset.filter(amount_local_currency__gt=1000000).count(),
            'unique_users': queryset.values('user_name').distinct().count(),
            'unique_accounts': queryset.values('gl_account').distinct().count(),
            'unique_profit_centers': queryset.values('profit_center').distinct().count(),
            'date_range': {
                'min_date': queryset.aggregate(min_date=Min('posting_date'))['min_date'],
                'max_date': queryset.aggregate(max_date=Max('posting_date'))['max_date']
            }
        }
        
        return Response(stats)
    
    @action(detail=False, methods=['get'])
    def top_users(self, request):
        """Get top users by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_users = queryset.values('user_name').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_users))
    
    @action(detail=False, methods=['get'])
    def top_accounts(self, request):
        """Get top G/L accounts by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_accounts = queryset.values('gl_account').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_accounts))

class DataFileViewSet(viewsets.ModelViewSet):
    """ViewSet for uploaded data files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    parser_classes = (MultiPartParser, FormParser)
    
    @action(detail=False, methods=['post'])
    def upload(self, request):
        """Upload and process CSV file"""
        try:
            file_obj = request.FILES.get('file')
            if not file_obj:
                return Response(
                    {'error': 'No file provided'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get additional fields from request data
            engagement_id = request.data.get('engagement_id', '')
            client_name = request.data.get('client_name', '')
            company_name = request.data.get('company_name', '')
            fiscal_year = request.data.get('fiscal_year', 2025)
            audit_start_date = request.data.get('audit_start_date')
            audit_end_date = request.data.get('audit_end_date')
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file in background (for now, process synchronously)
            result = self._process_csv_file(data_file, file_obj)
            
            if result['success']:
                return Response(
                    DataUploadResponseSerializer(data_file).data,
                    status=status.HTTP_201_CREATED
                )
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error uploading file: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_csv_file(self, data_file, file_obj):
        """Process uploaded CSV file"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            
            for row in csv_reader:
                try:
                    # Map CSV columns to model fields
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        
                        # Update date and amount ranges
                        if posting.posting_date:
                            if min_date is None or posting.posting_date < min_date:
                                min_date = posting.posting_date
                            if max_date is None or posting.posting_date > max_date:
                                max_date = posting.posting_date
                        
                        if min_amount is None or posting.amount_local_currency < min_amount:
                            min_amount = posting.amount_local_currency
                        if max_amount is None or posting.amount_local_currency > max_amount:
                            max_amount = posting.amount_local_currency
                except Exception as e:
                    logger.error(f"Error processing row: {e}")
                    failed_count += 1
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            return {'success': True}
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Map CSV columns to model fields
            # This mapping should be adjusted based on actual CSV structure
            posting = SAPGLPosting(
                document_number=row.get('Document', ''),
                document_type=row.get('Document type', ''),
                amount_local_currency=Decimal(row.get('Amount in Local Currency', '0').replace(',', '')),
                local_currency=row.get('Local Currency', 'SAR'),
                gl_account=row.get('G/L Account', ''),
                profit_center=row.get('Profit Center', ''),
                user_name=row.get('User Name', ''),
                fiscal_year=int(row.get('Fiscal Year', '2025')),
                posting_period=int(row.get('Posting period', '1')),
                text=row.get('Text', ''),
                segment=row.get('Segment', ''),
                clearing_document=row.get('Clearing Document', ''),
                offsetting_account=row.get('Offsetting', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                sales_document=row.get('Sales Document', ''),
                assignment=row.get('Assignment', ''),
                year_month=row.get('Year/Month', '')
            )
            
            # Parse dates
            posting_date_str = row.get('Posting Date', '')
            if posting_date_str:
                try:
                    parsed_date = datetime.strptime(posting_date_str, '%m/%d/%Y').date()
                    posting.posting_date = parsed_date
                except:
                    pass
            
            document_date_str = row.get('Document Date', '')
            if document_date_str:
                try:
                    parsed_date = datetime.strptime(document_date_str, '%m/%d/%Y').date()
                    posting.document_date = parsed_date
                except:
                    pass
            
            entry_date_str = row.get('Entry Date', '')
            if entry_date_str:
                try:
                    parsed_date = datetime.strptime(entry_date_str, '%m/%d/%Y').date()
                    posting.entry_date = parsed_date
                except:
                    pass
            
            return posting
            
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None

class AnalysisSessionViewSet(viewsets.ModelViewSet):
    """ViewSet for analysis sessions"""
    
    queryset = AnalysisSession.objects.all()
    serializer_class = AnalysisSessionSerializer
    
    def get_serializer_class(self):
        if self.action == 'create':
            return AnalysisSessionCreateSerializer
        return AnalysisSessionSerializer
    
    @action(detail=True, methods=['post'])
    def run_analysis(self, request, pk=None):
        """Run analysis for a session"""
        try:
            session = self.get_object()
            
            # Check if session is already running or completed
            if session.status in ['RUNNING', 'COMPLETED']:
                return Response(
                    {'error': f'Analysis session is already {session.status.lower()}'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Run analysis
            analyzer = SAPGLAnalyzer()
            result = analyzer.analyze_transactions(session)
            
            if 'error' in result:
                return Response(
                    {'error': result['error']},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"Error running analysis: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['get'])
    def summary(self, request, pk=None):
        """Get analysis summary"""
        try:
            session = self.get_object()
            analyzer = SAPGLAnalyzer()
            summary = analyzer.get_analysis_summary(session)
            
            return Response(AnalysisSummarySerializer(summary).data)
            
        except Exception as e:
            logger.error(f"Error getting summary: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class TransactionAnalysisViewSet(viewsets.ReadOnlyModelViewSet):
    """ViewSet for transaction analysis results"""
    
    queryset = TransactionAnalysis.objects.all()
    serializer_class = TransactionAnalysisSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return TransactionAnalysisListSerializer
        return TransactionAnalysisSerializer
    
    def get_queryset(self):
        queryset = TransactionAnalysis.objects.all()
        
        # Apply filters
        session_id = self.request.query_params.get('session_id', None)
        if session_id:
            queryset = queryset.filter(session_id=session_id)
        
        risk_level = self.request.query_params.get('risk_level', None)
        if risk_level:
            queryset = queryset.filter(risk_level=risk_level)
        
        min_risk_score = self.request.query_params.get('min_risk_score', None)
        if min_risk_score:
            queryset = queryset.filter(risk_score__gte=min_risk_score)
        
        max_risk_score = self.request.query_params.get('max_risk_score', None)
        if max_risk_score:
            queryset = queryset.filter(risk_score__lte=max_risk_score)
        
        # Filter by anomaly types
        amount_anomaly = self.request.query_params.get('amount_anomaly', None)
        if amount_anomaly:
            queryset = queryset.filter(amount_anomaly=amount_anomaly.lower() == 'true')
        
        timing_anomaly = self.request.query_params.get('timing_anomaly', None)
        if timing_anomaly:
            queryset = queryset.filter(timing_anomaly=timing_anomaly.lower() == 'true')
        
        user_anomaly = self.request.query_params.get('user_anomaly', None)
        if user_anomaly:
            queryset = queryset.filter(user_anomaly=user_anomaly.lower() == 'true')
        
        account_anomaly = self.request.query_params.get('account_anomaly', None)
        if account_anomaly:
            queryset = queryset.filter(account_anomaly=account_anomaly.lower() == 'true')
        
        pattern_anomaly = self.request.query_params.get('pattern_anomaly', None)
        if pattern_anomaly:
            queryset = queryset.filter(pattern_anomaly=pattern_anomaly.lower() == 'true')
        
        return queryset.order_by('-risk_score', '-created_at')

class GLAccountViewSet(viewsets.ModelViewSet):
    """ViewSet for GL Account management and analysis"""
    
    queryset = GLAccount.objects.all()
    serializer_class = GLAccountSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return GLAccountListSerializer
        return GLAccountSerializer
    
    def get_queryset(self):
        queryset = GLAccount.objects.all()
        
        # Apply filters
        account_id = self.request.query_params.get('account_id', None)
        if account_id:
            queryset = queryset.filter(account_id__icontains=account_id)
        
        account_name = self.request.query_params.get('account_name', None)
        if account_name:
            queryset = queryset.filter(account_name__icontains=account_name)
        
        account_type = self.request.query_params.get('account_type', None)
        if account_type:
            queryset = queryset.filter(account_type=account_type)
        
        account_category = self.request.query_params.get('account_category', None)
        if account_category:
            queryset = queryset.filter(account_category=account_category)
        
        normal_balance = self.request.query_params.get('normal_balance', None)
        if normal_balance:
            queryset = queryset.filter(normal_balance=normal_balance)
        
        is_active = self.request.query_params.get('is_active', None)
        if is_active:
            queryset = queryset.filter(is_active=is_active.lower() == 'true')
        
        return queryset.order_by('account_id')
    
    @action(detail=False, methods=['get'])
    def analysis(self, request):
        """Get comprehensive analysis for all GL accounts"""
        try:
            accounts = self.get_queryset()
            analysis_results = []
            
            print(f"Debug: Found {accounts.count()} GL accounts")  # Debug log
            
            for account in accounts:
                try:
                    # Get account postings using the gl_account field (not the relationship)
                    postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                    
                    print(f"Debug: Account {account.account_id} has {postings.count()} postings")  # Debug log
                    
                    if not postings.exists():
                        print(f"Debug: Account {account.account_id} has no postings, skipping")  # Debug log
                        continue
                    
                    # Calculate statistics
                    total_debits = postings.filter(transaction_type='DEBIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    total_credits = postings.filter(transaction_type='CREDIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    debit_count = postings.filter(transaction_type='DEBIT').count()
                    credit_count = postings.filter(transaction_type='CREDIT').count()
                    
                    # Risk analysis
                    high_value_count = postings.filter(amount_local_currency__gt=1000000).count()
                    
                    # Get analysis records for this account
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    flagged_count = analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    
                    # Calculate average risk score
                    avg_risk_score = analysis_records.aggregate(
                        avg_score=Avg('risk_score')
                    )['avg_score'] or 0.0
                    
                    # Activity analysis
                    first_transaction = postings.order_by('posting_date').first()
                    last_transaction = postings.order_by('-posting_date').first()
                    
                    avg_amount = postings.aggregate(
                        avg_amount=Avg('amount_local_currency')
                    )['avg_amount'] or Decimal('0.00')
                    
                    max_amount = postings.aggregate(
                        max_amount=Max('amount_local_currency')
                    )['max_amount'] or Decimal('0.00')
                    
                    analysis_results.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'account_type': account.account_type,
                        'account_category': account.account_category,
                        'normal_balance': account.normal_balance,
                        'current_balance': float(account.current_balance),
                        'total_debits': float(total_debits),
                        'total_credits': float(total_credits),
                        'transaction_count': postings.count(),
                        'debit_count': debit_count,
                        'credit_count': credit_count,
                        'high_value_transactions': high_value_count,
                        'flagged_transactions': flagged_count,
                        'risk_score': round(avg_risk_score, 2),
                        'first_transaction_date': first_transaction.posting_date if first_transaction else None,
                        'last_transaction_date': last_transaction.posting_date if last_transaction else None,
                        'avg_transaction_amount': float(avg_amount),
                        'max_transaction_amount': float(max_amount)
                    })
                    
                    print(f"Debug: Added analysis for account {account.account_id}")  # Debug log
                    
                except Exception as account_error:
                    logger.error(f"Error processing account {account.account_id}: {account_error}")
                    print(f"Debug: Error processing account {account.account_id}: {account_error}")  # Debug log
                    continue
            
            print(f"Debug: Total analysis results: {len(analysis_results)}")  # Debug log
            
            # Sort by risk score (highest first)
            analysis_results.sort(key=lambda x: x['risk_score'], reverse=True)
            
            return Response(analysis_results)
            
        except Exception as e:
            logger.error(f"Error in GL account analysis: {e}")
            print(f"Debug: GL account analysis error: {e}")  # Debug log
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def trial_balance(self, request):
        """Generate Trial Balance report"""
        try:
            # Get date range from request
            date_from = request.query_params.get('date_from', None)
            date_to = request.query_params.get('date_to', None)
            
            accounts = self.get_queryset()
            trial_balance_data = []
            
            for account in accounts:
                # Get postings for the account using gl_account field
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                if date_from:
                    postings = postings.filter(posting_date__gte=date_from)
                if date_to:
                    postings = postings.filter(posting_date__lte=date_to)
                
                if not postings.exists():
                    continue
                
                # Calculate period movements
                period_debit = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                period_credit = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                # Calculate closing balances
                if account.normal_balance == 'DEBIT':
                    closing_debit = period_debit - period_credit
                    closing_credit = Decimal('0.00')
                    if closing_debit < 0:
                        closing_credit = abs(closing_debit)
                        closing_debit = Decimal('0.00')
                else:
                    closing_credit = period_credit - period_debit
                    closing_debit = Decimal('0.00')
                    if closing_credit < 0:
                        closing_debit = abs(closing_credit)
                        closing_credit = Decimal('0.00')
                
                # Net balance
                net_balance = closing_debit - closing_credit
                
                trial_balance_data.append({
                    'account_id': account.account_id,
                    'account_name': account.account_name,
                    'account_type': account.account_type,
                    'account_category': account.account_category,
                    'normal_balance': account.normal_balance,
                    'opening_debit': float(Decimal('0.00')),  # Assuming no opening balances for now
                    'opening_credit': float(Decimal('0.00')),
                    'period_debit': float(period_debit),
                    'period_credit': float(period_credit),
                    'closing_debit': float(closing_debit),
                    'closing_credit': float(closing_credit),
                    'net_balance': float(net_balance),
                    'transaction_count': postings.count()
                })
            
            # Sort by account type and account ID
            trial_balance_data.sort(key=lambda x: (x['account_type'], x['account_id']))
            
            return Response(trial_balance_data)
            
        except Exception as e:
            logger.error(f"Error generating trial balance: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def charts(self, request):
        """Get charts data for GL accounts"""
        try:
            accounts = self.get_queryset()
            
            # Account type distribution
            type_distribution = {}
            category_distribution = {}
            
            for account in accounts:
                # Get postings for this account
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                # Type distribution
                if account.account_type not in type_distribution:
                    type_distribution[account.account_type] = {
                        'count': 0,
                        'total_balance': Decimal('0.00'),
                        'total_transactions': 0
                    }
                type_distribution[account.account_type]['count'] += 1
                type_distribution[account.account_type]['total_balance'] += account.current_balance
                type_distribution[account.account_type]['total_transactions'] += postings.count()
                
                # Category distribution
                if account.account_category not in category_distribution:
                    category_distribution[account.account_category] = {
                        'count': 0,
                        'total_balance': Decimal('0.00')
                    }
                category_distribution[account.account_category]['count'] += 1
                category_distribution[account.account_category]['total_balance'] += account.current_balance
            
            # Top accounts by balance
            top_accounts_by_balance = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                if postings.exists():
                    top_accounts_by_balance.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'balance': float(account.current_balance),
                        'account_type': account.account_type
                    })
            
            top_accounts_by_balance.sort(key=lambda x: abs(x['balance']), reverse=True)
            top_accounts_by_balance = top_accounts_by_balance[:10]
            
            # Top accounts by transaction count
            top_accounts_by_transactions = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                transaction_count = postings.count()
                if transaction_count > 0:
                    top_accounts_by_transactions.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'transaction_count': transaction_count,
                        'account_type': account.account_type
                    })
            
            top_accounts_by_transactions.sort(key=lambda x: x['transaction_count'], reverse=True)
            top_accounts_by_transactions = top_accounts_by_transactions[:10]
            
            # Debit vs Credit analysis
            total_debits = Decimal('0.00')
            total_credits = Decimal('0.00')
            debit_count = 0
            credit_count = 0
            
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                account_debits = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                account_credits = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                total_debits += account_debits
                total_credits += account_credits
                debit_count += postings.filter(transaction_type='DEBIT').count()
                credit_count += postings.filter(transaction_type='CREDIT').count()
            
            debit_credit_analysis = {
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'debit_count': debit_count,
                'credit_count': credit_count,
                'net_movement': float(total_debits - total_credits)
            }
            
            # Risk distribution by account type
            risk_distribution = []
            for account_type, data in type_distribution.items():
                accounts_of_type = accounts.filter(account_type=account_type)
                high_risk_count = 0
                total_risk_score = 0
                
                for account in accounts_of_type:
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    high_risk_count += analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    avg_score = analysis_records.aggregate(avg_score=Avg('risk_score'))['avg_score'] or 0
                    total_risk_score += avg_score
                
                avg_risk_score = total_risk_score / len(accounts_of_type) if accounts_of_type.exists() else 0
                
                risk_distribution.append({
                    'account_type': account_type,
                    'account_count': data['count'],
                    'high_risk_count': high_risk_count,
                    'avg_risk_score': round(avg_risk_score, 2)
                })
            
            charts_data = {
                'account_type_distribution': [
                    {
                        'account_type': account_type,
                        'count': data['count'],
                        'total_balance': float(data['total_balance']),
                        'total_transactions': data['total_transactions']
                    }
                    for account_type, data in type_distribution.items()
                ],
                'account_category_distribution': [
                    {
                        'category': category,
                        'count': data['count'],
                        'total_balance': float(data['total_balance'])
                    }
                    for category, data in category_distribution.items()
                ],
                'top_accounts_by_balance': top_accounts_by_balance,
                'top_accounts_by_transactions': top_accounts_by_transactions,
                'debit_credit_analysis': debit_credit_analysis,
                'risk_distribution': risk_distribution
            }
            
            return Response(charts_data)
            
        except Exception as e:
            logger.error(f"Error generating charts data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['post'])
    def upload_master_data(self, request):
        """Upload GL Account master data from CSV"""
        try:
            serializer = GLAccountUploadSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            file_obj = serializer.validated_data['file']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Process CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            created_count = 0
            updated_count = 0
            failed_count = 0
            
            for row in csv_reader:
                try:
                    account_id = row.get('Account ID', '').strip()
                    if not account_id:
                        failed_count += 1
                        continue
                    
                    account_data = {
                        'account_name': row.get('Account Name', '').strip(),
                        'account_type': row.get('Account Type', '').strip(),
                        'account_category': row.get('Account Category', '').strip(),
                        'account_subcategory': row.get('Account Subcategory', '').strip(),
                        'normal_balance': row.get('Normal Balance', 'DEBIT').strip().upper(),
                        'is_active': row.get('Is Active', 'TRUE').strip().upper() == 'TRUE'
                    }
                    
                    # Create or update account
                    account, created = GLAccount.objects.update_or_create(
                        account_id=account_id,
                        defaults=account_data
                    )
                    
                    if created:
                        created_count += 1
                    else:
                        updated_count += 1
                        
                except Exception as e:
                    logger.error(f"Error processing GL account row: {e}")
                    failed_count += 1
            
            return Response({
                'message': 'GL Account master data uploaded successfully',
                'created_count': created_count,
                'updated_count': updated_count,
                'failed_count': failed_count
            })
            
        except Exception as e:
            logger.error(f"Error uploading GL account master data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DashboardView(generics.GenericAPIView):
    """Dashboard view for system overview"""
    
    def get(self, request):
        """Get dashboard statistics"""
        try:
            # Date range (last 30 days)
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=30)
            
            # Overall statistics
            total_transactions = SAPGLPosting.objects.count()
            total_amount = SAPGLPosting.objects.aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0')
            total_files = DataFile.objects.count()
            total_sessions = AnalysisSession.objects.count()
            
            # Recent activity
            recent_transactions = SAPGLPosting.objects.filter(
                created_at__date__gte=start_date
            ).count()
            recent_files = DataFile.objects.filter(
                uploaded_at__date__gte=start_date
            ).count()
            recent_sessions = AnalysisSession.objects.filter(
                created_at__date__gte=start_date
            ).count()
            
            # Risk statistics
            flagged_transactions = TransactionAnalysis.objects.filter(
                risk_level__in=['HIGH', 'CRITICAL']
            ).count()
            high_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='HIGH'
            ).count()
            critical_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='CRITICAL'
            ).count()
            
            # Top statistics
            top_users = SAPGLPosting.objects.values('user_name').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_accounts = SAPGLPosting.objects.values('gl_account').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_profit_centers = SAPGLPosting.objects.values('profit_center').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_document_types = SAPGLPosting.objects.values('document_type').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            stats = {
                'total_transactions': total_transactions,
                'total_amount': float(total_amount),
                'total_files': total_files,
                'total_sessions': total_sessions,
                'recent_transactions': recent_transactions,
                'recent_files': recent_files,
                'recent_sessions': recent_sessions,
                'flagged_transactions': flagged_transactions,
                'high_risk_transactions': high_risk_transactions,
                'critical_risk_transactions': critical_risk_transactions,
                'top_users': list(top_users),
                'top_accounts': list(top_accounts),
                'top_profit_centers': list(top_profit_centers),
                'top_document_types': list(top_document_types),
                'date_from': start_date,
                'date_to': end_date
            }
            
            return Response(DashboardStatsSerializer(stats).data)
            
        except Exception as e:
            logger.error(f"Error getting dashboard stats: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileListView(generics.ListAPIView):
    """View to get list of uploaded files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    
    def get_queryset(self):
        queryset = DataFile.objects.all()
        
        # Apply filters
        status_filter = self.request.query_params.get('status', None)
        if status_filter:
            queryset = queryset.filter(status=status_filter)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(uploaded_at__date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(uploaded_at__date__lte=date_to)
        
        return queryset.order_by('-uploaded_at')
    
    def list(self, request, *args, **kwargs):
        queryset = self.get_queryset()
        serializer = self.get_serializer(queryset, many=True)
        
        # Get file data with session information
        files_data = []
        serializer_data = serializer.data
        
        for i, file_obj in enumerate(queryset):
            file_data = serializer_data[i].copy()  # Make a copy to avoid modifying the original
            
            # Get related analysis sessions for this file
            related_sessions = AnalysisSession.objects.filter(
                created_at__gte=file_obj.uploaded_at
            ).order_by('-created_at')
            
            # Add session information to file data
            file_data['sessions'] = []
            for session in related_sessions:
                session_data = {
                    'id': str(session.id),
                    'session_name': session.session_name,
                    'status': session.status,
                    'created_at': session.created_at,
                    'started_at': session.started_at,
                    'completed_at': session.completed_at,
                    'total_transactions': session.total_transactions,
                    'flagged_transactions': session.flagged_transactions,
                    'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions > 0 else 0
                }
                file_data['sessions'].append(session_data)
            
            files_data.append(file_data)
        
        # Add summary statistics
        total_files = queryset.count()
        total_records = queryset.aggregate(total=Sum('total_records'))['total'] or 0
        total_processed = queryset.aggregate(total=Sum('processed_records'))['total'] or 0
        total_failed = queryset.aggregate(total=Sum('failed_records'))['total'] or 0
        
        # Calculate total sessions across all files
        total_sessions = sum(len(file_data['sessions']) for file_data in files_data)
        
        response_data = {
            'files': files_data,
            'summary': {
                'total_files': total_files,
                'total_records': total_records,
                'total_processed': total_processed,
                'total_failed': total_failed,
                'total_sessions': total_sessions,
                'success_rate': (total_processed / total_records * 100) if total_records > 0 else 0
            }
        }
        
        return Response(response_data)


class FileUploadAnalysisView(generics.CreateAPIView):
    """View to upload file and run analysis"""
    
    parser_classes = (MultiPartParser, FormParser)
    serializer_class = DataFileUploadSerializer
    
    def create(self, request, *args, **kwargs):
        try:
            # Validate the request using serializer
            serializer = self.get_serializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Get the validated data
            file_obj = serializer.validated_data['file']
            description = serializer.validated_data.get('description', '')
            engagement_id = serializer.validated_data['engagement_id']
            client_name = serializer.validated_data['client_name']
            company_name = serializer.validated_data['company_name']
            fiscal_year = serializer.validated_data['fiscal_year']
            audit_start_date = serializer.validated_data['audit_start_date']
            audit_end_date = serializer.validated_data['audit_end_date']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file and run analysis
            result = self._process_and_analyze_file(data_file, file_obj)
            
            if result['success']:
                response_data = {
                        'file': DataFileSerializer(data_file).data,
                        'analysis': result.get('analysis', {}),
                    'processing_summary': result.get('processing_summary', {}),
                        'message': 'File uploaded and analysis completed successfully'
                }
                
                # Add validation errors if any
                if 'validation_errors' in result:
                    response_data['validation_errors'] = result['validation_errors']
                    response_data['validation_errors_summary'] = result.get('validation_errors_summary', '')
                
                return Response(response_data, status=status.HTTP_201_CREATED)
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error in file upload and analysis: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_and_analyze_file(self, data_file, file_obj):
        """Process uploaded CSV file and run analysis"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Keep existing transactions - don't delete them
            logger.info("Processing new file while keeping existing transactions")
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            new_transactions = []
            validation_errors = []
            row_number = 0
            
            for row in csv_reader:
                row_number += 1
                # Map CSV columns to model fields with minimal validation
                posting = self._create_posting_from_row(row)
                if posting:
                    processed_count += 1
                    new_transactions.append(posting)
                    
                    # Update date and amount ranges
                    if posting.posting_date:
                        if min_date is None or posting.posting_date < min_date:
                            min_date = posting.posting_date
                        if max_date is None or posting.posting_date > max_date:
                            max_date = posting.posting_date
                    
                    if min_amount is None or posting.amount_local_currency < min_amount:
                        min_amount = posting.amount_local_currency
                    if max_amount is None or posting.amount_local_currency > max_amount:
                        max_amount = posting.amount_local_currency
                else:
                    failed_count += 1
            
            # Save all transactions in batch
            if new_transactions:
                SAPGLPosting.objects.bulk_create(new_transactions)  # type: ignore
                logger.info(f"Saved {len(new_transactions)} transactions in batch")
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            # Initialize analyzer
            analyzer = SAPGLAnalyzer()
            
            # Run analysis if we have processed transactions
            analysis_result = {}
            if processed_count > 0:
                analysis_result = self._run_file_analysis(analyzer, data_file, new_transactions)
            
            # Prepare response
            response_data = {
                'success': True,
                'analysis': analysis_result,
                'processing_summary': {
                    'total_rows': processed_count + failed_count,
                    'processed_count': processed_count,
                    'failed_count': failed_count,
                    'success_rate': (processed_count / (processed_count + failed_count) * 100) if (processed_count + failed_count) > 0 else 0
                }
            }
            
            # Add validation errors if any
            if validation_errors:
                response_data['validation_errors'] = validation_errors[:10]  # Limit to first 10 errors
                if len(validation_errors) > 10:
                    response_data['validation_errors_summary'] = f"Showing first 10 of {len(validation_errors)} validation errors"
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Define required fields and their display names
            required_fields = {
                'Document Number': row.get('Document Number', '').strip(),
                'G/L Account': row.get('G/L Account', '').strip(),
                'Amount in Local Currency': row.get('Amount in Local Currency', '').strip(),
                'Posting Date': row.get('Posting Date', '').strip(),
                'User Name': row.get('User Name', '').strip()
            }
            
            # Check for missing required fields
            missing_fields = []
            print(required_fields)
            for field_name, field_value in required_fields.items():
                if not field_value:
                    missing_fields.append(field_name)
            
            # If any required fields are missing, raise an error
            if missing_fields:
                error_msg = f"Missing required fields: {', '.join(missing_fields)}"
                logger.error(f"Row validation failed: {error_msg}")
                logger.error(f"Complete row data for debugging: {dict(row)}")
                raise ValueError(error_msg)
            
            # Parse dates with multiple format support
            posting_date = self._parse_date(row.get('Posting Date', ''))
            document_date = self._parse_date(row.get('Document Date', ''))
            entry_date = self._parse_date(row.get('Entry Date', ''))
            
            # Parse amount with proper error handling
            amount_str = row.get('Amount in Local Currency', '0')
            try:
                # Clean the amount string - remove commas, spaces, and currency symbols
                amount_str = str(amount_str).replace(',', '').replace(' ', '').replace('SAR', '').replace('$', '').strip()
                if not amount_str or amount_str == '':
                    raise ValueError("Amount cannot be empty")
                amount_local_currency = Decimal(amount_str)
                if amount_local_currency <= 0:
                    raise ValueError("Amount must be greater than zero")
            except (ValueError, TypeError, InvalidOperation) as e:
                error_msg = f"Invalid amount format: {amount_str}. Error: {str(e)}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Determine transaction type (DEBIT/CREDIT)
            # Check if there's a specific transaction type field
            transaction_type = row.get('Transaction Type', '').strip().upper()
            if not transaction_type:
                # Try to determine from amount (negative = credit, positive = debit)
                # Or use a default based on account type
                if amount_local_currency < 0:
                    transaction_type = 'CREDIT'
                    amount_local_currency = abs(amount_local_currency)  # Make amount positive
                else:
                    transaction_type = 'DEBIT'
            elif transaction_type not in ['DEBIT', 'CREDIT']:
                transaction_type = 'DEBIT'  # Default to debit
            
            # Parse fiscal year and posting period with error handling
            try:
                fiscal_year = int(row.get('Fiscal Year', '0'))
            except (ValueError, TypeError):
                fiscal_year = 2025  # Default value
                
            try:
                posting_period = int(row.get('Posting Period', '0'))
            except (ValueError, TypeError):
                posting_period = 1  # Default value
            
            # Map CSV columns to model fields
            posting = SAPGLPosting(
                document_number=row.get('Document Number', ''),
                posting_date=posting_date,
                gl_account=row.get('G/L Account', ''),
                amount_local_currency=amount_local_currency,
                transaction_type=transaction_type,
                local_currency=row.get('Local Currency', 'SAR'),
                text=row.get('Text', ''),
                document_date=document_date,
                offsetting_account=row.get('Offsetting Account', ''),
                user_name=row.get('User Name', ''),
                entry_date=entry_date,
                document_type=row.get('Document Type', ''),
                profit_center=row.get('Profit Center', ''),
                cost_center=row.get('Cost Center', ''),
                clearing_document=row.get('Clearing Document', ''),
                segment=row.get('Segment', ''),
                wbs_element=row.get('WBS Element', ''),
                plant=row.get('Plant', ''),
                material=row.get('Material', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                billing_document=row.get('Billing Document', ''),
                sales_document=row.get('Sales Document', ''),
                purchasing_document=row.get('Purchasing Document', ''),
                order_number=row.get('Order Number', ''),
                asset_number=row.get('Asset Number', ''),
                network=row.get('Network', ''),
                assignment=row.get('Assignment', ''),
                tax_code=row.get('Tax Code', ''),
                account_assignment=row.get('Account Assignment', ''),
                fiscal_year=fiscal_year,
                posting_period=posting_period,
                year_month=row.get('Year/Month', '')
            )
            return posting
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None
    
    def _parse_date(self, date_str):
        """Parse date string with multiple format support"""
        if not date_str:
            return None
        
        date_formats = [
            '%Y-%m-%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%Y/%m/%d',
            '%m-%d-%Y',
            '%d-%m-%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"Could not parse date: {date_str}")
        return None
    

    
    def _run_file_analysis(self, analyzer, data_file, transactions):
        """Run analysis on the uploaded file data"""
        try:
            # Create analysis session
            session = AnalysisSession.objects.create(
                session_name=f"Analysis for {data_file.file_name}",
                description=f"Automated analysis for uploaded file {data_file.file_name}",
                date_from=data_file.min_date,
                date_to=data_file.max_date,
                min_amount=data_file.min_amount,
                max_amount=data_file.max_amount,
                status='RUNNING'
            )
            
            # Run analysis using the session
            analysis_results = analyzer.analyze_transactions(session)
            
            if 'error' in analysis_results:
                return {
                    'error': analysis_results['error'],
                    'analysis_status': 'FAILED'
                }
            
            # Get analysis summary
            summary = analyzer.get_analysis_summary(session)
            
            return {
                'session_id': str(session.id),
                'total_transactions': summary.get('total_transactions', 0),
                'total_amount': summary.get('total_amount', 0.0),
                'flagged_transactions': summary.get('flagged_transactions', 0),
                'high_value_transactions': summary.get('high_value_transactions', 0),
                'analysis_status': session.status,
                'flag_rate': summary.get('flag_rate', 0.0),
                'anomaly_summary': summary.get('anomaly_summary', {}),
                'risk_distribution': summary.get('risk_distribution', [])
            }
            
        except Exception as e:
            logger.error(f"Error running file analysis: {e}")
            return {
                'error': str(e),
                'analysis_status': 'FAILED'
            }


from rest_framework import viewsets, status, generics
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework.parsers import MultiPartParser, FormParser
from django.db import transaction
from django.db.models import Q
from django.utils import timezone
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
import pandas as pd
import csv
import io
import logging
from datetime import datetime, timedelta
from decimal import Decimal, InvalidOperation
from django.db.models import Sum, Avg, Count, Min, Max
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from django.db.models.manager import Manager
    # Type hints for Django models
    SAPGLPostingManager = Manager
    DataFileManager = Manager
    AnalysisSessionManager = Manager
    TransactionAnalysisManager = Manager

from .models import SAPGLPosting, DataFile, AnalysisSession, TransactionAnalysis, SystemMetrics, GLAccount, FileProcessingJob, MLModelTraining
from .serializers import (
    SAPGLPostingSerializer, SAPGLPostingListSerializer,
    DataFileSerializer, DataFileUploadSerializer,
    AnalysisSessionSerializer, AnalysisSessionCreateSerializer,
    TransactionAnalysisSerializer, TransactionAnalysisListSerializer,
    SystemMetricsSerializer, AnalysisRequestSerializer,
    AnalysisSummarySerializer, DataUploadResponseSerializer,
    DashboardStatsSerializer, GLAccountSerializer, GLAccountListSerializer,
    GLAccountAnalysisSerializer, TrialBalanceSerializer, GLAccountChartSerializer,
    GLAccountUploadSerializer, DuplicateAnomalySerializer, DuplicateAnomalyListSerializer,
    DuplicateChartDataSerializer, DuplicateAnalysisRequestSerializer, DuplicateTrainingDataSerializer,
    TargetedAnomalyUploadSerializer, FileProcessingJobSerializer, FileProcessingJobListSerializer,
    TargetedAnomalyResponseSerializer, MLModelTrainingSerializer, MLModelTrainingListSerializer,
    MLModelTrainingRequestSerializer, MLModelInfoSerializer, MLAnomalyResultsSerializer
)
from .analytics import SAPGLAnalyzer
from .ml_models import MLAnomalyDetector

logger = logging.getLogger(__name__)

class SAPGLPostingViewSet(viewsets.ModelViewSet):
    """ViewSet for SAP GL Posting data"""
    
    queryset = SAPGLPosting.objects.all()
    serializer_class = SAPGLPostingSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return SAPGLPostingListSerializer
        return SAPGLPostingSerializer
    
    def get_queryset(self):
        # type: ignore
        queryset = SAPGLPosting.objects.all()
        
        # Apply filters
        document_number = self.request.query_params.get('document_number', None)
        if document_number:
            queryset = queryset.filter(document_number__icontains=document_number)
        
        document_type = self.request.query_params.get('document_type', None)
        if document_type:
            queryset = queryset.filter(document_type=document_type)
        
        gl_account = self.request.query_params.get('gl_account', None)
        if gl_account:
            queryset = queryset.filter(gl_account__icontains=gl_account)
        
        profit_center = self.request.query_params.get('profit_center', None)
        if profit_center:
            queryset = queryset.filter(profit_center__icontains=profit_center)
        
        user_name = self.request.query_params.get('user_name', None)
        if user_name:
            queryset = queryset.filter(user_name__icontains=user_name)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(posting_date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(posting_date__lte=date_to)
        
        min_amount = self.request.query_params.get('min_amount', None)
        if min_amount:
            queryset = queryset.filter(amount_local_currency__gte=min_amount)
        
        max_amount = self.request.query_params.get('max_amount', None)
        if max_amount:
            queryset = queryset.filter(amount_local_currency__lte=max_amount)
        
        fiscal_year = self.request.query_params.get('fiscal_year', None)
        if fiscal_year:
            queryset = queryset.filter(fiscal_year=fiscal_year)
        
        return queryset
    
    @action(detail=False, methods=['get'])
    def statistics(self, request):
        """Get statistics for SAP GL postings"""
        queryset = self.get_queryset()
        
        stats = {
            'total_transactions': queryset.count(),
            'total_amount': float(queryset.aggregate(total=Sum('amount_local_currency'))['total'] or 0),
            'avg_amount': float(queryset.aggregate(avg=Avg('amount_local_currency'))['avg'] or 0),
            'high_value_transactions': queryset.filter(amount_local_currency__gt=1000000).count(),
            'unique_users': queryset.values('user_name').distinct().count(),
            'unique_accounts': queryset.values('gl_account').distinct().count(),
            'unique_profit_centers': queryset.values('profit_center').distinct().count(),
            'date_range': {
                'min_date': queryset.aggregate(min_date=Min('posting_date'))['min_date'],
                'max_date': queryset.aggregate(max_date=Max('posting_date'))['max_date']
            }
        }
        
        return Response(stats)
    
    @action(detail=False, methods=['get'])
    def top_users(self, request):
        """Get top users by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_users = queryset.values('user_name').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_users))
    
    @action(detail=False, methods=['get'])
    def top_accounts(self, request):
        """Get top G/L accounts by transaction count and amount"""
        queryset = self.get_queryset()
        
        top_accounts = queryset.values('gl_account').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return Response(list(top_accounts))

class DataFileViewSet(viewsets.ModelViewSet):
    """ViewSet for uploaded data files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    parser_classes = (MultiPartParser, FormParser)
    
    @action(detail=False, methods=['post'])
    def upload(self, request):
        """Upload and process CSV file"""
        try:
            file_obj = request.FILES.get('file')
            if not file_obj:
                return Response(
                    {'error': 'No file provided'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get additional fields from request data
            engagement_id = request.data.get('engagement_id', '')
            client_name = request.data.get('client_name', '')
            company_name = request.data.get('company_name', '')
            fiscal_year = request.data.get('fiscal_year', 2025)
            audit_start_date = request.data.get('audit_start_date')
            audit_end_date = request.data.get('audit_end_date')
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file in background (for now, process synchronously)
            result = self._process_csv_file(data_file, file_obj)
            
            if result['success']:
                return Response(
                    DataUploadResponseSerializer(data_file).data,
                    status=status.HTTP_201_CREATED
                )
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error uploading file: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_csv_file(self, data_file, file_obj):
        """Process uploaded CSV file"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            
            for row in csv_reader:
                try:
                    # Map CSV columns to model fields
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        
                        # Update date and amount ranges
                        if posting.posting_date:
                            if min_date is None or posting.posting_date < min_date:
                                min_date = posting.posting_date
                            if max_date is None or posting.posting_date > max_date:
                                max_date = posting.posting_date
                        
                        if min_amount is None or posting.amount_local_currency < min_amount:
                            min_amount = posting.amount_local_currency
                        if max_amount is None or posting.amount_local_currency > max_amount:
                            max_amount = posting.amount_local_currency
                except Exception as e:
                    logger.error(f"Error processing row: {e}")
                    failed_count += 1
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            return {'success': True}
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Map CSV columns to model fields
            # This mapping should be adjusted based on actual CSV structure
            posting = SAPGLPosting(
                document_number=row.get('Document', ''),
                document_type=row.get('Document type', ''),
                amount_local_currency=Decimal(row.get('Amount in Local Currency', '0').replace(',', '')),
                local_currency=row.get('Local Currency', 'SAR'),
                gl_account=row.get('G/L Account', ''),
                profit_center=row.get('Profit Center', ''),
                user_name=row.get('User Name', ''),
                fiscal_year=int(row.get('Fiscal Year', '2025')),
                posting_period=int(row.get('Posting period', '1')),
                text=row.get('Text', ''),
                segment=row.get('Segment', ''),
                clearing_document=row.get('Clearing Document', ''),
                offsetting_account=row.get('Offsetting', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                sales_document=row.get('Sales Document', ''),
                assignment=row.get('Assignment', ''),
                year_month=row.get('Year/Month', '')
            )
            
            # Parse dates
            posting_date_str = row.get('Posting Date', '')
            if posting_date_str:
                try:
                    parsed_date = datetime.strptime(posting_date_str, '%m/%d/%Y').date()
                    posting.posting_date = parsed_date
                except:
                    pass
            
            document_date_str = row.get('Document Date', '')
            if document_date_str:
                try:
                    parsed_date = datetime.strptime(document_date_str, '%m/%d/%Y').date()
                    posting.document_date = parsed_date
                except:
                    pass
            
            entry_date_str = row.get('Entry Date', '')
            if entry_date_str:
                try:
                    parsed_date = datetime.strptime(entry_date_str, '%m/%d/%Y').date()
                    posting.entry_date = parsed_date
                except:
                    pass
            
            return posting
            
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None

class AnalysisSessionViewSet(viewsets.ModelViewSet):
    """ViewSet for analysis sessions"""
    
    queryset = AnalysisSession.objects.all()
    serializer_class = AnalysisSessionSerializer
    
    def get_serializer_class(self):
        if self.action == 'create':
            return AnalysisSessionCreateSerializer
        return AnalysisSessionSerializer
    
    @action(detail=True, methods=['post'])
    def run_analysis(self, request, pk=None):
        """Run analysis for a session"""
        try:
            session = self.get_object()
            
            # Check if session is already running or completed
            if session.status in ['RUNNING', 'COMPLETED']:
                return Response(
                    {'error': f'Analysis session is already {session.status.lower()}'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Run analysis
            analyzer = SAPGLAnalyzer()
            result = analyzer.analyze_transactions(session)
            
            if 'error' in result:
                return Response(
                    {'error': result['error']},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            return Response(result)
            
        except Exception as e:
            logger.error(f"Error running analysis: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['get'])
    def summary(self, request, pk=None):
        """Get analysis summary"""
        try:
            session = self.get_object()
            analyzer = SAPGLAnalyzer()
            summary = analyzer.get_analysis_summary(session)
            
            return Response(AnalysisSummarySerializer(summary).data)
            
        except Exception as e:
            logger.error(f"Error getting summary: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class TransactionAnalysisViewSet(viewsets.ReadOnlyModelViewSet):
    """ViewSet for transaction analysis results"""
    
    queryset = TransactionAnalysis.objects.all()
    serializer_class = TransactionAnalysisSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return TransactionAnalysisListSerializer
        return TransactionAnalysisSerializer
    
    def get_queryset(self):
        queryset = TransactionAnalysis.objects.all()
        
        # Apply filters
        session_id = self.request.query_params.get('session_id', None)
        if session_id:
            queryset = queryset.filter(session_id=session_id)
        
        risk_level = self.request.query_params.get('risk_level', None)
        if risk_level:
            queryset = queryset.filter(risk_level=risk_level)
        
        min_risk_score = self.request.query_params.get('min_risk_score', None)
        if min_risk_score:
            queryset = queryset.filter(risk_score__gte=min_risk_score)
        
        max_risk_score = self.request.query_params.get('max_risk_score', None)
        if max_risk_score:
            queryset = queryset.filter(risk_score__lte=max_risk_score)
        
        # Filter by anomaly types
        amount_anomaly = self.request.query_params.get('amount_anomaly', None)
        if amount_anomaly:
            queryset = queryset.filter(amount_anomaly=amount_anomaly.lower() == 'true')
        
        timing_anomaly = self.request.query_params.get('timing_anomaly', None)
        if timing_anomaly:
            queryset = queryset.filter(timing_anomaly=timing_anomaly.lower() == 'true')
        
        user_anomaly = self.request.query_params.get('user_anomaly', None)
        if user_anomaly:
            queryset = queryset.filter(user_anomaly=user_anomaly.lower() == 'true')
        
        account_anomaly = self.request.query_params.get('account_anomaly', None)
        if account_anomaly:
            queryset = queryset.filter(account_anomaly=account_anomaly.lower() == 'true')
        
        pattern_anomaly = self.request.query_params.get('pattern_anomaly', None)
        if pattern_anomaly:
            queryset = queryset.filter(pattern_anomaly=pattern_anomaly.lower() == 'true')
        
        return queryset.order_by('-risk_score', '-created_at')

class GLAccountViewSet(viewsets.ModelViewSet):
    """ViewSet for GL Account management and analysis"""
    
    queryset = GLAccount.objects.all()
    serializer_class = GLAccountSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return GLAccountListSerializer
        return GLAccountSerializer
    
    def get_queryset(self):
        queryset = GLAccount.objects.all()
        
        # Apply filters
        account_id = self.request.query_params.get('account_id', None)
        if account_id:
            queryset = queryset.filter(account_id__icontains=account_id)
        
        account_name = self.request.query_params.get('account_name', None)
        if account_name:
            queryset = queryset.filter(account_name__icontains=account_name)
        
        account_type = self.request.query_params.get('account_type', None)
        if account_type:
            queryset = queryset.filter(account_type=account_type)
        
        account_category = self.request.query_params.get('account_category', None)
        if account_category:
            queryset = queryset.filter(account_category=account_category)
        
        normal_balance = self.request.query_params.get('normal_balance', None)
        if normal_balance:
            queryset = queryset.filter(normal_balance=normal_balance)
        
        is_active = self.request.query_params.get('is_active', None)
        if is_active:
            queryset = queryset.filter(is_active=is_active.lower() == 'true')
        
        return queryset.order_by('account_id')
    
    @action(detail=False, methods=['get'])
    def analysis(self, request):
        """Get comprehensive analysis for all GL accounts"""
        try:
            accounts = self.get_queryset()
            analysis_results = []
            
            print(f"Debug: Found {accounts.count()} GL accounts")  # Debug log
            
            for account in accounts:
                try:
                    # Get account postings using the gl_account field (not the relationship)
                    postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                    
                    print(f"Debug: Account {account.account_id} has {postings.count()} postings")  # Debug log
                    
                    if not postings.exists():
                        print(f"Debug: Account {account.account_id} has no postings, skipping")  # Debug log
                        continue
                    
                    # Calculate statistics
                    total_debits = postings.filter(transaction_type='DEBIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    total_credits = postings.filter(transaction_type='CREDIT').aggregate(
                        total=Sum('amount_local_currency')
                    )['total'] or Decimal('0.00')
                    
                    debit_count = postings.filter(transaction_type='DEBIT').count()
                    credit_count = postings.filter(transaction_type='CREDIT').count()
                    
                    # Risk analysis
                    high_value_count = postings.filter(amount_local_currency__gt=1000000).count()
                    
                    # Get analysis records for this account
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    flagged_count = analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    
                    # Calculate average risk score
                    avg_risk_score = analysis_records.aggregate(
                        avg_score=Avg('risk_score')
                    )['avg_score'] or 0.0
                    
                    # Activity analysis
                    first_transaction = postings.order_by('posting_date').first()
                    last_transaction = postings.order_by('-posting_date').first()
                    
                    avg_amount = postings.aggregate(
                        avg_amount=Avg('amount_local_currency')
                    )['avg_amount'] or Decimal('0.00')
                    
                    max_amount = postings.aggregate(
                        max_amount=Max('amount_local_currency')
                    )['max_amount'] or Decimal('0.00')
                    
                    analysis_results.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'account_type': account.account_type,
                        'account_category': account.account_category,
                        'normal_balance': account.normal_balance,
                        'current_balance': float(account.current_balance),
                        'total_debits': float(total_debits),
                        'total_credits': float(total_credits),
                        'transaction_count': postings.count(),
                        'debit_count': debit_count,
                        'credit_count': credit_count,
                        'high_value_transactions': high_value_count,
                        'flagged_transactions': flagged_count,
                        'risk_score': round(avg_risk_score, 2),
                        'first_transaction_date': first_transaction.posting_date if first_transaction else None,
                        'last_transaction_date': last_transaction.posting_date if last_transaction else None,
                        'avg_transaction_amount': float(avg_amount),
                        'max_transaction_amount': float(max_amount)
                    })
                    
                    print(f"Debug: Added analysis for account {account.account_id}")  # Debug log
                    
                except Exception as account_error:
                    logger.error(f"Error processing account {account.account_id}: {account_error}")
                    print(f"Debug: Error processing account {account.account_id}: {account_error}")  # Debug log
                    continue
            
            print(f"Debug: Total analysis results: {len(analysis_results)}")  # Debug log
            
            # Sort by risk score (highest first)
            analysis_results.sort(key=lambda x: x['risk_score'], reverse=True)
            
            return Response(analysis_results)
            
        except Exception as e:
            logger.error(f"Error in GL account analysis: {e}")
            print(f"Debug: GL account analysis error: {e}")  # Debug log
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def trial_balance(self, request):
        """Generate Trial Balance report"""
        try:
            # Get date range from request
            date_from = request.query_params.get('date_from', None)
            date_to = request.query_params.get('date_to', None)
            
            accounts = self.get_queryset()
            trial_balance_data = []
            
            for account in accounts:
                # Get postings for the account using gl_account field
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                if date_from:
                    postings = postings.filter(posting_date__gte=date_from)
                if date_to:
                    postings = postings.filter(posting_date__lte=date_to)
                
                if not postings.exists():
                    continue
                
                # Calculate period movements
                period_debit = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                period_credit = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                # Calculate closing balances
                if account.normal_balance == 'DEBIT':
                    closing_debit = period_debit - period_credit
                    closing_credit = Decimal('0.00')
                    if closing_debit < 0:
                        closing_credit = abs(closing_debit)
                        closing_debit = Decimal('0.00')
                else:
                    closing_credit = period_credit - period_debit
                    closing_debit = Decimal('0.00')
                    if closing_credit < 0:
                        closing_debit = abs(closing_credit)
                        closing_credit = Decimal('0.00')
                
                # Net balance
                net_balance = closing_debit - closing_credit
                
                trial_balance_data.append({
                    'account_id': account.account_id,
                    'account_name': account.account_name,
                    'account_type': account.account_type,
                    'account_category': account.account_category,
                    'normal_balance': account.normal_balance,
                    'opening_debit': float(Decimal('0.00')),  # Assuming no opening balances for now
                    'opening_credit': float(Decimal('0.00')),
                    'period_debit': float(period_debit),
                    'period_credit': float(period_credit),
                    'closing_debit': float(closing_debit),
                    'closing_credit': float(closing_credit),
                    'net_balance': float(net_balance),
                    'transaction_count': postings.count()
                })
            
            # Sort by account type and account ID
            trial_balance_data.sort(key=lambda x: (x['account_type'], x['account_id']))
            
            return Response(trial_balance_data)
            
        except Exception as e:
            logger.error(f"Error generating trial balance: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def charts(self, request):
        """Get charts data for GL accounts"""
        try:
            accounts = self.get_queryset()
            
            # Account type distribution
            type_distribution = {}
            category_distribution = {}
            
            for account in accounts:
                # Get postings for this account
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                
                # Type distribution
                if account.account_type not in type_distribution:
                    type_distribution[account.account_type] = {
                        'count': 0,
                        'total_balance': Decimal('0.00'),
                        'total_transactions': 0
                    }
                type_distribution[account.account_type]['count'] += 1
                type_distribution[account.account_type]['total_balance'] += account.current_balance
                type_distribution[account.account_type]['total_transactions'] += postings.count()
                
                # Category distribution
                if account.account_category not in category_distribution:
                    category_distribution[account.account_category] = {
                        'count': 0,
                        'total_balance': Decimal('0.00')
                    }
                category_distribution[account.account_category]['count'] += 1
                category_distribution[account.account_category]['total_balance'] += account.current_balance
            
            # Top accounts by balance
            top_accounts_by_balance = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                if postings.exists():
                    top_accounts_by_balance.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'balance': float(account.current_balance),
                        'account_type': account.account_type
                    })
            
            top_accounts_by_balance.sort(key=lambda x: abs(x['balance']), reverse=True)
            top_accounts_by_balance = top_accounts_by_balance[:10]
            
            # Top accounts by transaction count
            top_accounts_by_transactions = []
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                transaction_count = postings.count()
                if transaction_count > 0:
                    top_accounts_by_transactions.append({
                        'account_id': account.account_id,
                        'account_name': account.account_name,
                        'transaction_count': transaction_count,
                        'account_type': account.account_type
                    })
            
            top_accounts_by_transactions.sort(key=lambda x: x['transaction_count'], reverse=True)
            top_accounts_by_transactions = top_accounts_by_transactions[:10]
            
            # Debit vs Credit analysis
            total_debits = Decimal('0.00')
            total_credits = Decimal('0.00')
            debit_count = 0
            credit_count = 0
            
            for account in accounts:
                postings = SAPGLPosting.objects.filter(gl_account=account.account_id)
                account_debits = postings.filter(transaction_type='DEBIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                account_credits = postings.filter(transaction_type='CREDIT').aggregate(
                    total=Sum('amount_local_currency')
                )['total'] or Decimal('0.00')
                
                total_debits += account_debits
                total_credits += account_credits
                debit_count += postings.filter(transaction_type='DEBIT').count()
                credit_count += postings.filter(transaction_type='CREDIT').count()
            
            debit_credit_analysis = {
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'debit_count': debit_count,
                'credit_count': credit_count,
                'net_movement': float(total_debits - total_credits)
            }
            
            # Risk distribution by account type
            risk_distribution = []
            for account_type, data in type_distribution.items():
                accounts_of_type = accounts.filter(account_type=account_type)
                high_risk_count = 0
                total_risk_score = 0
                
                for account in accounts_of_type:
                    analysis_records = TransactionAnalysis.objects.filter(
                        transaction__gl_account=account.account_id
                    )
                    high_risk_count += analysis_records.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
                    avg_score = analysis_records.aggregate(avg_score=Avg('risk_score'))['avg_score'] or 0
                    total_risk_score += avg_score
                
                avg_risk_score = total_risk_score / len(accounts_of_type) if accounts_of_type.exists() else 0
                
                risk_distribution.append({
                    'account_type': account_type,
                    'account_count': data['count'],
                    'high_risk_count': high_risk_count,
                    'avg_risk_score': round(avg_risk_score, 2)
                })
            
            charts_data = {
                'account_type_distribution': [
                    {
                        'account_type': account_type,
                        'count': data['count'],
                        'total_balance': float(data['total_balance']),
                        'total_transactions': data['total_transactions']
                    }
                    for account_type, data in type_distribution.items()
                ],
                'account_category_distribution': [
                    {
                        'category': category,
                        'count': data['count'],
                        'total_balance': float(data['total_balance'])
                    }
                    for category, data in category_distribution.items()
                ],
                'top_accounts_by_balance': top_accounts_by_balance,
                'top_accounts_by_transactions': top_accounts_by_transactions,
                'debit_credit_analysis': debit_credit_analysis,
                'risk_distribution': risk_distribution
            }
            
            return Response(charts_data)
            
        except Exception as e:
            logger.error(f"Error generating charts data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['post'])
    def upload_master_data(self, request):
        """Upload GL Account master data from CSV"""
        try:
            serializer = GLAccountUploadSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            file_obj = serializer.validated_data['file']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Process CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            created_count = 0
            updated_count = 0
            failed_count = 0
            
            for row in csv_reader:
                try:
                    account_id = row.get('Account ID', '').strip()
                    if not account_id:
                        failed_count += 1
                        continue
                    
                    account_data = {
                        'account_name': row.get('Account Name', '').strip(),
                        'account_type': row.get('Account Type', '').strip(),
                        'account_category': row.get('Account Category', '').strip(),
                        'account_subcategory': row.get('Account Subcategory', '').strip(),
                        'normal_balance': row.get('Normal Balance', 'DEBIT').strip().upper(),
                        'is_active': row.get('Is Active', 'TRUE').strip().upper() == 'TRUE'
                    }
                    
                    # Create or update account
                    account, created = GLAccount.objects.update_or_create(
                        account_id=account_id,
                        defaults=account_data
                    )
                    
                    if created:
                        created_count += 1
                    else:
                        updated_count += 1
                        
                except Exception as e:
                    logger.error(f"Error processing GL account row: {e}")
                    failed_count += 1
            
            return Response({
                'message': 'GL Account master data uploaded successfully',
                'created_count': created_count,
                'updated_count': updated_count,
                'failed_count': failed_count
            })
            
        except Exception as e:
            logger.error(f"Error uploading GL account master data: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class DashboardView(generics.GenericAPIView):
    """Dashboard view for system overview"""
    
    def get(self, request):
        """Get dashboard statistics"""
        try:
            # Date range (last 30 days)
            end_date = timezone.now().date()
            start_date = end_date - timedelta(days=30)
            
            # Overall statistics
            total_transactions = SAPGLPosting.objects.count()
            total_amount = SAPGLPosting.objects.aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0')
            total_files = DataFile.objects.count()
            total_sessions = AnalysisSession.objects.count()
            
            # Recent activity
            recent_transactions = SAPGLPosting.objects.filter(
                created_at__date__gte=start_date
            ).count()
            recent_files = DataFile.objects.filter(
                uploaded_at__date__gte=start_date
            ).count()
            recent_sessions = AnalysisSession.objects.filter(
                created_at__date__gte=start_date
            ).count()
            
            # Risk statistics
            flagged_transactions = TransactionAnalysis.objects.filter(
                risk_level__in=['HIGH', 'CRITICAL']
            ).count()
            high_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='HIGH'
            ).count()
            critical_risk_transactions = TransactionAnalysis.objects.filter(
                risk_level='CRITICAL'
            ).count()
            
            # Top statistics
            top_users = SAPGLPosting.objects.values('user_name').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_accounts = SAPGLPosting.objects.values('gl_account').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_profit_centers = SAPGLPosting.objects.values('profit_center').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            top_document_types = SAPGLPosting.objects.values('document_type').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:5]
            
            stats = {
                'total_transactions': total_transactions,
                'total_amount': float(total_amount),
                'total_files': total_files,
                'total_sessions': total_sessions,
                'recent_transactions': recent_transactions,
                'recent_files': recent_files,
                'recent_sessions': recent_sessions,
                'flagged_transactions': flagged_transactions,
                'high_risk_transactions': high_risk_transactions,
                'critical_risk_transactions': critical_risk_transactions,
                'top_users': list(top_users),
                'top_accounts': list(top_accounts),
                'top_profit_centers': list(top_profit_centers),
                'top_document_types': list(top_document_types),
                'date_from': start_date,
                'date_to': end_date
            }
            
            return Response(DashboardStatsSerializer(stats).data)
            
        except Exception as e:
            logger.error(f"Error getting dashboard stats: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileListView(generics.ListAPIView):
    """View to get list of uploaded files"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    
    def get_queryset(self):
        queryset = DataFile.objects.all()
        
        # Apply filters
        status_filter = self.request.query_params.get('status', None)
        if status_filter:
            queryset = queryset.filter(status=status_filter)
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(uploaded_at__date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(uploaded_at__date__lte=date_to)
        
        return queryset.order_by('-uploaded_at')
    
    def list(self, request, *args, **kwargs):
        queryset = self.get_queryset()
        serializer = self.get_serializer(queryset, many=True)
        
        # Get file data with session information
        files_data = []
        serializer_data = serializer.data
        
        for i, file_obj in enumerate(queryset):
            file_data = serializer_data[i].copy()  # Make a copy to avoid modifying the original
            
            # Get related analysis sessions for this file
            related_sessions = AnalysisSession.objects.filter(
                created_at__gte=file_obj.uploaded_at
            ).order_by('-created_at')
            
            # Add session information to file data
            file_data['sessions'] = []
            for session in related_sessions:
                session_data = {
                    'id': str(session.id),
                    'session_name': session.session_name,
                    'status': session.status,
                    'created_at': session.created_at,
                    'started_at': session.started_at,
                    'completed_at': session.completed_at,
                    'total_transactions': session.total_transactions,
                    'flagged_transactions': session.flagged_transactions,
                    'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions > 0 else 0
                }
                file_data['sessions'].append(session_data)
            
            files_data.append(file_data)
        
        # Add summary statistics
        total_files = queryset.count()
        total_records = queryset.aggregate(total=Sum('total_records'))['total'] or 0
        total_processed = queryset.aggregate(total=Sum('processed_records'))['total'] or 0
        total_failed = queryset.aggregate(total=Sum('failed_records'))['total'] or 0
        
        # Calculate total sessions across all files
        total_sessions = sum(len(file_data['sessions']) for file_data in files_data)
        
        response_data = {
            'files': files_data,
            'summary': {
                'total_files': total_files,
                'total_records': total_records,
                'total_processed': total_processed,
                'total_failed': total_failed,
                'total_sessions': total_sessions,
                'success_rate': (total_processed / total_records * 100) if total_records > 0 else 0
            }
        }
        
        return Response(response_data)


class FileUploadAnalysisView(generics.CreateAPIView):
    """View to upload file and run analysis"""
    
    parser_classes = (MultiPartParser, FormParser)
    serializer_class = DataFileUploadSerializer
    
    def create(self, request, *args, **kwargs):
        try:
            # Validate the request using serializer
            serializer = self.get_serializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Get the validated data
            file_obj = serializer.validated_data['file']
            description = serializer.validated_data.get('description', '')
            engagement_id = serializer.validated_data['engagement_id']
            client_name = serializer.validated_data['client_name']
            company_name = serializer.validated_data['company_name']
            fiscal_year = serializer.validated_data['fiscal_year']
            audit_start_date = serializer.validated_data['audit_start_date']
            audit_end_date = serializer.validated_data['audit_end_date']
            
            # Validate file type
            if not file_obj.name.endswith('.csv'):
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Create DataFile record with new fields
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            
            # Process file and run analysis
            result = self._process_and_analyze_file(data_file, file_obj)
            
            if result['success']:
                response_data = {
                        'file': DataFileSerializer(data_file).data,
                        'analysis': result.get('analysis', {}),
                    'processing_summary': result.get('processing_summary', {}),
                        'message': 'File uploaded and analysis completed successfully'
                }
                
                # Add validation errors if any
                if 'validation_errors' in result:
                    response_data['validation_errors'] = result['validation_errors']
                    response_data['validation_errors_summary'] = result.get('validation_errors_summary', '')
                
                return Response(response_data, status=status.HTTP_201_CREATED)
            else:
                return Response(
                    {'error': result['error']}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
                
        except Exception as e:
            logger.error(f"Error in file upload and analysis: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _process_and_analyze_file(self, data_file, file_obj):
        """Process uploaded CSV file and run analysis"""
        try:
            # Update status
            data_file.status = 'PROCESSING'
            data_file.save()
            
            # Keep existing transactions - don't delete them
            logger.info("Processing new file while keeping existing transactions")
            
            # Read CSV file
            content = file_obj.read().decode('utf-8')
            csv_reader = csv.DictReader(io.StringIO(content))
            
            # Process rows
            processed_count = 0
            failed_count = 0
            min_date = None
            max_date = None
            min_amount = None
            max_amount = None
            new_transactions = []
            validation_errors = []
            row_number = 0
            
            for row in csv_reader:
                row_number += 1
                try:
                    # Map CSV columns to model fields
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        new_transactions.append(posting)
                        
                        # Update date and amount ranges
                        if posting.posting_date:
                            if min_date is None or posting.posting_date < min_date:
                                min_date = posting.posting_date
                            if max_date is None or posting.posting_date > max_date:
                                max_date = posting.posting_date
                        
                        if min_amount is None or posting.amount_local_currency < min_amount:
                            min_amount = posting.amount_local_currency
                        if max_amount is None or posting.amount_local_currency > max_amount:
                            max_amount = posting.amount_local_currency
                except ValueError as e:
                    # This is a validation error - collect details
                    error_info = {
                        'row_number': row_number,
                        'error': str(e),
                        'complete_row_data': dict(row),  # Include all row data
                        'document_number': row.get('Document Number', 'N/A'),
                        'gl_account': row.get('G/L Account', 'N/A'),
                        'amount': row.get('Amount in Local Currency', 'N/A'),
                        'user_name': row.get('User Name', 'N/A')
                    }
                    validation_errors.append(error_info)
                    logger.error(f"Validation error in row {row_number}: {e}")
                    logger.error(f"Complete row data: {dict(row)}")
                    failed_count += 1
                except Exception as e:
                    # This is a general processing error
                    error_info = {
                        'row_number': row_number,
                        'error': f"Processing error: {str(e)}",
                        'complete_row_data': dict(row),  # Include all row data
                        'document_number': row.get('Document Number', 'N/A'),
                        'gl_account': row.get('G/L Account', 'N/A'),
                        'amount': row.get('Amount in Local Currency', 'N/A'),
                        'user_name': row.get('User Name', 'N/A')
                    }
                    validation_errors.append(error_info)
                    logger.error(f"Error processing row {row_number}: {e}")
                    logger.error(f"Complete row data: {dict(row)}")
                    failed_count += 1
            
            # Save all transactions in batch
            if new_transactions:
                SAPGLPosting.objects.bulk_create(new_transactions)  # type: ignore
                logger.info(f"Saved {len(new_transactions)} transactions in batch")
            
            # Update DataFile record
            data_file.total_records = processed_count + failed_count
            data_file.processed_records = processed_count
            data_file.failed_records = failed_count
            data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
            data_file.processed_at = timezone.now()
            data_file.min_date = min_date
            data_file.max_date = max_date
            data_file.min_amount = min_amount
            data_file.max_amount = max_amount
            data_file.save()
            
            # Initialize analyzer
            analyzer = SAPGLAnalyzer()
            
            # Run analysis if we have processed transactions
            analysis_result = {}
            if processed_count > 0:
                analysis_result = self._run_file_analysis(analyzer, data_file, new_transactions)
            
            # Prepare response
            response_data = {
                'success': True,
                'analysis': analysis_result,
                'processing_summary': {
                    'total_rows': processed_count + failed_count,
                    'processed_count': processed_count,
                    'failed_count': failed_count,
                    'success_rate': (processed_count / (processed_count + failed_count) * 100) if (processed_count + failed_count) > 0 else 0
                }
            }
            
            # Add validation errors if any
            if validation_errors:
                response_data['validation_errors'] = validation_errors[:10]  # Limit to first 10 errors
                if len(validation_errors) > 10:
                    response_data['validation_errors_summary'] = f"Showing first 10 of {len(validation_errors)} validation errors"
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            data_file.status = 'FAILED'
            data_file.error_message = str(e)
            data_file.processed_at = timezone.now()
            data_file.save()
            return {'success': False, 'error': str(e)}
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row"""
        try:
            # Define required fields and their display names
            required_fields = {
                'Document Number': row.get('Document Number', '').strip(),
                'G/L Account': row.get('G/L Account', '').strip(),
                'Amount in Local Currency': row.get('Amount in Local Currency', '').strip(),
                'Posting Date': row.get('Posting Date', '').strip(),
                'User Name': row.get('User Name', '').strip()
            }
            
            # Check for missing required fields
            missing_fields = []
            print(required_fields)
            for field_name, field_value in required_fields.items():
                if not field_value:
                    missing_fields.append(field_name)
            
            # If any required fields are missing, raise an error
            if missing_fields:
                error_msg = f"Missing required fields: {', '.join(missing_fields)}"
                logger.error(f"Row validation failed: {error_msg}")
                logger.error(f"Complete row data for debugging: {dict(row)}")
                raise ValueError(error_msg)
            
            # Parse dates with multiple format support
            posting_date = self._parse_date(row.get('Posting Date', ''))
            document_date = self._parse_date(row.get('Document Date', ''))
            entry_date = self._parse_date(row.get('Entry Date', ''))
            
            # Parse amount with proper error handling
            amount_str = row.get('Amount in Local Currency', '0')
            try:
                # Clean the amount string - remove commas, spaces, and currency symbols
                amount_str = str(amount_str).replace(',', '').replace(' ', '').replace('SAR', '').replace('$', '').strip()
                if not amount_str or amount_str == '':
                    raise ValueError("Amount cannot be empty")
                amount_local_currency = Decimal(amount_str)
                if amount_local_currency <= 0:
                    raise ValueError("Amount must be greater than zero")
            except (ValueError, TypeError, InvalidOperation) as e:
                error_msg = f"Invalid amount format: {amount_str}. Error: {str(e)}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Determine transaction type (DEBIT/CREDIT)
            # Check if there's a specific transaction type field
            transaction_type = row.get('Transaction Type', '').strip().upper()
            if not transaction_type:
                # Try to determine from amount (negative = credit, positive = debit)
                # Or use a default based on account type
                if amount_local_currency < 0:
                    transaction_type = 'CREDIT'
                    amount_local_currency = abs(amount_local_currency)  # Make amount positive
                else:
                    transaction_type = 'DEBIT'
            elif transaction_type not in ['DEBIT', 'CREDIT']:
                transaction_type = 'DEBIT'  # Default to debit
            
            # Parse fiscal year and posting period with error handling
            try:
                fiscal_year = int(row.get('Fiscal Year', '0'))
            except (ValueError, TypeError):
                fiscal_year = 2025  # Default value
                
            try:
                posting_period = int(row.get('Posting Period', '0'))
            except (ValueError, TypeError):
                posting_period = 1  # Default value
            
            # Map CSV columns to model fields
            posting = SAPGLPosting(
                document_number=row.get('Document Number', ''),
                posting_date=posting_date,
                gl_account=row.get('G/L Account', ''),
                amount_local_currency=amount_local_currency,
                transaction_type=transaction_type,
                local_currency=row.get('Local Currency', 'SAR'),
                text=row.get('Text', ''),
                document_date=document_date,
                offsetting_account=row.get('Offsetting Account', ''),
                user_name=row.get('User Name', ''),
                entry_date=entry_date,
                document_type=row.get('Document Type', ''),
                profit_center=row.get('Profit Center', ''),
                cost_center=row.get('Cost Center', ''),
                clearing_document=row.get('Clearing Document', ''),
                segment=row.get('Segment', ''),
                wbs_element=row.get('WBS Element', ''),
                plant=row.get('Plant', ''),
                material=row.get('Material', ''),
                invoice_reference=row.get('Invoice Reference', ''),
                billing_document=row.get('Billing Document', ''),
                sales_document=row.get('Sales Document', ''),
                purchasing_document=row.get('Purchasing Document', ''),
                order_number=row.get('Order Number', ''),
                asset_number=row.get('Asset Number', ''),
                network=row.get('Network', ''),
                assignment=row.get('Assignment', ''),
                tax_code=row.get('Tax Code', ''),
                account_assignment=row.get('Account Assignment', ''),
                fiscal_year=fiscal_year,
                posting_period=posting_period,
                year_month=row.get('Year/Month', '')
            )
            return posting
        except Exception as e:
            logger.error(f"Error creating posting from row: {e}")
            return None
    
    def _parse_date(self, date_str):
        """Parse date string with multiple format support"""
        if not date_str:
            return None
        
        date_formats = [
            '%Y-%m-%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%Y/%m/%d',
            '%m-%d-%Y',
            '%d-%m-%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"Could not parse date: {date_str}")
        return None
    

    
    def _run_file_analysis(self, analyzer, data_file, transactions):
        """Run analysis on the uploaded file data"""
        try:
            # Create analysis session
            session = AnalysisSession.objects.create(
                session_name=f"Analysis for {data_file.file_name}",
                description=f"Automated analysis for uploaded file {data_file.file_name}",
                date_from=data_file.min_date,
                date_to=data_file.max_date,
                min_amount=data_file.min_amount,
                max_amount=data_file.max_amount,
                status='RUNNING'
            )
            
            # Run analysis using the session
            analysis_results = analyzer.analyze_transactions(session)
            
            if 'error' in analysis_results:
                return {
                    'error': analysis_results['error'],
                    'analysis_status': 'FAILED'
                }
            
            # Get analysis summary
            summary = analyzer.get_analysis_summary(session)
            
            return {
                'session_id': str(session.id),
                'total_transactions': summary.get('total_transactions', 0),
                'total_amount': summary.get('total_amount', 0.0),
                'flagged_transactions': summary.get('flagged_transactions', 0),
                'high_value_transactions': summary.get('high_value_transactions', 0),
                'analysis_status': session.status,
                'flag_rate': summary.get('flag_rate', 0.0),
                'anomaly_summary': summary.get('anomaly_summary', {}),
                'risk_distribution': summary.get('risk_distribution', [])
            }
            
        except Exception as e:
            logger.error(f"Error running file analysis: {e}")
            return {
                'error': str(e),
                'analysis_status': 'FAILED'
            }


class FileSummaryView(generics.RetrieveAPIView):
    """View to get summary by file ID"""
    
    queryset = DataFile.objects.all()
    serializer_class = DataFileSerializer
    lookup_field = 'id'
    
    def retrieve(self, request, *args, **kwargs):
        data_file = self.get_object()
        serializer = self.get_serializer(data_file)
        
        # Get related data - get all transactions first for statistics
        all_related_transactions = SAPGLPosting.objects.filter(
            created_at__gte=data_file.uploaded_at
        )
        
        # Get limited transactions for display
        related_transactions = all_related_transactions.order_by('-created_at')[:100]
        
        # Calculate additional statistics using the full dataset
        total_amount = all_related_transactions.aggregate(
            total=Sum('amount_local_currency')
        )['total'] or Decimal('0.00')
        
        # Get currency information from transactions
        currency_info = all_related_transactions.values('local_currency').distinct()
        primary_currency = currency_info.first()['local_currency'] if currency_info.exists() else 'SAR'
        
        unique_users = all_related_transactions.values('user_name').distinct().count()
        unique_accounts = all_related_transactions.values('gl_account').distinct().count()
        unique_profit_centers = all_related_transactions.values('profit_center').distinct().count()
        
        # Get analysis sessions for this file
        analysis_sessions = AnalysisSession.objects.filter(
            created_at__gte=data_file.uploaded_at
        ).order_by('-created_at')
        
        # Get transaction analysis records for this file's transactions
        transaction_analyses = TransactionAnalysis.objects.filter(
            transaction__in=all_related_transactions
        )
        
        # If no analysis records exist, run real-time anomaly detection
        if not transaction_analyses.exists():
            # Run real-time anomaly detection
            analyzer = SAPGLAnalyzer()
            transactions_list = list(all_related_transactions)
            
            # Run all anomaly tests
            duplicate_anomalies = analyzer.detect_duplicate_entries(transactions_list)
            user_anomalies = analyzer.detect_user_anomalies(transactions_list)
            backdated_anomalies = analyzer.detect_backdated_entries(transactions_list)
            closing_anomalies = analyzer.detect_closing_entries(transactions_list)
            unusual_day_anomalies = analyzer.detect_unusual_days(transactions_list)
            holiday_anomalies = analyzer.detect_holiday_entries(transactions_list)
            
            # Count anomalies by type - only the 6 specific types we're implementing
            duplicate_count = 0
            user_anomaly_count = 0
            backdated_count = 0
            closing_count = 0
            unusual_days_count = 0
            holiday_count = 0
            
            # 1) Count Duplicate Entries
            if duplicate_anomalies:
                duplicate_transaction_ids = set()
                for dup in duplicate_anomalies:
                    for transaction in dup.get('transactions', []):
                        duplicate_transaction_ids.add(transaction['id'])
                duplicate_count = len(duplicate_transaction_ids)
            
            # 2) Count User Analysis anomalies
            if user_anomalies:
                user_anomaly_users = set()
                for anomaly in user_anomalies:
                    user_anomaly_users.add(anomaly.get('user_name'))
                user_anomaly_count = len(user_anomaly_users)
            
            # 3) Count Backdated Entries
            if backdated_anomalies:
                backdated_transaction_ids = set()
                for anomaly in backdated_anomalies:
                    backdated_transaction_ids.add(anomaly.get('transaction_id'))
                backdated_count = len(backdated_transaction_ids)
            
            # 4) Count Closing Entries
            if closing_anomalies:
                closing_transaction_ids = set()
                for anomaly in closing_anomalies:
                    closing_transaction_ids.add(anomaly.get('transaction_id'))
                closing_count = len(closing_transaction_ids)
            
            # 5) Count Unusual Days
            if unusual_day_anomalies:
                unusual_days_transaction_ids = set()
                for anomaly in unusual_day_anomalies:
                    unusual_days_transaction_ids.add(anomaly.get('transaction_id'))
                unusual_days_count = len(unusual_days_transaction_ids)
            
            # 6) Count Holiday Entries
            if holiday_anomalies:
                holiday_transaction_ids = set()
                for anomaly in holiday_anomalies:
                    holiday_transaction_ids.add(anomaly.get('transaction_id'))
                holiday_count = len(holiday_transaction_ids)
            
            # Calculate total anomalies (sum of all 6 types)
            total_anomalies = duplicate_count + user_anomaly_count + backdated_count + closing_count + unusual_days_count + holiday_count
            
            # Calculate total anomalies
            total_anomalies = amount_anomaly_count + timing_anomaly_count + user_anomaly_count + account_anomaly_count + pattern_anomaly_count
            
            # Create risk distribution based on anomaly counts
            total_transactions = all_related_transactions.count()
            risk_distribution = [
                {
                    'risk_level': 'LOW',
                    'count': total_transactions - total_anomalies,
                    'percentage': round(((total_transactions - total_anomalies) / total_transactions * 100), 2) if total_transactions > 0 else 0
                },
                {
                    'risk_level': 'MEDIUM',
                    'count': 0,
                    'percentage': 0
                },
                {
                    'risk_level': 'HIGH',
                    'count': total_anomalies,
                    'percentage': round((total_anomalies / total_transactions * 100), 2) if total_transactions > 0 else 0
                },
                {
                    'risk_level': 'CRITICAL',
                    'count': 0,
                    'percentage': 0
                }
            ]
            
            anomaly_summary = {
                'duplicate_entries': duplicate_count,
                'user_anomalies': user_anomaly_count,
                'backdated_entries': backdated_count,
                'closing_entries': closing_count,
                'unusual_days': unusual_days_count,
                'holiday_entries': holiday_count,
                'total_anomalies': total_anomalies
            }
            
        else:
            # Use existing analysis records
            # Calculate risk distribution
            risk_distribution = [
                {
                    'risk_level': 'LOW',
                    'count': transaction_analyses.filter(risk_level='LOW').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='LOW').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                },
                {
                    'risk_level': 'MEDIUM',
                    'count': transaction_analyses.filter(risk_level='MEDIUM').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='MEDIUM').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                },
                {
                    'risk_level': 'HIGH',
                    'count': transaction_analyses.filter(risk_level='HIGH').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='HIGH').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                },
                {
                    'risk_level': 'CRITICAL',
                    'count': transaction_analyses.filter(risk_level='CRITICAL').count(),
                    'percentage': round((transaction_analyses.filter(risk_level='CRITICAL').count() / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
                }
            ]
            
            # Calculate anomaly summary - use same structure as real-time detection
            # For existing analysis records, we need to run real-time detection to get accurate counts
            # since the boolean flags don't map directly to our 6 specific types
            
            # Run real-time anomaly detection to get accurate counts
            analyzer = SAPGLAnalyzer()
            transactions_list = list(all_related_transactions)
            
            # Run all anomaly tests
            duplicate_anomalies = analyzer.detect_duplicate_entries(transactions_list)
            user_anomalies = analyzer.detect_user_anomalies(transactions_list)
            backdated_anomalies = analyzer.detect_backdated_entries(transactions_list)
            closing_anomalies = analyzer.detect_closing_entries(transactions_list)
            unusual_day_anomalies = analyzer.detect_unusual_days(transactions_list)
            holiday_anomalies = analyzer.detect_holiday_entries(transactions_list)
            
            # Count anomalies by type - only the 6 specific types we're implementing
            duplicate_count = 0
            user_anomaly_count = 0
            backdated_count = 0
            closing_count = 0
            unusual_days_count = 0
            holiday_count = 0
            
            # 1) Count Duplicate Entries
            if duplicate_anomalies:
                duplicate_transaction_ids = set()
                for dup in duplicate_anomalies:
                    for transaction in dup.get('transactions', []):
                        duplicate_transaction_ids.add(transaction['id'])
                duplicate_count = len(duplicate_transaction_ids)
            
            # 2) Count User Analysis anomalies
            if user_anomalies:
                user_anomaly_users = set()
                for anomaly in user_anomalies:
                    user_anomaly_users.add(anomaly.get('user_name'))
                user_anomaly_count = len(user_anomaly_users)
            
            # 3) Count Backdated Entries
            if backdated_anomalies:
                backdated_transaction_ids = set()
                for anomaly in backdated_anomalies:
                    backdated_transaction_ids.add(anomaly.get('transaction_id'))
                backdated_count = len(backdated_transaction_ids)
            
            # 4) Count Closing Entries
            if closing_anomalies:
                closing_transaction_ids = set()
                for anomaly in closing_anomalies:
                    closing_transaction_ids.add(anomaly.get('transaction_id'))
                closing_count = len(closing_transaction_ids)
            
            # 5) Count Unusual Days
            if unusual_day_anomalies:
                unusual_days_transaction_ids = set()
                for anomaly in unusual_day_anomalies:
                    unusual_days_transaction_ids.add(anomaly.get('transaction_id'))
                unusual_days_count = len(unusual_days_transaction_ids)
            
            # 6) Count Holiday Entries
            if holiday_anomalies:
                holiday_transaction_ids = set()
                for anomaly in holiday_anomalies:
                    holiday_transaction_ids.add(anomaly.get('transaction_id'))
                holiday_count = len(holiday_transaction_ids)
            
            # Calculate total anomalies (sum of all 6 types)
            total_anomalies = duplicate_count + user_anomaly_count + backdated_count + closing_count + unusual_days_count + holiday_count
            
            anomaly_summary = {
                'duplicate_entries': duplicate_count,
                'user_anomalies': user_anomaly_count,
                'backdated_entries': backdated_count,
                'closing_entries': closing_count,
                'unusual_days': unusual_days_count,
                'holiday_entries': holiday_count,
                'total_anomalies': total_anomalies
            }
        
        # Get detailed anomaly data
        duplicate_entries = []
        user_anomalies = []
        backdated_entries = []
        closing_entries = []
        unusual_days = []
        holiday_entries = []
        
        # Use the anomaly detection results from above if available, otherwise run detection
        if not transaction_analyses.exists():
            # Use the results from real-time detection above
            duplicate_entries = duplicate_anomalies[:10]  # Limit to top 10
            
            # Convert user anomalies to the expected format
            for anomaly in user_anomalies:
                user_anomalies.append({
                    'user_name': anomaly.get('user_name'),
                    'anomaly_count': anomaly.get('statistics', {}).get('transaction_count', 0),
                    'avg_risk_score': anomaly.get('risk_score', 0)
                })
            
            # Convert timing anomalies to the expected format
            for anomaly in backdated_anomalies:
                backdated_entries.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
            
            for anomaly in closing_anomalies:
                closing_entries.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
            
            for anomaly in unusual_day_anomalies:
                unusual_days.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
            
            for anomaly in holiday_anomalies:
                holiday_entries.append({
                    'document_number': anomaly.get('document_number'),
                    'posting_date': anomaly.get('posting_date'),
                    'amount_local_currency': anomaly.get('amount'),
                    'currency': anomaly.get('currency', 'SAR'),
                    'user_name': anomaly.get('user_name')
                })
        else:
            # Use existing analysis records for detailed data
            if all_related_transactions.exists():
                analyzer = SAPGLAnalyzer()
                duplicates = analyzer.detect_duplicate_entries(list(all_related_transactions))
                duplicate_entries = duplicates[:10]  # Limit to top 10
            
            # Get user anomalies (transactions by users with unusual patterns)
            high_risk_users = transaction_analyses.filter(
                user_anomaly=True, risk_level__in=['HIGH', 'CRITICAL']
            ).values('transaction__user_name').annotate(
                count=Count('id'),
                avg_risk_score=Avg('risk_score')
            ).order_by('-avg_risk_score')[:10]
            
            for user_data in high_risk_users:
                user_anomalies.append({
                    'user_name': user_data['transaction__user_name'],
                    'anomaly_count': user_data['count'],
                    'avg_risk_score': round(user_data['avg_risk_score'], 2)
                })
            
            # Get backdated entries (transactions posted on weekends/holidays)
            backdated_entries = all_related_transactions.filter(
                posting_date__week_day__in=[1, 7]  # Monday=1, Sunday=7
            ).values('document_number', 'posting_date', 'amount_local_currency', 'local_currency', 'user_name')[:10]
            
            # Get closing entries (transactions at month-end)
            closing_entries = all_related_transactions.filter(
                posting_date__day__gte=25  # Last week of month
            ).values('document_number', 'posting_date', 'amount_local_currency', 'local_currency', 'user_name')[:10]
        
        # Generate charts data
        charts_data = {
            'monthly_transaction_volume': self._generate_monthly_transaction_volume(all_related_transactions),
            'risk_distribution_chart': {
                'labels': [item['risk_level'] for item in risk_distribution],
                'data': [item['count'] for item in risk_distribution],
                'percentages': [item['percentage'] for item in risk_distribution]
            },
            'top_users_by_amount': self._generate_top_users_by_amount(all_related_transactions),
            'top_accounts_by_transactions': self._generate_top_accounts_by_transactions(all_related_transactions),
            'anomaly_breakdown': {
                'labels': ['Duplicate Entries', 'User Anomalies', 'Backdated Entries', 'Closing Entries', 'Unusual Days', 'Holiday Entries'],
                'data': [
                    anomaly_summary['duplicate_entries'],
                    anomaly_summary['user_anomalies'],
                    anomaly_summary['backdated_entries'],
                    anomaly_summary['closing_entries'],
                    anomaly_summary['unusual_days'],
                    anomaly_summary['holiday_entries']
                ]
            }
        }
        
        # Calculate flag rate
        flagged_transactions = transaction_analyses.filter(risk_level__in=['HIGH', 'CRITICAL']).count()
        flag_rate = round((flagged_transactions / transaction_analyses.count() * 100), 2) if transaction_analyses.count() > 0 else 0
        
        # Prepare statistics-only response
        response_data = {
            'file_info': {
                'id': str(data_file.id),
                'file_name': data_file.file_name,
                'status': data_file.status,
                'uploaded_at': data_file.uploaded_at,
                'processed_at': data_file.processed_at,
                'total_records': data_file.total_records,
                'processed_records': data_file.processed_records,
                'failed_records': data_file.failed_records,
                'currency': primary_currency
            },
            'summary_statistics': {
                'total_transactions': all_related_transactions.count(),
                'total_amount': float(total_amount),
                'currency': primary_currency,
                'flagged_transactions': flagged_transactions,
                'high_value_transactions': all_related_transactions.filter(amount_local_currency__gt=1000000).count(),
                'flag_rate': flag_rate,
                'unique_users': unique_users,
                'unique_accounts': unique_accounts,
                'unique_profit_centers': unique_profit_centers,
                'avg_amount': float(total_amount / all_related_transactions.count()) if all_related_transactions.count() > 0 else 0,
                'min_amount': float(all_related_transactions.aggregate(Min('amount_local_currency'))['amount_local_currency__min'] or 0),
                'max_amount': float(all_related_transactions.aggregate(Max('amount_local_currency'))['amount_local_currency__max'] or 0),
                'date_range': {
                    'start_date': all_related_transactions.aggregate(Min('posting_date'))['posting_date__min'],
                    'end_date': all_related_transactions.aggregate(Max('posting_date'))['posting_date__max']
                }
            },
            'risk_distribution': risk_distribution,
            'anomaly_summary': anomaly_summary,
            'charts_data': {
                'risk_distribution_chart': charts_data['risk_distribution_chart'],
                'anomaly_breakdown': charts_data['anomaly_breakdown'],
                'top_users_by_amount': charts_data['top_users_by_amount'][:5],  # Limit to top 5
                'top_accounts_by_transactions': charts_data['top_accounts_by_transactions'][:5],  # Limit to top 5
                'monthly_transaction_volume': charts_data['monthly_transaction_volume']
            },
            'analysis_sessions_summary': {
                'total_sessions': analysis_sessions.count(),
                'latest_session': {
                    'id': str(analysis_sessions.first().id),
                    'session_name': analysis_sessions.first().session_name,
                    'status': analysis_sessions.first().status,
                    'created_at': analysis_sessions.first().created_at
                } if analysis_sessions.exists() else None
            },
            'gl_charts_data': self._generate_gl_charts_data(all_related_transactions),
            'gl_account_summary': self._generate_gl_account_summary(all_related_transactions)
        }
        
        return Response(response_data)
    
    def _generate_monthly_transaction_volume(self, transactions):
        """Generate monthly transaction volume data"""
        monthly_data = {}
        
        # Get the primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        for transaction in transactions:
            if transaction.posting_date:
                month_key = transaction.posting_date.strftime('%Y-%m')
                if month_key not in monthly_data:
                    monthly_data[month_key] = {
                        'month': month_key,
                        'transaction_count': 0,
                        'total_amount': Decimal('0.00'),
                        'debit_amount': Decimal('0.00'),
                        'credit_amount': Decimal('0.00')
                    }
                
                monthly_data[month_key]['transaction_count'] += 1
                monthly_data[month_key]['total_amount'] += transaction.amount_local_currency
                
                if transaction.transaction_type == 'DEBIT':
                    monthly_data[month_key]['debit_amount'] += transaction.amount_local_currency
                else:
                    monthly_data[month_key]['credit_amount'] += transaction.amount_local_currency
        
        return [
            {
                'month': data['month'],
                'transaction_count': data['transaction_count'],
                'total_amount': float(data['total_amount']),
                'debit_amount': float(data['debit_amount']),
                'credit_amount': float(data['credit_amount']),
                'currency': currency
            }
            for data in sorted(monthly_data.values(), key=lambda x: x['month'])
        ]
    
    def _generate_top_users_by_amount(self, transactions):
        """Generate top users by transaction amount"""
        user_stats = transactions.values('user_name', 'local_currency').annotate(
            total_amount=Sum('amount_local_currency'),
            transaction_count=Count('id'),
            avg_amount=Avg('amount_local_currency')
        ).order_by('-total_amount')[:10]
        
        return [
            {
                'user_name': stat['user_name'],
                'total_amount': float(stat['total_amount']),
                'currency': stat['local_currency'],
                'transaction_count': stat['transaction_count'],
                'avg_amount': float(stat['avg_amount'])
            }
            for stat in user_stats
        ]
    
    def _generate_top_accounts_by_transactions(self, transactions):
        """Generate top accounts by transaction count"""
        account_stats = transactions.values('gl_account', 'local_currency').annotate(
            transaction_count=Count('id'),
            total_amount=Sum('amount_local_currency'),
            avg_amount=Avg('amount_local_currency')
        ).order_by('-transaction_count')[:10]
        
        return [
            {
                'gl_account': stat['gl_account'],
                'transaction_count': stat['transaction_count'],
                'total_amount': float(stat['total_amount']),
                'currency': stat['local_currency'],
                'avg_amount': float(stat['avg_amount'])
            }
            for stat in account_stats
        ]
    
    def _generate_gl_account_data(self, transactions):
        """Generate GL account data including Trial Balance and Trading Equity"""
        # Get unique GL accounts from transactions
        gl_accounts = transactions.values('gl_account').distinct()
        
        gl_account_data = []
        
        for account_data in gl_accounts:
            account_id = account_data['gl_account']
            
            # Get transactions for this account
            account_transactions = transactions.filter(gl_account=account_id)
            
            # Calculate Trial Balance (TB) - Debits minus Credits
            total_debits = account_transactions.filter(transaction_type='DEBIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            total_credits = account_transactions.filter(transaction_type='CREDIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            # Trial Balance = Debits - Credits
            trial_balance = total_debits - total_credits
            
            # Calculate Trading Equity (TE) - Net change in equity accounts
            # For equity accounts (typically 3xxxx series), TE = Credits - Debits
            # For other accounts, TE = Debits - Credits (same as TB)
            if account_id.startswith('3'):  # Equity accounts
                trading_equity = total_credits - total_debits
            else:
                trading_equity = trial_balance
            
            # Get account details from GLAccount model if available
            try:
                gl_account_obj = GLAccount.objects.get(account_id=account_id)
                account_name = gl_account_obj.account_name
                account_type = gl_account_obj.account_type
                account_category = gl_account_obj.account_category
                normal_balance = gl_account_obj.normal_balance
            except GLAccount.DoesNotExist:
                account_name = f"Account {account_id}"
                account_type = "Unknown"
                account_category = "Unknown"
                normal_balance = "DEBIT"
            
            # Calculate additional statistics
            transaction_count = account_transactions.count()
            debit_count = account_transactions.filter(transaction_type='DEBIT').count()
            credit_count = account_transactions.filter(transaction_type='CREDIT').count()
            avg_amount = account_transactions.aggregate(avg=Avg('amount_local_currency'))['avg'] or Decimal('0.00')
            
            # Get currency for this account (use the most common currency)
            account_currency = account_transactions.values('local_currency').annotate(
                count=Count('id')
            ).order_by('-count').first()
            currency = account_currency['local_currency'] if account_currency else 'SAR'
            
            gl_account_data.append({
                'account_id': account_id,
                'account_name': account_name,
                'account_type': account_type,
                'account_category': account_category,
                'normal_balance': normal_balance,
                'trial_balance': float(trial_balance),
                'trading_equity': float(trading_equity),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'transaction_count': transaction_count,
                'debit_count': debit_count,
                'credit_count': credit_count,
                'avg_amount': float(avg_amount),
                'currency': currency,
                'balance_type': 'CREDIT' if trial_balance < 0 else 'DEBIT' if trial_balance > 0 else 'ZERO'
            })
        
        # Sort by absolute trial balance (highest first)
        gl_account_data.sort(key=lambda x: abs(x['trial_balance']), reverse=True)
        
        return gl_account_data
    
    def _generate_gl_charts_data(self, transactions):
        """Generate charts data for GL accounts"""
        # Get GL account statistics
        account_stats = transactions.values('gl_account', 'local_currency').annotate(
            total_amount=Sum('amount_local_currency'),
            transaction_count=Count('id'),
            total_debits=Sum('amount_local_currency', filter=Q(transaction_type='DEBIT')),
            total_credits=Sum('amount_local_currency', filter=Q(transaction_type='CREDIT'))
        ).order_by('-total_amount')[:20]  # Top 20 accounts by amount
        
        # Get primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        # Prepare chart data
        chart_data = {
            'top_accounts_by_amount': [
                {
                    'account_id': stat['gl_account'],
                    'total_amount': float(stat['total_amount']),
                    'currency': stat['local_currency'],
                    'transaction_count': stat['transaction_count'],
                    'trial_balance': float((stat['total_debits'] or 0) - (stat['total_credits'] or 0))
                }
                for stat in account_stats
            ],
            'account_type_distribution': self._get_account_type_distribution(transactions),
            'balance_distribution': self._get_balance_distribution(transactions),
            'monthly_account_activity': self._get_monthly_account_activity(transactions),
            'currency': currency
        }
        
        return chart_data
    
    def _generate_gl_account_summary(self, transactions):
        """Generate comprehensive GL account summary with TB, TE, credits, and debits"""
        # Get all unique GL accounts from transactions
        gl_accounts = transactions.values('gl_account').distinct()
        
        # Get primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        gl_account_summary = []
        
        for account_data in gl_accounts:
            account_id = account_data['gl_account']
            
            # Get transactions for this account
            account_transactions = transactions.filter(gl_account=account_id)
            
            # Calculate Trial Balance (TB) - Debits minus Credits
            total_debits = account_transactions.filter(transaction_type='DEBIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            total_credits = account_transactions.filter(transaction_type='CREDIT').aggregate(
                total=Sum('amount_local_currency')
            )['total'] or Decimal('0.00')
            
            # Trial Balance = Debits - Credits
            trial_balance = total_debits - total_credits
            
            # Calculate Trading Equity (TE) - Net change in equity accounts
            # For equity accounts (typically 3xxxx series), TE = Credits - Debits
            # For other accounts, TE = Debits - Credits (same as TB)
            if account_id.startswith('3'):  # Equity accounts
                trading_equity = total_credits - total_debits
            else:
                trading_equity = trial_balance
            
            # Get account details from GLAccount model if available
            try:
                gl_account_obj = GLAccount.objects.get(account_id=account_id)
                account_name = gl_account_obj.account_name
                account_type = gl_account_obj.account_type
                account_category = gl_account_obj.account_category
                normal_balance = gl_account_obj.normal_balance
            except GLAccount.DoesNotExist:
                account_name = f"Account {account_id}"
                account_type = "Unknown"
                account_category = "Unknown"
                normal_balance = "DEBIT"
            
            # Calculate additional statistics
            transaction_count = account_transactions.count()
            debit_count = account_transactions.filter(transaction_type='DEBIT').count()
            credit_count = account_transactions.filter(transaction_type='CREDIT').count()
            avg_amount = account_transactions.aggregate(avg=Avg('amount_local_currency'))['avg'] or Decimal('0.00')
            
            # Get currency for this account (use the most common currency)
            account_currency = account_transactions.values('local_currency').annotate(
                count=Count('id')
            ).order_by('-count').first()
            account_currency_code = account_currency['local_currency'] if account_currency else currency
            
            # Determine balance type
            if trial_balance > 0:
                balance_type = 'DEBIT'
            elif trial_balance < 0:
                balance_type = 'CREDIT'
            else:
                balance_type = 'ZERO'
            
            # Check if balance is normal (matches normal_balance)
            is_normal_balance = (
                (normal_balance == 'DEBIT' and trial_balance >= 0) or
                (normal_balance == 'CREDIT' and trial_balance <= 0) or
                trial_balance == 0
            )
            
            gl_account_summary.append({
                'account_id': account_id,
                'account_name': account_name,
                'account_type': account_type,
                'account_category': account_category,
                'normal_balance': normal_balance,
                'currency': account_currency_code,
                
                # Balance information
                'trial_balance': float(trial_balance),
                'trading_equity': float(trading_equity),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'balance_type': balance_type,
                'is_normal_balance': is_normal_balance,
                
                # Transaction statistics
                'transaction_count': transaction_count,
                'debit_count': debit_count,
                'credit_count': credit_count,
                'avg_amount': float(avg_amount),
                
                # Additional metrics
                'debit_credit_ratio': float(total_debits / total_credits) if total_credits > 0 else None,
                'avg_debit_amount': float(total_debits / debit_count) if debit_count > 0 else 0,
                'avg_credit_amount': float(total_credits / credit_count) if credit_count > 0 else 0,
                'credit_debit_ratio': float(total_credits / total_debits) if total_debits > 0 else None
            })
        
        # Sort by absolute trial balance (highest first)
        gl_account_summary.sort(key=lambda x: abs(x['trial_balance']), reverse=True)
        
        # Calculate summary statistics
        total_accounts = len(gl_account_summary)
        total_trial_balance = sum(account['trial_balance'] for account in gl_account_summary)
        total_trading_equity = sum(account['trading_equity'] for account in gl_account_summary)
        total_debits = sum(account['total_debits'] for account in gl_account_summary)
        total_credits = sum(account['total_credits'] for account in gl_account_summary)
        
        # Count accounts by balance type
        debit_balance_accounts = sum(1 for account in gl_account_summary if account['balance_type'] == 'DEBIT')
        credit_balance_accounts = sum(1 for account in gl_account_summary if account['balance_type'] == 'CREDIT')
        zero_balance_accounts = sum(1 for account in gl_account_summary if account['balance_type'] == 'ZERO')
        
        # Count accounts with normal vs abnormal balances
        normal_balance_accounts = sum(1 for account in gl_account_summary if account['is_normal_balance'])
        abnormal_balance_accounts = total_accounts - normal_balance_accounts
        
        return {
            'summary_statistics': {
                'total_accounts': total_accounts,
                'total_trial_balance': float(total_trial_balance),
                'total_trading_equity': float(total_trading_equity),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'currency': currency,
                'debit_balance_accounts': debit_balance_accounts,
                'credit_balance_accounts': credit_balance_accounts,
                'zero_balance_accounts': zero_balance_accounts,
                'normal_balance_accounts': normal_balance_accounts,
                'abnormal_balance_accounts': abnormal_balance_accounts
            },
            'accounts': gl_account_summary
        }
    
    def _get_account_type_distribution(self, transactions):
        """Get distribution of transactions by account type"""
        # Get account types from GLAccount model
        account_types = {}
        
        # Get primary currency for the dataset
        primary_currency = transactions.values('local_currency').annotate(
            count=Count('id')
        ).order_by('-count').first()
        currency = primary_currency['local_currency'] if primary_currency else 'SAR'
        
        for transaction in transactions:
            account_id = transaction.gl_account
            try:
                gl_account = GLAccount.objects.get(account_id=account_id)
                account_type = gl_account.account_type
            except GLAccount.DoesNotExist:
                account_type = "Unknown"
            
            if account_type not in account_types:
                account_types[account_type] = {
                    'type': account_type,
                    'count': 0,
                    'total_amount': Decimal('0.00')
                }
            
            account_types[account_type]['count'] += 1
            account_types[account_type]['total_amount'] += transaction.amount_local_currency
        
        return [
            {
                'type': data['type'],
                'count': data['count'],
                'total_amount': float(data['total_amount']),
                'currency': currency
            }
            for data in account_types.values()
        ]
    
    def _get_balance_distribution(self, transactions):
        """Get distribution of account balances"""
        balance_ranges = {
            'Negative (< 0)': 0,
            'Zero (0)': 0,
            'Low (0-10K)': 0,
            'Medium (10K-100K)': 0,
            'High (100K-1M)': 0,
            'Very High (> 1M)': 0
        }
        
        # Calculate trial balance for each account
        account_balances = {}
        for transaction in transactions:
            account_id = transaction.gl_account
            if account_id not in account_balances:
                account_balances[account_id] = {'debits': Decimal('0.00'), 'credits': Decimal('0.00')}
            
            if transaction.transaction_type == 'DEBIT':
                account_balances[account_id]['debits'] += transaction.amount_local_currency
            else:
                account_balances[account_id]['credits'] += transaction.amount_local_currency
        
        # Categorize balances
        for balance in account_balances.values():
            trial_balance = balance['debits'] - balance['credits']
            balance_amount = float(trial_balance)
            
            if balance_amount < 0:
                balance_ranges['Negative (< 0)'] += 1
            elif balance_amount == 0:
                balance_ranges['Zero (0)'] += 1
            elif balance_amount <= 10000:
                balance_ranges['Low (0-10K)'] += 1
            elif balance_amount <= 100000:
                balance_ranges['Medium (10K-100K)'] += 1
            elif balance_amount <= 1000000:
                balance_ranges['High (100K-1M)'] += 1
            else:
                balance_ranges['Very High (> 1M)'] += 1
        
        return [
            {
                'range': range_name,
                'count': count
            }
            for range_name, count in balance_ranges.items()
        ]
    
    def _get_monthly_account_activity(self, transactions):
        """Get monthly activity by account type"""
        monthly_data = {}
        
        for transaction in transactions:
            if transaction.posting_date:
                month_key = transaction.posting_date.strftime('%Y-%m')
                
                if month_key not in monthly_data:
                    monthly_data[month_key] = {
                        'month': month_key,
                        'total_transactions': 0,
                        'total_amount': Decimal('0.00'),
                        'account_types': {}
                    }
                
                monthly_data[month_key]['total_transactions'] += 1
                monthly_data[month_key]['total_amount'] += transaction.amount_local_currency
                
                # Get account type
                try:
                    gl_account = GLAccount.objects.get(account_id=transaction.gl_account)
                    account_type = gl_account.account_type
                except GLAccount.DoesNotExist:
                    account_type = "Unknown"
                
                if account_type not in monthly_data[month_key]['account_types']:
                    monthly_data[month_key]['account_types'][account_type] = {
                        'count': 0,
                        'amount': Decimal('0.00')
                    }
                
                monthly_data[month_key]['account_types'][account_type]['count'] += 1
                monthly_data[month_key]['account_types'][account_type]['amount'] += transaction.amount_local_currency
        
        return [
            {
                'month': data['month'],
                'total_transactions': data['total_transactions'],
                'total_amount': float(data['total_amount']),
                'account_types': [
                    {
                        'type': account_type,
                        'count': type_data['count'],
                        'amount': float(type_data['amount'])
                    }
                    for account_type, type_data in data['account_types'].items()
                ]
            }
            for data in sorted(monthly_data.values(), key=lambda x: x['month'])
        ]

class DuplicateAnomalyViewSet(viewsets.ViewSet):
    """ViewSet for duplicate anomaly detection and analysis"""
    
    def list(self, request):
        """Get comprehensive duplicate anomaly data for a specific sheet"""
        try:
            # Get sheet_id parameter (required)
            sheet_id = request.query_params.get('sheet_id', None)
            if not sheet_id:
                return Response(
                    {'error': 'sheet_id parameter is required'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get filter parameters
            date_from = request.query_params.get('date_from', None)
            date_to = request.query_params.get('date_to', None)
            min_amount = request.query_params.get('min_amount', None)
            max_amount = request.query_params.get('max_amount', None)
            gl_accounts = request.query_params.getlist('gl_accounts', [])
            users = request.query_params.getlist('users', [])
            document_types = request.query_params.getlist('document_types', [])
            duplicate_threshold = int(request.query_params.get('duplicate_threshold', 2))
            duplicate_types = request.query_params.getlist('duplicate_types', [])
            
            # Build query - filter by sheet_id first
            query = SAPGLPosting.objects.all()  # type: ignore
            
            # Filter by sheet_id (assuming sheet_id maps to a specific data file or session)
            # You can modify this logic based on how sheet_id relates to your data
            if sheet_id:
                # Option 1: If sheet_id is a file ID
                try:
                    data_file = DataFile.objects.get(id=sheet_id)  # type: ignore
                    # Filter transactions uploaded after this file
                    query = query.filter(created_at__gte=data_file.uploaded_at)
                except DataFile.DoesNotExist:
                    # Option 2: If sheet_id is a session ID
                    try:
                        session = AnalysisSession.objects.get(id=sheet_id)  # type: ignore
                        if session.date_from:
                            query = query.filter(posting_date__gte=session.date_from)
                        if session.date_to:
                            query = query.filter(posting_date__lte=session.date_to)
                    except AnalysisSession.DoesNotExist:
                        # Option 3: If sheet_id is a custom identifier
                        # You can implement custom logic here
                        pass
            
            # Apply additional filters
            if date_from:
                query = query.filter(posting_date__gte=date_from)
            if date_to:
                query = query.filter(posting_date__lte=date_to)
            if min_amount:
                query = query.filter(amount_local_currency__gte=min_amount)
            if max_amount:
                query = query.filter(amount_local_currency__lte=max_amount)
            if gl_accounts:
                query = query.filter(gl_account__in=gl_accounts)
            if users:
                query = query.filter(user_name__in=users)
            if document_types:
                query = query.filter(document_type__in=document_types)
            
            # Get transactions
            transactions = list(query)
            
            if not transactions:
                return Response({
                    'sheet_id': sheet_id,
                    'total_duplicates': 0,  # Number of duplicate groups
                    'unique_duplicate_transactions': 0,  # Number of unique transactions involved in duplicates
                    'total_transactions_involved': 0,
                    'total_amount_involved': 0.0,
                    'type_breakdown': {},
                    'duplicates': [],
                    'charts_data': {
                        'duplicate_flags_breakdown': {'labels': [], 'data': []},
                        'monthly_duplicate_data': [],
                        'user_breakdown': [],
                        'duplicate_type_breakdown': {},
                        'fs_line_breakdown': []
                    },
                    'training_data': {
                        'training_features': [],
                        'training_labels': [],
                        'total_samples': 0,
                        'duplicate_samples': 0,
                        'non_duplicate_samples': 0,
                        'feature_importance': {},
                        'model_metrics': {}
                    }
                })
            
            # Run enhanced duplicate detection with ML model support
            analyzer = SAPGLAnalyzer()
            analyzer.analysis_config['duplicate_threshold'] = duplicate_threshold
            
            # Get rule-based duplicate detection
            duplicate_results = analyzer.detect_duplicate_entries(transactions)
            
            # Enhance with ML model predictions if available
            try:
                from .ml_models import DuplicateDetectionModel
                duplicate_model = DuplicateDetectionModel()
                
                if duplicate_model.is_trained():
                    # Get ML predictions
                    ml_predictions = duplicate_model.predict_duplicates(transactions)
                    
                    # Enhance duplicate results with ML predictions
                    if ml_predictions:
                        # Create mapping of transaction IDs to ML predictions
                        ml_prediction_map = {pred['transaction_id']: pred for pred in ml_predictions}
                        
                        # Enhance each duplicate group with ML confidence
                        for dup in duplicate_results.get('duplicates', []):
                            for transaction in dup.get('transactions', []):
                                transaction_id = transaction.get('id')
                                if transaction_id in ml_prediction_map:
                                    ml_pred = ml_prediction_map[transaction_id]
                                    transaction['ml_confidence'] = ml_pred['duplicate_probability']
                                    transaction['ml_risk_score'] = ml_pred['risk_score']
                                    transaction['ml_prediction'] = ml_pred['is_duplicate']
                        
                        # Add ML model info to results
                        duplicate_results['ml_model_info'] = duplicate_model.get_model_info()
                        duplicate_results['ml_enhanced'] = True
                    else:
                        duplicate_results['ml_enhanced'] = False
                        duplicate_results['ml_model_info'] = {'status': 'no_predictions'}
                else:
                    duplicate_results['ml_enhanced'] = False
                    duplicate_results['ml_model_info'] = {'status': 'model_not_trained'}
                    
            except Exception as ml_error:
                logger.warning(f"ML model enhancement failed: {ml_error}")
                duplicate_results['ml_enhanced'] = False
                duplicate_results['ml_model_info'] = {'status': 'error', 'error': str(ml_error)}
            
            # Extract components from the enhanced results
            all_duplicates = duplicate_results.get('duplicates', [])
            summary = duplicate_results.get('summary', {})
            drilldown_data = duplicate_results.get('drilldown_data', [])
            export_data = duplicate_results.get('export_data', [])
            
            # Filter by duplicate types if specified
            if duplicate_types:
                type_mapping = {
                    '1': 'Type 1 Duplicate',
                    '2': 'Type 2 Duplicate', 
                    '3': 'Type 3 Duplicate',
                    '4': 'Type 4 Duplicate',
                    '5': 'Type 5 Duplicate',
                    '6': 'Type 6 Duplicate'
                }
                filtered_duplicates = []
                for dup in all_duplicates:
                    for type_num in duplicate_types:
                        if dup['type'] == type_mapping.get(type_num):
                            filtered_duplicates.append(dup)
                            break
                all_duplicates = filtered_duplicates
            
            # Calculate summary statistics
            total_duplicates = len(all_duplicates)
            total_transactions_involved = sum(dup['count'] for dup in all_duplicates)
            total_amount_involved = sum(dup['amount'] * dup['count'] for dup in all_duplicates)
            
            # Count unique transactions involved in duplicates (consistent with file-summary)
            unique_duplicate_transactions = set()
            for dup in all_duplicates:
                for transaction in dup.get('transactions', []):
                    unique_duplicate_transactions.add(transaction['id'])
            unique_duplicate_count = len(unique_duplicate_transactions)
            
            # Type breakdown
            type_breakdown = {}
            for dup in all_duplicates:
                dup_type = dup['type']
                if dup_type not in type_breakdown:
                    type_breakdown[dup_type] = {
                        'count': 0,
                        'total_transactions': 0,
                        'total_amount': 0.0
                    }
                type_breakdown[dup_type]['count'] += 1
                type_breakdown[dup_type]['total_transactions'] += dup['count']
                type_breakdown[dup_type]['total_amount'] += dup['amount'] * dup['count']
            
            # Generate charts data
            charts_data = {
                'duplicate_flags_breakdown': {
                    'labels': ['Type 1', 'Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6'],
                    'data': [
                        len([d for d in all_duplicates if d['type'] == 'Type 1 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 2 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 3 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 4 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 5 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 6 Duplicate'])
                    ]
                },
                'monthly_duplicate_data': self._generate_monthly_duplicate_data(transactions, all_duplicates),
                'user_breakdown': self._generate_user_breakdown(all_duplicates),
                'duplicate_type_breakdown': self._generate_duplicate_type_breakdown(all_duplicates),
                'fs_line_breakdown': self._generate_fs_line_breakdown(all_duplicates)
            }
            
            # Generate training data
            training_data = self._generate_training_data(transactions)
            
            # Enhanced comprehensive response with drilldown and export data
            response_data = {
                'sheet_id': sheet_id,
                'total_duplicates': total_duplicates,  # Number of duplicate groups
                'unique_duplicate_transactions': unique_duplicate_count,  # Number of unique transactions involved in duplicates
                'total_transactions_involved': total_transactions_involved,
                'total_amount_involved': float(total_amount_involved),
                'type_breakdown': type_breakdown,
                'duplicates': all_duplicates,
                'summary': summary,  # Enhanced summary with detailed breakdowns
                'drilldown_data': drilldown_data,  # Final selection drilldown data
                'export_data': export_data,  # CSV export ready data
                'charts_data': charts_data,
                'training_data': training_data,
                'export_instructions': {
                    'message': 'Use the drilldown_data for final selection and export to CSV format for Spark Selections Workbook',
                    'csv_columns': [
                        'Duplicate_Type', 'Duplicate_Criteria', 'GL_Account', 'Amount', 'Duplicate_Count',
                        'Risk_Score', 'Transaction_ID', 'Document_Number', 'Posting_Date', 'Document_Date',
                        'User_Name', 'Document_Type', 'Transaction_Type', 'Text', 'Fiscal_Year',
                        'Posting_Period', 'Profit_Center', 'Cost_Center', 'Local_Currency',
                        'Debit_Count', 'Credit_Count', 'Debit_Amount', 'Credit_Amount'
                    ],
                    'duplicate_definitions': {
                        'Type 1 Duplicate': 'Account Number + Amount',
                        'Type 2 Duplicate': 'Account Number + Source + Amount',
                        'Type 3 Duplicate': 'Account Number + User + Amount',
                        'Type 4 Duplicate': 'Account Number + Posted Date + Amount',
                        'Type 5 Duplicate': 'Account Number + Effective Date + Amount',
                        'Type 6 Duplicate': 'Account Number + Effective Date + Posted Date + User + Source + Amount'
                    }
                }
            }
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error in duplicate anomaly analysis: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['get'])
    def export_csv(self, request):
        """Export duplicate analysis results to CSV format for Spark Selections Workbook"""
        try:
            from django.http import HttpResponse
            import csv
            from io import StringIO
            
            # Get sheet_id parameter (required)
            sheet_id = request.query_params.get('sheet_id', None)
            if not sheet_id:
                return Response(
                    {'error': 'sheet_id parameter is required'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get filter parameters (same as list method)
            date_from = request.query_params.get('date_from', None)
            date_to = request.query_params.get('date_to', None)
            min_amount = request.query_params.get('min_amount', None)
            max_amount = request.query_params.get('max_amount', None)
            gl_accounts = request.query_params.getlist('gl_accounts', [])
            users = request.query_params.getlist('users', [])
            document_types = request.query_params.getlist('document_types', [])
            duplicate_threshold = int(request.query_params.get('duplicate_threshold', 2))
            duplicate_types = request.query_params.getlist('duplicate_types', [])
            
            # Build query (same logic as list method)
            query = SAPGLPosting.objects.all()
            
            # Apply filters
            if date_from:
                query = query.filter(posting_date__date__gte=date_from)
            if date_to:
                query = query.filter(posting_date__date__lte=date_to)
            if min_amount:
                query = query.filter(amount_local_currency__gte=min_amount)
            if max_amount:
                query = query.filter(amount_local_currency__lte=max_amount)
            if gl_accounts:
                query = query.filter(gl_account__in=gl_accounts)
            if users:
                query = query.filter(user_name__in=users)
            if document_types:
                query = query.filter(document_type__in=document_types)
            
            # Get transactions
            transactions = list(query)
            
            if not transactions:
                # Return empty CSV
                output = StringIO()
                writer = csv.writer(output)
                writer.writerow([
                    'Duplicate_Type', 'Duplicate_Criteria', 'GL_Account', 'Amount', 'Duplicate_Count',
                    'Risk_Score', 'Transaction_ID', 'Document_Number', 'Posting_Date', 'Document_Date',
                    'User_Name', 'Document_Type', 'Transaction_Type', 'Text', 'Fiscal_Year',
                    'Posting_Period', 'Profit_Center', 'Cost_Center', 'Local_Currency',
                    'Debit_Count', 'Credit_Count', 'Debit_Amount', 'Credit_Amount'
                ])
                
                response = HttpResponse(
                    output.getvalue(),
                    content_type='text/csv'
                )
                response['Content-Disposition'] = f'attachment; filename="duplicate_analysis_{sheet_id}_empty.csv"'
                return response
            
            # Run enhanced duplicate detection
            analyzer = SAPGLAnalyzer()
            analyzer.analysis_config['duplicate_threshold'] = duplicate_threshold
            
            duplicate_results = analyzer.detect_duplicate_entries(transactions)
            export_data = duplicate_results.get('export_data', [])
            
            # Filter by duplicate types if specified
            if duplicate_types:
                filtered_ids = set()
                for dup in duplicate_results.get('duplicates', []):
                    if dup['type'] in duplicate_types:
                        for transaction in dup['transactions']:
                            filtered_ids.add(transaction['id'])
                export_data = [item for item in export_data if item['Transaction_ID'] in filtered_ids]
            
            # Create CSV content
            output = StringIO()
            writer = csv.DictWriter(output, fieldnames=[
                'Duplicate_Type', 'Duplicate_Criteria', 'GL_Account', 'Amount', 'Duplicate_Count',
                'Risk_Score', 'Transaction_ID', 'Document_Number', 'Posting_Date', 'Document_Date',
                'User_Name', 'Document_Type', 'Transaction_Type', 'Text', 'Fiscal_Year',
                'Posting_Period', 'Profit_Center', 'Cost_Center', 'Local_Currency',
                'Debit_Count', 'Credit_Count', 'Debit_Amount', 'Credit_Amount'
            ])
            
            writer.writeheader()
            for row in export_data:
                writer.writerow(row)
            
            # Create HTTP response
            response = HttpResponse(
                output.getvalue(),
                content_type='text/csv'
            )
            response['Content-Disposition'] = f'attachment; filename="duplicate_analysis_{sheet_id}.csv"'
            
            return response
            
        except Exception as e:
            logger.error(f"Error in duplicate CSV export: {e}")
            return Response(
                {'error': f'Error exporting duplicate analysis: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

    @action(detail=False, methods=['post'])
    def analyze(self, request):
        """Run comprehensive duplicate analysis with custom parameters for a specific sheet"""
        try:
            # Get sheet_id from request data
            sheet_id = request.data.get('sheet_id', None)
            if not sheet_id:
                return Response(
                    {'error': 'sheet_id is required in request body'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Validate other parameters
            serializer = DuplicateAnalysisRequestSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Get validated data
            data = serializer.validated_data
            
            # Set default values if not provided
            if not data:
                data = {}
            
            # Build query - filter by sheet_id first
            query = SAPGLPosting.objects.all()  # type: ignore
            
            # Filter by sheet_id
            if sheet_id:
                try:
                    data_file = DataFile.objects.get(id=sheet_id)  # type: ignore
                    query = query.filter(created_at__gte=data_file.uploaded_at)
                except DataFile.DoesNotExist:
                    try:
                        session = AnalysisSession.objects.get(id=sheet_id)  # type: ignore
                        if session.date_from:
                            query = query.filter(posting_date__gte=session.date_from)
                        if session.date_to:
                            query = query.filter(posting_date__lte=session.date_to)
                    except AnalysisSession.DoesNotExist:
                        pass
            
            # Apply additional filters
            if data and data.get('date_from'):
                query = query.filter(posting_date__gte=data['date_from'])
            if data and data.get('date_to'):
                query = query.filter(posting_date__lte=data['date_to'])
            if data and data.get('min_amount'):
                query = query.filter(amount_local_currency__gte=data['min_amount'])
            if data and data.get('max_amount'):
                query = query.filter(amount_local_currency__lte=data['max_amount'])
            if data and data.get('gl_accounts'):
                query = query.filter(gl_account__in=data['gl_accounts'])
            if data and data.get('users'):
                query = query.filter(user_name__in=data['users'])
            if data and data.get('document_types'):
                query = query.filter(document_type__in=data['document_types'])
            
            # Get transactions
            transactions = list(query)
            
            if not transactions:
                return Response({
                    'sheet_id': sheet_id,
                    'message': 'No transactions found matching criteria',
                    'total_duplicates': 0,  # Number of duplicate groups
                    'unique_duplicate_transactions': 0,  # Number of unique transactions involved in duplicates
                    'total_transactions_involved': 0,
                    'total_amount_involved': 0.0,
                    'type_breakdown': {},
                    'duplicates': [],
                    'charts_data': {},
                    'training_data': {}
                })
            
            # Run duplicate detection
            analyzer = SAPGLAnalyzer()
            analyzer.analysis_config['duplicate_threshold'] = data.get('duplicate_threshold', 2) if data else 2
            
            all_duplicates = analyzer.detect_duplicate_entries(transactions)
            
            # Filter by duplicate types if specified
            if data and not data.get('include_all_types', True) and data.get('duplicate_types'):
                type_mapping = {
                    1: 'Type 1 Duplicate',
                    2: 'Type 2 Duplicate', 
                    3: 'Type 3 Duplicate',
                    4: 'Type 4 Duplicate',
                    5: 'Type 5 Duplicate',
                    6: 'Type 6 Duplicate'
                }
                filtered_duplicates = []
                for dup in all_duplicates:
                    for type_num in data['duplicate_types']:
                        if dup['type'] == type_mapping.get(type_num):
                            filtered_duplicates.append(dup)
                            break
                all_duplicates = filtered_duplicates
            
            # Calculate summary statistics
            total_duplicates = len(all_duplicates)
            total_transactions_involved = sum(dup['count'] for dup in all_duplicates)
            total_amount_involved = sum(dup['amount'] * dup['count'] for dup in all_duplicates)
            
            # Count unique transactions involved in duplicates (consistent with file-summary)
            unique_duplicate_transactions = set()
            for dup in all_duplicates:
                for transaction in dup.get('transactions', []):
                    unique_duplicate_transactions.add(transaction['id'])
            unique_duplicate_count = len(unique_duplicate_transactions)
            
            # Count unique transactions involved in duplicates (consistent with file-summary)
            unique_duplicate_transactions = set()
            for dup in all_duplicates:
                for transaction in dup.get('transactions', []):
                    unique_duplicate_transactions.add(transaction['id'])
            unique_duplicate_count = len(unique_duplicate_transactions)
            
            # Type breakdown
            type_breakdown = {}
            for dup in all_duplicates:
                dup_type = dup['type']
                if dup_type not in type_breakdown:
                    type_breakdown[dup_type] = {
                        'count': 0,
                        'total_transactions': 0,
                        'total_amount': 0.0
                    }
                type_breakdown[dup_type]['count'] += 1
                type_breakdown[dup_type]['total_transactions'] += dup['count']
                type_breakdown[dup_type]['total_amount'] += dup['amount'] * dup['count']
            
            # Generate charts data
            charts_data = {
                'duplicate_flags_breakdown': {
                    'labels': ['Type 1', 'Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6'],
                    'data': [
                        len([d for d in all_duplicates if d['type'] == 'Type 1 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 2 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 3 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 4 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 5 Duplicate']),
                        len([d for d in all_duplicates if d['type'] == 'Type 6 Duplicate'])
                    ]
                },
                'monthly_duplicate_data': self._generate_monthly_duplicate_data(transactions, all_duplicates),
                'user_breakdown': self._generate_user_breakdown(all_duplicates),
                'duplicate_type_breakdown': self._generate_duplicate_type_breakdown(all_duplicates),
                'fs_line_breakdown': self._generate_fs_line_breakdown(all_duplicates)
            }
            
            # Generate training data
            training_data = self._generate_training_data(transactions)
            
            response_data = {
                'sheet_id': sheet_id,
                'message': 'Duplicate analysis completed successfully',
                'total_duplicates': total_duplicates,  # Number of duplicate groups
                'unique_duplicate_transactions': unique_duplicate_count,  # Number of unique transactions involved in duplicates (consistent with file-summary)
                'total_transactions_involved': total_transactions_involved,
                'total_amount_involved': float(total_amount_involved),
                'type_breakdown': type_breakdown,
                'duplicates': all_duplicates,
                'charts_data': charts_data,
                'training_data': training_data
            }
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error in duplicate analysis: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_monthly_duplicate_data(self, transactions, duplicates):
        """Generate monthly duplicate data with amounts and counts"""
        monthly_data = {}
        
        # Group duplicates by month
        for dup in duplicates:
            for tx in dup['transactions']:
                if tx.get('posting_date'):
                    try:
                        month_key = tx['posting_date'][:7]  # YYYY-MM format
                        if month_key not in monthly_data:
                            monthly_data[month_key] = {
                                'month': month_key,
                                'debit_amount': 0.0,
                                'credit_amount': 0.0,
                                'journal_line_count': 0,
                                'duplicate_count': 0
                            }
                        
                        # Add amounts
                        if tx.get('transaction_type') == 'DEBIT':
                            monthly_data[month_key]['debit_amount'] += tx['amount']
                        else:
                            monthly_data[month_key]['credit_amount'] += tx['amount']
                        
                        monthly_data[month_key]['journal_line_count'] += 1
                        monthly_data[month_key]['duplicate_count'] += 1
                    except:
                        continue
        
        return list(monthly_data.values())
    
    def _generate_user_breakdown(self, duplicates):
        """Generate breakdown of duplicates per impacted user"""
        user_data = {}
        
        for dup in duplicates:
            for tx in dup['transactions']:
                user_name = tx.get('user_name', 'Unknown')
                if user_name not in user_data:
                    user_data[user_name] = {
                        'user_name': user_name,
                        'duplicate_count': 0,
                        'total_amount': 0.0,
                        'duplicate_types': set()
                    }
                
                user_data[user_name]['duplicate_count'] += 1
                user_data[user_name]['total_amount'] += tx['amount']
                user_data[user_name]['duplicate_types'].add(dup['type'])
        
        # Convert sets to lists for JSON serialization
        for user in user_data.values():
            user['duplicate_types'] = list(user['duplicate_types'])
        
        return list(user_data.values())
    
    def _generate_duplicate_type_breakdown(self, duplicates):
        """Generate detailed breakdown by duplicate type"""
        type_data = {
            'Type 3 Duplicate': {'count': 0, 'users': set(), 'amounts': []},
            'Type 4 Duplicate': {'count': 0, 'dates': set(), 'amounts': []},
            'Type 5 Duplicate': {'count': 0, 'dates': set(), 'amounts': []},
            'Type 6 Duplicate': {'count': 0, 'combinations': set(), 'amounts': []}
        }
        
        for dup in duplicates:
            if dup['type'] in type_data:
                type_data[dup['type']]['count'] += 1
                type_data[dup['type']]['amounts'].append(dup['amount'])
                
                if dup['type'] == 'Type 3 Duplicate':
                    type_data[dup['type']]['users'].add(dup.get('user_name', 'Unknown'))
                elif dup['type'] == 'Type 4 Duplicate':
                    type_data[dup['type']]['dates'].add(dup.get('posting_date', 'Unknown'))
                elif dup['type'] == 'Type 5 Duplicate':
                    type_data[dup['type']]['dates'].add(dup.get('document_date', 'Unknown'))
                elif dup['type'] == 'Type 6 Duplicate':
                    combination = f"{dup.get('user_name', 'Unknown')}-{dup.get('posting_date', 'Unknown')}-{dup.get('document_date', 'Unknown')}"
                    type_data[dup['type']]['combinations'].add(combination)
        
        # Convert sets to lists for JSON serialization
        for dup_type in type_data.values():
            for key, value in dup_type.items():
                if isinstance(value, set):
                    dup_type[key] = list(value)
        
        return type_data
    
    def _generate_fs_line_breakdown(self, duplicates):
        """Generate breakdown of duplicates per impacted FS line"""
        fs_line_data = {}
        
        for dup in duplicates:
            gl_account = dup.get('gl_account', 'Unknown')
            if gl_account not in fs_line_data:
                fs_line_data[gl_account] = {
                    'gl_account': gl_account,
                    'duplicate_count': 0,
                    'total_amount': 0.0,
                    'duplicate_types': set(),
                    'transaction_count': 0
                }
            
            fs_line_data[gl_account]['duplicate_count'] += 1
            fs_line_data[gl_account]['total_amount'] += dup['amount'] * dup['count']
            fs_line_data[gl_account]['duplicate_types'].add(dup['type'])
            fs_line_data[gl_account]['transaction_count'] += dup['count']
        
        # Convert sets to lists for JSON serialization
        for fs_line in fs_line_data.values():
            fs_line['duplicate_types'] = list(fs_line['duplicate_types'])
        
        return list(fs_line_data.values())
    
    def _generate_training_data(self, transactions):
        """Generate training data for machine learning"""
        import pandas as pd
        import numpy as np
        from datetime import datetime
        
        # Convert transactions to DataFrame with proper date handling
        transaction_data = []
        for t in transactions:
            # Convert dates to pandas datetime for proper handling
            posting_date = pd.to_datetime(t.posting_date) if t.posting_date else None
            document_date = pd.to_datetime(t.document_date) if t.document_date else None
            
            transaction_data.append({
                'id': str(t.id),
                'gl_account': t.gl_account or 'UNKNOWN',
                'amount': float(t.amount_local_currency),
                'user_name': t.user_name,
                'posting_date': posting_date,
                'document_date': document_date,
                'document_type': t.document_type,
                'text': t.text or '',
                'transaction_type': t.transaction_type
            })
        
        df = pd.DataFrame(transaction_data)
        
        # Create features
        features = []
        labels = []
        
        # Generate positive samples (duplicates)
        analyzer = SAPGLAnalyzer()
        duplicates = analyzer.detect_duplicate_entries(transactions)
        
        duplicate_ids = set()
        for dup in duplicates:
            for tx in dup['transactions']:
                duplicate_ids.add(tx['id'])
        
        # Create feature vectors
        for i, row in df.iterrows():
            # Basic features with proper date handling
            posting_date = row['posting_date']
            day_of_week = posting_date.dayofweek if pd.notna(posting_date) else 0
            day_of_month = posting_date.day if pd.notna(posting_date) else 0
            month = posting_date.month if pd.notna(posting_date) else 0
            
            feature_vector = [
                float(row['amount']),
                len(str(row['gl_account'])),
                len(str(row['user_name'])),
                day_of_week,
                day_of_month,
                month,
                1 if row['transaction_type'] == 'DEBIT' else 0,
                len(str(row['text']))
            ]
            
            features.append(feature_vector)
            labels.append(1 if row['id'] in duplicate_ids else 0)
        
        # Calculate feature importance (simple correlation-based)
        feature_names = ['amount', 'gl_account_len', 'user_name_len', 'day_of_week', 
                        'day_of_month', 'month', 'is_debit', 'text_len']
        feature_importance = {}
        
        if len(features) > 0:
            features_array = np.array(features)
            for i, name in enumerate(feature_names):
                if i < features_array.shape[1]:
                    correlation = np.corrcoef(features_array[:, i], labels)[0, 1]
                    feature_importance[name] = abs(correlation) if not np.isnan(correlation) else 0.0
        
        # Calculate model metrics
        total_samples = len(labels)
        duplicate_samples = sum(labels)
        non_duplicate_samples = total_samples - duplicate_samples
        
        model_metrics = {
            'total_samples': total_samples,
            'duplicate_samples': duplicate_samples,
            'non_duplicate_samples': non_duplicate_samples,
            'duplicate_ratio': duplicate_samples / total_samples if total_samples > 0 else 0
        }
        
        return {
            'training_features': features,
            'training_labels': labels,
            'total_samples': total_samples,
            'duplicate_samples': duplicate_samples,
            'non_duplicate_samples': non_duplicate_samples,
            'feature_importance': feature_importance,
            'model_metrics': model_metrics
        }

class TargetedAnomalyUploadView(generics.CreateAPIView):
    """View for file upload with targeted anomaly detection"""
    
    parser_classes = (MultiPartParser, FormParser)
    serializer_class = TargetedAnomalyUploadSerializer
    
    def create(self, request, *args, **kwargs):
        """Handle file upload with conditional anomaly detection"""
        print(f" DEBUG: ===== TARGETED ANOMALY UPLOAD STARTED =====")
        print(f" DEBUG: Request method: {request.method}")
        print(f" DEBUG: Request content type: {request.content_type}")
        print(f" DEBUG: Request data keys: {list(request.data.keys()) if hasattr(request.data, 'keys') else 'No data'}")
        
        try:
            print(f" DEBUG: Validating request data...")
            # Validate the request
            serializer = self.get_serializer(data=request.data)
            print(f" DEBUG: Running serializer validation...")
            serializer.is_valid(raise_exception=True)
            print(f" DEBUG: Serializer validation passed")
            
            # Get validated data
            print(f" DEBUG: Extracting validated data...")
            file_obj = serializer.validated_data['file']
            print(f" DEBUG: File object: {file_obj.name}, size: {file_obj.size}")
            
            engagement_id = serializer.validated_data['engagement_id']
            client_name = serializer.validated_data['client_name']
            company_name = serializer.validated_data['company_name']
            fiscal_year = serializer.validated_data['fiscal_year']
            audit_start_date = serializer.validated_data['audit_start_date']
            audit_end_date = serializer.validated_data['audit_end_date']
            description = serializer.validated_data.get('description', '')
            run_anomalies = serializer.validated_data.get('run_anomalies', False)
            requested_anomalies = serializer.validated_data.get('anomalies', [])
            
            print(f" DEBUG: Engagement ID: {engagement_id}")
            print(f" DEBUG: Client Name: {client_name}")
            print(f" DEBUG: Company Name: {company_name}")
            print(f" DEBUG: Fiscal Year: {fiscal_year}")
            print(f" DEBUG: Run Anomalies: {run_anomalies}")
            print(f" DEBUG: Requested Anomalies: {requested_anomalies}")
            print(f" DEBUG: Anomalies type: {type(requested_anomalies)}")
            # Validate file type
            print(f" DEBUG: Validating file type...")
            if not file_obj.name.endswith('.csv'):
                print(f" DEBUG: Invalid file type: {file_obj.name}")
                return Response(
                    {'error': 'Only CSV files are supported'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            print(f" DEBUG: File type validation passed")
            
            # Calculate file hash for duplicate detection
            print(f" DEBUG: Reading file content...")
            file_content = file_obj.read()
            print(f" DEBUG: File content size: {len(file_content)} bytes")
            print(f" DEBUG: Calculating file hash...")
            file_hash = self._calculate_file_hash(file_content)
            print(f" DEBUG: File hash: {file_hash}")
            
            # Check for duplicate content
            print(f" DEBUG: Checking for duplicate content...")
            existing_job = FileProcessingJob.objects.filter(
                file_hash=file_hash,
                status='COMPLETED'
            ).first()
            
            if existing_job:
                print(f" DEBUG: Duplicate content found, returning existing results")
                # Return reference to existing results
                return Response({
                    'job_id': str(existing_job.id),
                    'status': 'SKIPPED',
                    'message': 'File content already processed. Returning existing results.',
                    'is_duplicate_content': True,
                    'existing_job_id': str(existing_job.id),
                    'file_info': DataFileSerializer(existing_job.data_file).data,
                    'analytics_results': existing_job.analytics_results,
                    'anomaly_results': existing_job.anomaly_results,
                    'processing_duration': existing_job.processing_duration,
                    'created_at': existing_job.created_at,
                    'started_at': existing_job.started_at,
                    'completed_at': existing_job.completed_at,
                }, status=status.HTTP_200_OK)
            print(f" DEBUG: No duplicate content found, proceeding with processing")
            
            # Create DataFile record
            print(f" DEBUG: Creating DataFile record...")
            data_file = DataFile.objects.create(
                file_name=file_obj.name,
                file_size=file_obj.size,
                engagement_id=engagement_id,
                client_name=client_name,
                company_name=company_name,
                fiscal_year=fiscal_year,
                audit_start_date=audit_start_date,
                audit_end_date=audit_end_date,
                status='PENDING'
            )
            print(f" DEBUG: DataFile created with ID: {data_file.id}")
            
            # Process and save CSV data immediately after file upload
            print(f" DEBUG: Processing and saving CSV data immediately...")
            try:
                processed_count, failed_count = self._process_and_save_csv_data(data_file, file_content)
                print(f" DEBUG: CSV data saved successfully")
                print(f" DEBUG: Processed: {processed_count}, Failed: {failed_count}")
                
                # Update DataFile with processing results
                data_file.total_records = processed_count + failed_count
                data_file.processed_records = processed_count
                data_file.failed_records = failed_count
                data_file.status = 'COMPLETED' if failed_count == 0 else 'PARTIAL'
                data_file.processed_at = timezone.now()
                data_file.save()
                print(f" DEBUG: DataFile updated with processing results")
                
            except Exception as data_error:
                print(f" DEBUG: Error processing CSV data: {data_error}")
                print(f" DEBUG: Error type: {type(data_error).__name__}")
                # Continue with job creation even if data processing fails
                data_file.status = 'FAILED'
                data_file.error_message = str(data_error)
                data_file.save()
            
            # Create FileProcessingJob record
            print(f" DEBUG: Creating FileProcessingJob record...")
            processing_job = FileProcessingJob.objects.create(
                data_file=data_file,
                file_hash=file_hash,
                run_anomalies=run_anomalies,
                requested_anomalies=requested_anomalies,
                status='PENDING'
            )
            print(f" DEBUG: FileProcessingJob created with ID: {processing_job.id}")
            
            # Start background processing using Celery task - NON-BLOCKING
            print(f" DEBUG: Starting NON-BLOCKING Celery task for job {processing_job.id}")
            logger.info(f"Starting NON-BLOCKING Celery task for job {processing_job.id}")
            
            # Save job to queue and let worker pick it up when available
            print(f" DEBUG: Saving job to queue for worker processing...")
            
            # Update job status to QUEUED
            processing_job.status = 'QUEUED'
            processing_job.save()
            
            print(f" DEBUG: Job saved to queue with ID: {processing_job.id}")
            logger.info(f"Job {processing_job.id} saved to queue for processing")
            
            # Job is already saved to queue - no direct Celery call needed
            print(f" DEBUG: Job saved to queue - will be processed by worker")
            logger.info(f"Job {processing_job.id} saved to queue for processing")
            
            # Return immediately with job information - NON-BLOCKING RESPONSE
            return Response({
                'job_id': str(processing_job.id),
                'status': 'QUEUED',
                'message': 'File uploaded successfully. Job queued for processing.',
                'is_duplicate_content': False,
                'existing_job_id': None,
                'file_info': DataFileSerializer(data_file).data,
                'celery_task_id': None,  # No direct Celery call - using queue
                'queued_at': processing_job.created_at.isoformat(),
                'estimated_completion': 'Job will be processed when worker becomes available',
                'status_endpoint': f'/api/file-processing-jobs/{processing_job.id}/status/',
                'analytics_endpoint': f'/api/analysis/file/{data_file.id}/',
                'duplicate_analysis_endpoint': f'/api/duplicate-anomalies/?sheet_id={data_file.id}'
            }, status=status.HTTP_202_ACCEPTED)
            
        except Exception as e:
            logger.error(f"Error in targeted anomaly upload: {e}")
            return Response(
                {'error': str(e)}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _calculate_file_hash(self, file_content):
        """Calculate SHA256 hash of file content"""
        import hashlib
        return hashlib.sha256(file_content).hexdigest()
    
    def _process_and_save_csv_data(self, data_file, file_content):
        """Process CSV data and save transactions immediately after file upload"""
        print(f" DEBUG: ===== _process_and_save_csv_data STARTED =====")
        print(f" DEBUG: Processing CSV for DataFile ID: {data_file.id}")
        print(f" DEBUG: File content size: {len(file_content)} bytes")
        
        try:
            # Create CSV reader
            csv_reader = csv.DictReader(io.StringIO(file_content.decode('utf-8')))
            print(f" DEBUG: CSV reader created successfully")
            print(f" DEBUG: CSV fieldnames: {csv_reader.fieldnames}")
            
            # Process rows and create transactions
            processed_count = 0
            failed_count = 0
            new_transactions = []
            
            print(f" DEBUG: Starting to process CSV rows...")
            for row_num, row in enumerate(csv_reader):
                try:
                    posting = self._create_posting_from_row(row)
                    if posting:
                        processed_count += 1
                        new_transactions.append(posting)
                        if processed_count % 100 == 0:  # Log every 100 records
                            print(f"   Processed {processed_count} transactions...")
                    else:
                        failed_count += 1
                        print(f"   Failed to create transaction {row_num + 1}: posting creation returned None")
                except Exception as row_error:
                    failed_count += 1
                    print(f"   Failed to create transaction {row_num + 1}: {row_error}")
                    print(f"     Error type: {type(row_error).__name__}")
            
            print(f" DEBUG: CSV processing completed")
            print(f" DEBUG: Processed count: {processed_count}")
            print(f" DEBUG: Failed count: {failed_count}")
            print(f" DEBUG: New transactions count: {len(new_transactions)}")
            
            # Save transactions in batch
            if new_transactions:
                print(f" DEBUG: Saving transactions in batch...")
                try:
                    SAPGLPosting.objects.bulk_create(new_transactions)
                    print(f" DEBUG: Transactions saved successfully to database")
                except Exception as save_error:
                    print(f" DEBUG: Error saving transactions: {save_error}")
                    print(f" DEBUG: Save error type: {type(save_error).__name__}")
                    raise
            else:
                print(f" DEBUG: No transactions to save")
            
            print(f" DEBUG: ===== _process_and_save_csv_data COMPLETED =====")
            return processed_count, failed_count
            
        except Exception as e:
            print(f" DEBUG: Error in _process_and_save_csv_data!")
            print(f" DEBUG: Error type: {type(e).__name__}")
            print(f" DEBUG: Error message: {str(e)}")
            logger.error(f"Error processing CSV data: {e}")
            raise
    
    def _test_celery_connection(self):
        """Test Celery connection with improved error handling and retry logic"""
        import time
        from celery.exceptions import OperationalError
        
        max_retries = 3
        retry_delay = 2  # seconds
        
        for attempt in range(max_retries):
            try:
                log_task_info("celery_test", "N/A", f"Testing Celery connection (attempt {attempt + 1}/{max_retries})")
                
                # Test basic connection
                from analytics.celery import app
                inspect = app.control.inspect()
                stats = inspect.stats()
                
                if stats:
                    log_task_info("celery_test", "N/A", f" Celery connection successful on attempt {attempt + 1}")
                    return True
                else:
                    log_task_info("celery_test", "N/A", f"  No workers found on attempt {attempt + 1}")
                    
            except OperationalError as e:
                log_task_info("celery_test", "N/A", f" Celery connection failed on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    log_task_info("celery_test", "N/A", f" Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    log_task_info("celery_test", "N/A", f" All connection attempts failed")
                    return False
            except Exception as e:
                log_task_info("celery_test", "N/A", f" Unexpected error testing Celery: {e}")
                return False
        
        return False

    def _test_redis_connection(self):
        """Test Redis connection directly"""
        try:
            import redis
            from django.conf import settings
            
            # Get Redis URL from settings
            redis_url = settings.CELERY_BROKER_URL
            log_task_info("redis_test", "N/A", f"Testing Redis connection to: {redis_url}")
            
            # Create Redis client
            r = redis.from_url(redis_url)
            
            # Test basic operations
            r.ping()
            r.set('test_key', 'test_value', ex=10)
            test_value = r.get('test_key')
            r.delete('test_key')
            
            log_task_info("redis_test", "N/A", " Redis connection successful")
            return True
            
        except Exception as e:
            log_task_info("redis_test", "N/A", f" Redis connection failed: {e}")
            return False

    def _test_celery_task_submission(self):
        """Test queue system instead of direct task submission"""
        try:
            log_task_info("queue_test", "N/A", "Testing queue system...")
            
            # No direct Celery call - using queue system
            log_task_info("queue_test", "N/A", " Queue system active - no direct Celery calls")
            return True
            
        except Exception as e:
            log_task_info("queue_test", "N/A", f" Queue system test failed: {e}")
            return False

    def _process_file_synchronously(self, processing_job, data_file):
        """Process file synchronously as fallback when Celery is not available"""
        print(f" DEBUG: ===== SYNCHRONOUS PROCESSING STARTED =====")
        print(f" DEBUG: Processing job ID: {processing_job.id}")
        print(f" DEBUG: Data file: {data_file.file_name}")
        
        try:
            # Update job status
            processing_job.status = 'PROCESSING'
            processing_job.started_at = timezone.now()
            processing_job.save()
            
            print(f" DEBUG: Processing file content...")
            # Process file content
            result = _process_file_content(processing_job)
            
            if result['success']:
                print(f" DEBUG: File content processed successfully")
                
                # Run analytics
                print(f" DEBUG: Running default analytics...")
                analytics_results = _run_default_analytics(result['transactions'], data_file)
                
                # Run anomalies if requested
                anomaly_results = {}
                if processing_job.run_anomalies and processing_job.requested_anomalies:
                    print(f" DEBUG: Running requested anomalies...")
                    anomaly_results = _run_requested_anomalies(result['transactions'], processing_job.requested_anomalies)
                
                # Update job with results
                processing_job.analytics_results = analytics_results
                processing_job.anomaly_results = anomaly_results
                processing_job.status = 'COMPLETED'
                processing_job.completed_at = timezone.now()
                processing_job.processing_duration = (timezone.now() - processing_job.started_at).total_seconds()
                processing_job.save()
                
                print(f" DEBUG: Synchronous processing completed successfully")
                
                return Response({
                    'job_id': str(processing_job.id),
                    'status': 'COMPLETED',
                    'message': 'File processed successfully using synchronous fallback.',
                    'is_duplicate_content': False,
                    'existing_job_id': None,
                    'file_info': DataFileSerializer(data_file).data,
                    'processing_duration': processing_job.processing_duration,
                    'status_endpoint': f'/api/file-processing-jobs/{processing_job.id}/status/',
                    'analytics_endpoint': f'/api/analysis/file/{data_file.id}/',
                    'duplicate_analysis_endpoint': f'/api/duplicate-anomalies/?sheet_id={data_file.id}'
                }, status=status.HTTP_200_OK)
            else:
                # Handle processing failure
                processing_job.status = 'FAILED'
                processing_job.error_message = result.get('error', 'Unknown error')
                processing_job.completed_at = timezone.now()
                processing_job.save()
                
                return Response({
                    'job_id': str(processing_job.id),
                    'status': 'FAILED',
                    'error': 'File processing failed',
                    'details': result.get('error', 'Unknown error'),
                    'file_info': DataFileSerializer(data_file).data,
                    'status_endpoint': f'/api/file-processing-jobs/{processing_job.id}/status/'
                }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
                
        except Exception as e:
            print(f" DEBUG: Error in synchronous processing: {e}")
            print(f" DEBUG: Error type: {type(e).__name__}")
            import traceback
            print(f" DEBUG: Traceback: {traceback.format_exc()}")
            
            # Update job status
            processing_job.status = 'FAILED'
            processing_job.error_message = str(e)
            processing_job.completed_at = timezone.now()
            processing_job.save()
            
            return Response({
                'job_id': str(processing_job.id),
                'status': 'FAILED',
                'error': 'Synchronous processing failed',
                'details': str(e),
                'file_info': DataFileSerializer(data_file).data,
                'status_endpoint': f'/api/file-processing-jobs/{processing_job.id}/status/'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _handle_celery_failure(self, processing_job, celery_error, context="task_submission"):
        """Handle Celery failure and update database fields"""
        print(f" DEBUG: Handling Celery failure for {context}")
        print(f" DEBUG: Error type: {type(celery_error).__name__}")
        print(f" DEBUG: Error message: {str(celery_error)}")
        
        logger.error(f"Celery failure in {context}: {celery_error}")
        
        # Test Celery connection for better error reporting
        connection_status = self._test_celery_connection()
        
        # Update job status to indicate Celery failure
        try:
            processing_job.status = 'CELERY_ERROR'
            processing_job.error_message = f"Celery {context} failed: {str(celery_error)}. Connection status: {connection_status['connected']}"
            processing_job.completed_at = timezone.now()
            processing_job.save()
            print(f" DEBUG: Job status updated to CELERY_ERROR successfully")
            logger.info(f"Job {processing_job.id} status updated to CELERY_ERROR")
        except Exception as db_error:
            print(f" DEBUG: Error updating job status in database: {db_error}")
            logger.error(f"Failed to update job status in database: {db_error}")
        
        return {
            'job_id': str(processing_job.id),
            'status': 'CELERY_ERROR',
            'message': f'File uploaded but background processing failed due to Celery connection error in {context}.',
            'error': str(celery_error),
            'error_type': type(celery_error).__name__,
            'celery_connection_status': connection_status,
            'retry_endpoint': f'/api/file-processing-jobs/{processing_job.id}/retry/'
        }
    
    def _process_file_background(self, processing_job, file_content):
        """Process file in background with conditional anomaly detection"""
        print(f" DEBUG: ===== _process_file_background STARTED =====")
        print(f" DEBUG: Processing job ID: {processing_job.id}")
        print(f" DEBUG: File content size: {len(file_content)} bytes")
        
        try:
            print(f" DEBUG: Updating job status to PROCESSING...")
            # Update job status
            processing_job.status = 'PROCESSING'
            processing_job.started_at = timezone.now()
            processing_job.save()
            print(f" DEBUG: Job status updated successfully")
            
            start_time = timezone.now()
            print(f" DEBUG: Start time: {start_time}")
            
            # Data is already saved during file upload, so we just need to retrieve it
            print(f" DEBUG: Data already saved during upload, retrieving transactions...")
            data_file = processing_job.data_file
            
            # Get transactions that were already saved for this data file
            # We'll use a simple approach to get transactions related to this file
            # In a real implementation, you might want to add a foreign key relationship
            new_transactions = SAPGLPosting.objects.filter(
                document_number__startswith=data_file.engagement_id[:3]  # Simple filter
            ).order_by('-created_at')[:data_file.processed_records]
            
            print(f" DEBUG: Retrieved {len(new_transactions)} transactions for analytics")
            
            # Verify we have the expected number of transactions
            if len(new_transactions) != data_file.processed_records:
                print(f" DEBUG: Warning: Expected {data_file.processed_records} transactions, found {len(new_transactions)}")
                # Fallback: get all recent transactions
                new_transactions = SAPGLPosting.objects.order_by('-created_at')[:data_file.processed_records]
                print(f" DEBUG: Using fallback: {len(new_transactions)} recent transactions")
            
            # Job is already in queue - no direct Celery call needed
            print(f" DEBUG: Job already in queue - no direct Celery call needed")
            logger.info(f"Job {processing_job.id} already in queue for processing")

            # Job is already queued - no need to call Celery directly
            print(f" DEBUG: Job will be processed by worker from queue")
            logger.info(f"Job {processing_job.id} will be processed by worker from queue")
            
            print(f" DEBUG: Background processing completed for job {processing_job.id}")
            logger.info(f"Background processing completed for job {processing_job.id}")
            
        except Exception as e:
            print(f" DEBUG: Error in background processing!")
            print(f" DEBUG: Error type: {type(e).__name__}")
            print(f" DEBUG: Error message: {str(e)}")
            print(f" DEBUG: Full error details: {e}")
            logger.error(f"Error in background processing: {e}")
            processing_job.status = 'FAILED'
            processing_job.error_message = str(e)
            processing_job.completed_at = timezone.now()
            processing_job.save()
            print(f" DEBUG: Job status updated to FAILED")
    
    def _run_synchronous_analytics(self, processing_job, transactions):
        """Run analytics synchronously as fallback when Celery is not available"""
        print(f" DEBUG: ===== _run_synchronous_analytics STARTED =====")
        print(f" DEBUG: Processing job ID: {processing_job.id}")
        print(f" DEBUG: Transactions count: {len(transactions)}")
        print(f" DEBUG: Run anomalies: {processing_job.run_anomalies}")
        print(f" DEBUG: Requested anomalies: {processing_job.requested_anomalies}")
        
        try:
            logger.info("Running synchronous analytics as fallback...")
            start_time = timezone.now()
            print(f" DEBUG: Start time: {start_time}")
            
            # Run default analytics
            print(f" DEBUG: Running default analytics...")
            try:
                analytics_results = self._run_default_analytics(transactions, processing_job.data_file)
                print(f" DEBUG: Default analytics completed successfully")
            except Exception as analytics_error:
                print(f" DEBUG: Error in default analytics: {analytics_error}")
                print(f" DEBUG: Analytics error type: {type(analytics_error).__name__}")
                analytics_results = {'error': str(analytics_error)}
            
            # Run requested anomaly tests
            print(f" DEBUG: Running requested anomaly tests...")
            anomaly_results = {}
            if processing_job.run_anomalies and processing_job.requested_anomalies:
                try:
                    anomaly_results = self._run_requested_anomalies(
                        transactions, 
                        processing_job.requested_anomalies
                    )
                    print(f" DEBUG: Requested anomalies completed successfully")
                except Exception as anomaly_error:
                    print(f" DEBUG: Error in requested anomalies: {anomaly_error}")
                    print(f" DEBUG: Anomaly error type: {type(anomaly_error).__name__}")
                    anomaly_results = {'error': str(anomaly_error)}
            else:
                print(f" DEBUG: No anomalies requested, skipping")
            
            # Auto-train ML models
            print(f" DEBUG: Running ML model training...")
            try:
                ml_training_result = self._auto_train_ml_models(transactions, processing_job.data_file)
                print(f" DEBUG: ML training completed successfully")
            except Exception as ml_error:
                print(f" DEBUG: Error in ML training: {ml_error}")
                print(f" DEBUG: ML error type: {type(ml_error).__name__}")
                ml_training_result = {'error': str(ml_error)}
            
            # Run comprehensive expense analytics
            print(f" DEBUG: Running comprehensive expense analytics...")
            try:
                expense_analytics = self._run_comprehensive_expense_analytics(transactions, processing_job.data_file)
                print(f" DEBUG: Comprehensive expense analytics completed successfully")
            except Exception as expense_error:
                print(f" DEBUG: Error in comprehensive expense analytics: {expense_error}")
                print(f" DEBUG: Expense error type: {type(expense_error).__name__}")
                expense_analytics = {'error': str(expense_error)}
            
            # Calculate processing duration
            end_time = timezone.now()
            processing_duration = (end_time - start_time).total_seconds()
            print(f" DEBUG: Processing duration: {processing_duration:.2f} seconds")
            
            # Update job with results
            print(f" DEBUG: Updating job with results...")
            try:
                processing_job.analytics_results = {
                    **analytics_results,
                    'expense_analytics': expense_analytics,
                    'ml_training': ml_training_result
                }
                processing_job.anomaly_results = anomaly_results
                processing_job.status = 'COMPLETED'
                processing_job.completed_at = end_time
                processing_job.processing_duration = processing_duration
                processing_job.save()
                print(f" DEBUG: Job updated successfully")
            except Exception as save_error:
                print(f" DEBUG: Error saving job results: {save_error}")
                print(f" DEBUG: Save error type: {type(save_error).__name__}")
                raise
            
            logger.info(f"Synchronous analytics completed in {processing_duration:.2f} seconds")
            print(f" DEBUG: Synchronous analytics completed successfully")
            
        except Exception as e:
            print(f" DEBUG: General error in synchronous analytics!")
            print(f" DEBUG: Error type: {type(e).__name__}")
            print(f" DEBUG: Error message: {str(e)}")
            logger.error(f"Error in synchronous analytics: {e}")
            processing_job.status = 'FAILED'
            processing_job.error_message = str(e)
            processing_job.completed_at = timezone.now()
            processing_job.save()
            print(f" DEBUG: Job status updated to FAILED")
    
    def _create_posting_from_row(self, row):
        """Create SAPGLPosting from CSV row with minimal validation"""
        try:
            # Simple field extraction without strict validation
            document_number = row.get('Document Number', '').strip() or 'UNKNOWN'
            gl_account = row.get('G/L Account', '').strip() or 'UNKNOWN'
            amount_str = row.get('Amount in Local Currency', '0').strip()
            posting_date_str = row.get('Posting Date', '').strip()
            user_name = row.get('User Name', '').strip() or 'UNKNOWN'
            
            # Parse amount with fallback
            try:
                amount_str = str(amount_str).replace(',', '').replace(' ', '').replace('SAR', '').replace('$', '').strip()
                amount_local_currency = Decimal(amount_str) if amount_str else Decimal('0')
            except:
                amount_local_currency = Decimal('0')
            
            # Parse dates with fallback
            posting_date = self._parse_date(posting_date_str) or datetime.now().date()
            document_date = self._parse_date(row.get('Document Date', ''))
            entry_date = self._parse_date(row.get('Entry Date', ''))
            
            # Determine transaction type
            transaction_type = row.get('Transaction Type', '').strip().upper()
            if not transaction_type or transaction_type not in ['DEBIT', 'CREDIT']:
                transaction_type = 'DEBIT'
            
            # Parse fiscal year and posting period with fallbacks
            try:
                fiscal_year = int(row.get('Fiscal Year', '0')) or 2025
            except:
                fiscal_year = 2025
                
            try:
                posting_period = int(row.get('Posting Period', '0')) or 1
            except:
                posting_period = 1
            
            # Create posting
            print(f" DEBUG: Creating SAPGLPosting object...")
            try:
                posting = SAPGLPosting(
                    document_number=row.get('Document Number', ''),
                    posting_date=posting_date,
                    gl_account=row.get('G/L Account', ''),
                    amount_local_currency=amount_local_currency,
                    transaction_type=transaction_type,
                    local_currency=row.get('Local Currency', 'SAR'),
                    text=row.get('Text', ''),
                    document_date=document_date,
                    offsetting_account=row.get('Offsetting Account', ''),
                    user_name=row.get('User Name', ''),
                    entry_date=entry_date,
                    document_type=row.get('Document Type', ''),
                    profit_center=row.get('Profit Center', ''),
                    cost_center=row.get('Cost Center', ''),
                    clearing_document=row.get('Clearing Document', ''),
                    segment=row.get('Segment', ''),
                    wbs_element=row.get('WBS Element', ''),
                    plant=row.get('Plant', ''),
                    material=row.get('Material', ''),
                    invoice_reference=row.get('Invoice Reference', ''),
                    billing_document=row.get('Billing Document', ''),
                    sales_document=row.get('Sales Document', ''),
                    purchasing_document=row.get('Purchasing Document', ''),
                    order_number=row.get('Order Number', ''),
                    asset_number=row.get('Asset Number', ''),
                    network=row.get('Network', ''),
                    assignment=row.get('Assignment', ''),
                    tax_code=row.get('Tax Code', ''),
                    account_assignment=row.get('Account Assignment', ''),
                    fiscal_year=fiscal_year,
                    posting_period=posting_period,
                    year_month=row.get('Year/Month', '')
                )
                print(f" DEBUG: SAPGLPosting object created successfully")
                print(f" DEBUG: Posting ID: {posting.id if hasattr(posting, 'id') else 'Not saved yet'}")
                return posting
            except Exception as posting_error:
                print(f" DEBUG: Error creating SAPGLPosting object: {posting_error}")
                print(f" DEBUG: Posting error type: {type(posting_error).__name__}")
                raise
                
        except Exception as e:
            print(f" DEBUG: General error in _create_posting_from_row!")
            print(f" DEBUG: Error type: {type(e).__name__}")
            print(f" DEBUG: Error message: {str(e)}")
            logger.error(f"Error creating posting from row: {e}")
            return None
    
    def _parse_date(self, date_str):
        """Parse date string with multiple format support"""
        print(f" DEBUG: ===== _parse_date STARTED =====")
        print(f" DEBUG: Input date_str: '{date_str}'")
        
        if not date_str:
            print(f" DEBUG: Date string is empty or None, returning None")
            return None
        
        date_str = date_str.strip()
        print(f" DEBUG: Stripped date_str: '{date_str}'")
        
        date_formats = [
            '%Y-%m-%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%Y/%m/%d',
            '%m-%d-%Y',
            '%d-%m-%Y'
        ]
        
        print(f" DEBUG: Trying date formats...")
        for i, fmt in enumerate(date_formats):
            try:
                parsed_date = datetime.strptime(date_str, fmt).date()
                print(f" DEBUG: Successfully parsed with format {i+1}: {fmt} -> {parsed_date}")
                return parsed_date
            except ValueError as format_error:
                print(f" DEBUG: Format {i+1} failed ({fmt}): {format_error}")
                continue
        
        print(f" DEBUG: No date format worked, logging warning and returning None")
        logger.warning(f"Could not parse date: {date_str}")
        return None
    
    def _ensure_json_serializable(self, data):
        """Ensure data is JSON serializable by converting dates and other objects"""
        import json
        from datetime import date, datetime
        from decimal import Decimal
        
        def convert_value(value):
            if isinstance(value, (date, datetime)):
                return value.isoformat()
            elif isinstance(value, Decimal):
                return float(value)
            elif isinstance(value, dict):
                return {k: convert_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [convert_value(item) for item in value]
            else:
                return value
        
        return convert_value(data)
    
    def _run_default_analytics(self, transactions, data_file):
        """Run default analytics (TB, TE, GL summaries)"""
        try:
            # Calculate basic statistics
            total_transactions = len(transactions)
            total_amount = sum(t.amount_local_currency for t in transactions)
            
            # Trial Balance calculation
            total_debits = sum(t.amount_local_currency for t in transactions if t.transaction_type == 'DEBIT')
            total_credits = sum(t.amount_local_currency for t in transactions if t.transaction_type == 'CREDIT')
            trial_balance = total_debits - total_credits
            
            # GL Account summaries
            gl_accounts = {}
            for transaction in transactions:
                account_id = transaction.gl_account
                if account_id not in gl_accounts:
                    gl_accounts[account_id] = {
                        'account_id': account_id,
                        'total_debits': Decimal('0.00'),
                        'total_credits': Decimal('0.00'),
                        'transaction_count': 0
                    }
                
                gl_accounts[account_id]['transaction_count'] += 1
                if transaction.transaction_type == 'DEBIT':
                    gl_accounts[account_id]['total_debits'] += transaction.amount_local_currency
                else:
                    gl_accounts[account_id]['total_credits'] += transaction.amount_local_currency
            
            # Calculate trial balance for each account
            for account_data in gl_accounts.values():
                account_data['trial_balance'] = float(account_data['total_debits'] - account_data['total_credits'])
                account_data['total_debits'] = float(account_data['total_debits'])
                account_data['total_credits'] = float(account_data['total_credits'])
            
            result = {
                'total_transactions': total_transactions,
                'total_amount': float(total_amount),
                'total_debits': float(total_debits),
                'total_credits': float(total_credits),
                'trial_balance': float(trial_balance),
                'gl_account_summaries': list(gl_accounts.values()),
                'unique_accounts': len(gl_accounts),
                'unique_users': len(set(t.user_name for t in transactions)),
                'date_range': {
                    'min_date': min(t.posting_date for t in transactions if t.posting_date).isoformat() if transactions and any(t.posting_date for t in transactions) else None,
                    'max_date': max(t.posting_date for t in transactions if t.posting_date).isoformat() if transactions and any(t.posting_date for t in transactions) else None
                } if transactions else {}
            }
            
            # Ensure JSON serializable
            return self._ensure_json_serializable(result)
            
        except Exception as e:
            logger.error(f"Error running default analytics: {e}")
            return {'error': str(e)}
    
    def _run_requested_anomalies(self, transactions, requested_anomalies):
        """Run requested anomaly tests"""
        try:
            analyzer = SAPGLAnalyzer()
            results = {}
            
            # Map anomaly types to analyzer methods
            anomaly_methods = {
                'duplicate': analyzer.detect_duplicate_entries,
                'backdated': analyzer.detect_backdated_entries,
                'closing': analyzer.detect_closing_entries,
                'unusual_days': analyzer.detect_unusual_days,
                'holiday': analyzer.detect_holiday_entries,
                'user_anomalies': analyzer.detect_user_anomalies,
            }
            
            for anomaly_type in requested_anomalies:
                if anomaly_type in anomaly_methods:
                    try:
                        method = anomaly_methods[anomaly_type]
                        anomaly_results = method(transactions)
                        results[anomaly_type] = {
                            'anomalies_found': len(anomaly_results),
                            'details': anomaly_results[:10]  # Limit to first 10 for response
                        }
                    except Exception as e:
                        logger.error(f"Error running {anomaly_type} anomaly detection: {e}")
                        results[anomaly_type] = {
                            'anomalies_found': 0,
                            'error': str(e)
                        }
            
            # Ensure JSON serializable
            return self._ensure_json_serializable(results)
            
        except Exception as e:
            logger.error(f"Error running requested anomalies: {e}")
            return {'error': str(e)}
    
    def _auto_train_ml_models(self, transactions, data_file):
        """Train duplicate detection model once with enhanced duplicate definitions"""
        try:
            if len(transactions) < 10:
                return {
                    'status': 'SKIPPED',
                    'reason': f'Insufficient data for training. Found {len(transactions)} transactions, need at least 10.',
                    'transactions_count': len(transactions)
                }
            
            # Check if duplicate model is already trained
            from .ml_models import DuplicateDetectionModel
            duplicate_model = DuplicateDetectionModel()
            
            # Check if model already exists
            if duplicate_model.is_trained():
                return {
                    'status': 'SKIPPED',
                    'reason': 'Duplicate detection model already trained',
                    'transactions_count': len(transactions),
                    'model_loaded': True,
                    'duplicate_types': [
                        'Type 1 Duplicate - Account Number + Amount',
                        'Type 2 Duplicate - Account Number + Source + Amount',
                        'Type 3 Duplicate - Account Number + User + Amount',
                        'Type 4 Duplicate - Account Number + Posted Date + Amount',
                        'Type 5 Duplicate - Account Number + Effective Date + Amount',
                        'Type 6 Duplicate - Account Number + Effective Date + Posted Date + User + Source + Amount'
                    ]
                }
            
            # Create training session for duplicate model only
            training_session = MLModelTraining.objects.create(
                session_name=f'Duplicate-Model-Training-{data_file.file_name}-{timezone.now().strftime("%Y%m%d_%H%M%S")}',
                description=f'Duplicate detection model training with 6 duplicate types: {data_file.file_name}',
                model_type='duplicate_detection_only',
                training_data_size=len(transactions),
                feature_count=12,  # Features specific to duplicate detection
                training_parameters={
                    'auto_training': True,
                    'source_file': str(data_file.id),
                    'transactions_count': len(transactions),
                    'model_type': 'duplicate_only',
                    'duplicate_types': [
                        'Type 1 Duplicate - Account Number + Amount',
                        'Type 2 Duplicate - Account Number + Source + Amount',
                        'Type 3 Duplicate - Account Number + User + Amount',
                        'Type 4 Duplicate - Account Number + Posted Date + Amount',
                        'Type 5 Duplicate - Account Number + Effective Date + Amount',
                        'Type 6 Duplicate - Account Number + Effective Date + Posted Date + User + Source + Amount'
                    ],
                    'duplicate_threshold': 2,
                    'risk_scoring': {
                        'Type 1': 10,
                        'Type 2': 12,
                        'Type 3': 15,
                        'Type 4': 18,
                        'Type 5': 20,
                        'Type 6': 25
                    }
                },
                status='PENDING'
            )
            
            # Train duplicate model synchronously (one-time training)
            try:
                # Use enhanced duplicate detection for training
                from .analytics import SAPGLAnalyzer
                analyzer = SAPGLAnalyzer()
                
                # Get enhanced duplicate data for training
                duplicate_results = analyzer.detect_duplicate_entries(transactions)
                enhanced_duplicates = duplicate_results.get('duplicates', [])
                
                # Train the duplicate model
                training_result = duplicate_model.train_once(
                    transactions=transactions,
                    enhanced_duplicates=enhanced_duplicates,
                    training_session=training_session
                )
                
                if training_result['status'] == 'COMPLETED':
                    # Update training session
                    training_session.training_data_size = len(transactions)
                    training_session.performance_metrics = {
                        'duplicate_groups_found': len(enhanced_duplicates),
                        'duplicate_types_detected': len(set(dup['type'] for dup in enhanced_duplicates)),
                        'model_type': 'duplicate_detection_only',
                        'training_accuracy': training_result.get('accuracy', 0),
                        'duplicate_breakdown': training_result.get('duplicate_breakdown', {})
                    }
                    training_session.status = 'COMPLETED'
                    training_session.save()
                    
                    return {
                        'status': 'COMPLETED',
                        'training_session_id': str(training_session.id),
                        'session_name': training_session.session_name,
                        'transactions_count': len(transactions),
                        'message': 'Duplicate detection model trained successfully with 6 duplicate types',
                        'model_type': 'duplicate_only',
                        'duplicate_types_supported': [
                            'Type 1 Duplicate - Account Number + Amount',
                            'Type 2 Duplicate - Account Number + Source + Amount',
                            'Type 3 Duplicate - Account Number + User + Amount',
                            'Type 4 Duplicate - Account Number + Posted Date + Amount',
                            'Type 5 Duplicate - Account Number + Effective Date + Amount',
                            'Type 6 Duplicate - Account Number + Effective Date + Posted Date + User + Source + Amount'
                        ],
                        'training_results': training_result
                    }
                else:
                    training_session.status = 'FAILED'
                    training_session.error_message = training_result.get('error', 'Unknown training error')
                    training_session.save()
                    
                    return {
                        'status': 'FAILED',
                        'error': training_result.get('error', 'Training failed'),
                        'transactions_count': len(transactions)
                    }
                    
            except Exception as training_error:
                logger.error(f"Duplicate model training failed: {training_error}")
                training_session.status = 'FAILED'
                training_session.error_message = str(training_error)
                training_session.save()
                return {
                    'status': 'FAILED',
                    'error': str(training_error),
                    'transactions_count': len(transactions)
                }
            
        except Exception as e:
            logger.error(f"Error in duplicate model training: {e}")
            return {
                'status': 'FAILED',
                'error': str(e),
                'transactions_count': len(transactions)
            }
    
    def _run_comprehensive_expense_analytics(self, transactions, data_file):
        """Run comprehensive expense analytics on the file data"""
        try:
            if not transactions:
                return {'error': 'No transactions to analyze'}
            
            # Initialize analyzer
            from .analytics import SAPGLAnalyzer
            analyzer = SAPGLAnalyzer()
            
            # Run comprehensive analysis - create a simple analysis session
            from .models import AnalysisSession
            analysis_session = AnalysisSession.objects.create(
                session_name=f"Auto-Analysis-{data_file.file_name}",
                description=f"Automatic analysis for {data_file.file_name}",
                date_from=min(t.posting_date for t in transactions if t.posting_date) if any(t.posting_date for t in transactions) else None,
                date_to=max(t.posting_date for t in transactions if t.posting_date) if any(t.posting_date for t in transactions) else None,
                status='PENDING'
            )
            
            # Run analysis
            analysis_result = analyzer.analyze_transactions(analysis_session)
            
            # Generate detailed expense breakdown
            expense_breakdown = self._generate_expense_breakdown(transactions)
            
            # Generate user expense patterns
            user_patterns = self._generate_user_expense_patterns(transactions)
            
            # Generate account expense patterns
            account_patterns = self._generate_account_expense_patterns(transactions)
            
            # Generate temporal patterns
            temporal_patterns = self._generate_temporal_patterns(transactions)
            
            # Generate risk assessment
            risk_assessment = self._generate_risk_assessment(transactions)
            
            return {
                'summary': {
                    'total_transactions': len(transactions),
                    'total_amount': float(sum(t.amount_local_currency for t in transactions)),
                    'unique_users': len(set(t.user_name for t in transactions)),
                    'unique_accounts': len(set(t.gl_account for t in transactions)),
                    'date_range': {
                        'start': min(t.posting_date for t in transactions if t.posting_date).isoformat() if any(t.posting_date for t in transactions) else None,
                        'end': max(t.posting_date for t in transactions if t.posting_date).isoformat() if any(t.posting_date for t in transactions) else None
                    }
                },
                'expense_breakdown': expense_breakdown,
                'user_patterns': user_patterns,
                'account_patterns': account_patterns,
                'temporal_patterns': temporal_patterns,
                'risk_assessment': risk_assessment,
                'analysis_details': analysis_result
            }
            
        except Exception as e:
            logger.error(f"Error in comprehensive expense analytics: {e}")
            return {'error': str(e)}
    
    def _generate_expense_breakdown(self, transactions):
        """Generate detailed expense breakdown by category"""
        try:
            # Group by GL account
            account_totals = {}
            for transaction in transactions:
                account = transaction.gl_account
                if account not in account_totals:
                    account_totals[account] = {
                        'count': 0,
                        'total_amount': 0,
                        'avg_amount': 0,
                        'min_amount': float('inf'),
                        'max_amount': 0
                    }
                
                amount = float(transaction.amount_local_currency)
                account_totals[account]['count'] += 1
                account_totals[account]['total_amount'] += amount
                account_totals[account]['min_amount'] = min(account_totals[account]['min_amount'], amount)
                account_totals[account]['max_amount'] = max(account_totals[account]['max_amount'], amount)
            
            # Calculate averages
            for account in account_totals:
                account_totals[account]['avg_amount'] = account_totals[account]['total_amount'] / account_totals[account]['count']
            
            # Sort by total amount
            sorted_accounts = sorted(account_totals.items(), key=lambda x: x[1]['total_amount'], reverse=True)
            
            return {
                'by_account': dict(sorted_accounts),
                'top_accounts': sorted_accounts[:10],
                'total_accounts': len(account_totals)
            }
            
        except Exception as e:
            logger.error(f"Error generating expense breakdown: {e}")
            return {'error': str(e)}
    
    def _generate_user_expense_patterns(self, transactions):
        """Generate user expense patterns and analysis"""
        try:
            # Group by user
            user_totals = {}
            user_transactions = {}
            
            for transaction in transactions:
                user = transaction.user_name
                if user not in user_totals:
                    user_totals[user] = {
                        'count': 0,
                        'total_amount': 0,
                        'avg_amount': 0,
                        'min_amount': float('inf'),
                        'max_amount': 0,
                        'accounts_used': set(),
                        'date_range': {'min': None, 'max': None}
                    }
                    user_transactions[user] = []
                
                amount = float(transaction.amount_local_currency)
                user_totals[user]['count'] += 1
                user_totals[user]['total_amount'] += amount
                user_totals[user]['min_amount'] = min(user_totals[user]['min_amount'], amount)
                user_totals[user]['max_amount'] = max(user_totals[user]['max_amount'], amount)
                user_totals[user]['accounts_used'].add(transaction.gl_account)
                
                if transaction.posting_date:
                    if user_totals[user]['date_range']['min'] is None or transaction.posting_date < user_totals[user]['date_range']['min']:
                        user_totals[user]['date_range']['min'] = transaction.posting_date
                    if user_totals[user]['date_range']['max'] is None or transaction.posting_date > user_totals[user]['date_range']['max']:
                        user_totals[user]['date_range']['max'] = transaction.posting_date
                
                user_transactions[user].append(transaction)
            
            # Calculate averages and convert sets to lists
            for user in user_totals:
                user_totals[user]['avg_amount'] = user_totals[user]['total_amount'] / user_totals[user]['count']
                user_totals[user]['accounts_used'] = list(user_totals[user]['accounts_used'])
                user_totals[user]['accounts_count'] = len(user_totals[user]['accounts_used'])
                
                # Convert dates to ISO format
                if user_totals[user]['date_range']['min']:
                    user_totals[user]['date_range']['min'] = user_totals[user]['date_range']['min'].isoformat()
                if user_totals[user]['date_range']['max']:
                    user_totals[user]['date_range']['max'] = user_totals[user]['date_range']['max'].isoformat()
            
            # Sort by total amount
            sorted_users = sorted(user_totals.items(), key=lambda x: x[1]['total_amount'], reverse=True)
            
            return {
                'by_user': dict(sorted_users),
                'top_users': sorted_users[:10],
                'total_users': len(user_totals),
                'user_activity': {
                    'most_active': sorted_users[0] if sorted_users else None,
                    'highest_spender': sorted_users[0] if sorted_users else None,
                    'most_accounts': max(user_totals.items(), key=lambda x: x[1]['accounts_count']) if user_totals else None
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating user patterns: {e}")
            return {'error': str(e)}
    
    def _generate_account_expense_patterns(self, transactions):
        """Generate account expense patterns and analysis"""
        try:
            # Group by account and analyze patterns
            account_analysis = {}
            
            for transaction in transactions:
                account = transaction.gl_account
                if account not in account_analysis:
                    account_analysis[account] = {
                        'transactions': [],
                        'users': set(),
                        'amounts': [],
                        'dates': []
                    }
                
                account_analysis[account]['transactions'].append(transaction)
                account_analysis[account]['users'].add(transaction.user_name)
                account_analysis[account]['amounts'].append(float(transaction.amount_local_currency))
                if transaction.posting_date:
                    account_analysis[account]['dates'].append(transaction.posting_date)
            
            # Analyze patterns for each account
            for account, data in account_analysis.items():
                amounts = data['amounts']
                dates = data['dates']
                
                data['summary'] = {
                    'total_transactions': len(data['transactions']),
                    'total_amount': sum(amounts),
                    'avg_amount': sum(amounts) / len(amounts) if amounts else 0,
                    'min_amount': min(amounts) if amounts else 0,
                    'max_amount': max(amounts) if amounts else 0,
                    'unique_users': len(data['users']),
                    'users_list': list(data['users']),
                    'date_range': {
                        'min': min(dates).isoformat() if dates else None,
                        'max': max(dates).isoformat() if dates else None
                    }
                }
                
                # Remove raw data to keep response clean
                del data['transactions']
                del data['amounts']
                del data['dates']
                data['users'] = list(data['users'])
            
            return {
                'by_account': account_analysis,
                'total_accounts': len(account_analysis),
                'most_used_accounts': sorted(account_analysis.items(), key=lambda x: x[1]['summary']['total_transactions'], reverse=True)[:10]
            }
            
        except Exception as e:
            logger.error(f"Error generating account patterns: {e}")
            return {'error': str(e)}
    
    def _generate_temporal_patterns(self, transactions):
        """Generate temporal patterns in expense data"""
        try:
            # Group by month
            monthly_data = {}
            # Group by day of week
            daily_data = {}
            
            for transaction in transactions:
                if not transaction.posting_date:
                    continue
                
                # Monthly patterns
                month_key = transaction.posting_date.strftime('%Y-%m')
                if month_key not in monthly_data:
                    monthly_data[month_key] = {'count': 0, 'amount': 0}
                monthly_data[month_key]['count'] += 1
                monthly_data[month_key]['amount'] += float(transaction.amount_local_currency)
                
                # Daily patterns
                day_key = transaction.posting_date.strftime('%A')
                if day_key not in daily_data:
                    daily_data[day_key] = {'count': 0, 'amount': 0}
                daily_data[day_key]['count'] += 1
                daily_data[day_key]['amount'] += float(transaction.amount_local_currency)
            
            # Calculate averages
            for month in monthly_data:
                monthly_data[month]['avg_amount'] = monthly_data[month]['amount'] / monthly_data[month]['count']
            
            for day in daily_data:
                daily_data[day]['avg_amount'] = daily_data[day]['amount'] / daily_data[day]['count']
            
            return {
                'monthly_patterns': monthly_data,
                'daily_patterns': daily_data,
                'peak_months': sorted(monthly_data.items(), key=lambda x: x[1]['amount'], reverse=True)[:3],
                'peak_days': sorted(daily_data.items(), key=lambda x: x[1]['amount'], reverse=True)[:3]
            }
            
        except Exception as e:
            logger.error(f"Error generating temporal patterns: {e}")
            return {'error': str(e)}
    
    def _generate_risk_assessment(self, transactions):
        """Generate risk assessment for expense data"""
        try:
            risk_factors = {
                'high_value_transactions': 0,
                'unusual_patterns': 0,
                'weekend_transactions': 0,
                'holiday_transactions': 0,
                'late_hour_transactions': 0,
                'duplicate_amounts': 0,
                'round_amounts': 0
            }
            
            amounts = []
            dates = []
            
            for transaction in transactions:
                amount = float(transaction.amount_local_currency)
                amounts.append(amount)
                
                if transaction.posting_date:
                    dates.append(transaction.posting_date)
                
                # Check for high value transactions (above 95th percentile)
                if amount > 1000000:  # 1M SAR threshold
                    risk_factors['high_value_transactions'] += 1
                
                # Check for round amounts
                if amount % 1000 == 0:
                    risk_factors['round_amounts'] += 1
            
            # Calculate statistics
            if amounts:
                mean_amount = sum(amounts) / len(amounts)
                sorted_amounts = sorted(amounts)
                median_amount = sorted_amounts[len(sorted_amounts) // 2]
                p95_amount = sorted_amounts[int(len(sorted_amounts) * 0.95)]
                
                # Check for unusual patterns
                for amount in amounts:
                    if amount > p95_amount * 2:
                        risk_factors['unusual_patterns'] += 1
            else:
                mean_amount = median_amount = p95_amount = 0
            
            # Calculate risk score
            total_transactions = len(transactions)
            risk_score = 0
            
            if total_transactions > 0:
                risk_score += (risk_factors['high_value_transactions'] / total_transactions) * 30
                risk_score += (risk_factors['unusual_patterns'] / total_transactions) * 25
                risk_score += (risk_factors['round_amounts'] / total_transactions) * 15
            
            # Determine risk level
            if risk_score > 50:
                risk_level = 'HIGH'
            elif risk_score > 25:
                risk_level = 'MEDIUM'
            else:
                risk_level = 'LOW'
            
            return {
                'risk_factors': risk_factors,
                'risk_score': round(risk_score, 2),
                'risk_level': risk_level,
                'statistics': {
                    'mean_amount': mean_amount,
                    'median_amount': median_amount,
                    'p95_amount': p95_amount,
                    'total_transactions': total_transactions
                },
                'recommendations': self._generate_risk_recommendations(risk_factors, risk_score)
            }
            
        except Exception as e:
            logger.error(f"Error generating risk assessment: {e}")
            return {'error': str(e)}
    
    def _generate_risk_recommendations(self, risk_factors, risk_score):
        """Generate risk-based recommendations"""
        recommendations = []
        
        if risk_factors['high_value_transactions'] > 0:
            recommendations.append({
                'type': 'HIGH_VALUE',
                'message': f"Found {risk_factors['high_value_transactions']} high-value transactions (>1M SAR). Review these for approval compliance.",
                'priority': 'HIGH'
            })
        
        if risk_factors['unusual_patterns'] > 0:
            recommendations.append({
                'type': 'UNUSUAL_PATTERNS',
                'message': f"Found {risk_factors['unusual_patterns']} transactions with unusual patterns. Investigate for potential anomalies.",
                'priority': 'MEDIUM'
            })
        
        if risk_factors['round_amounts'] > len(risk_factors) * 0.1:
            recommendations.append({
                'type': 'ROUND_AMOUNTS',
                'message': f"High percentage of round amounts ({risk_factors['round_amounts']} transactions). Verify supporting documentation.",
                'priority': 'LOW'
            })
        
        if risk_score > 50:
            recommendations.append({
                'type': 'OVERALL_RISK',
                'message': f"Overall risk score is {risk_score} (HIGH). Consider detailed audit review.",
                'priority': 'HIGH'
            })
        
        return recommendations

class FileProcessingJobViewSet(viewsets.ReadOnlyModelViewSet):
    """ViewSet for FileProcessingJob management"""
    
    queryset = FileProcessingJob.objects.all()
    serializer_class = FileProcessingJobSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return FileProcessingJobListSerializer
        return FileProcessingJobSerializer
    
    def get_queryset(self):
        queryset = FileProcessingJob.objects.all()
        
        # Apply filters
        status_filter = self.request.query_params.get('status', None)
        if status_filter:
            queryset = queryset.filter(status=status_filter)
        
        run_anomalies = self.request.query_params.get('run_anomalies', None)
        if run_anomalies is not None:
            queryset = queryset.filter(run_anomalies=run_anomalies.lower() == 'true')
        
        date_from = self.request.query_params.get('date_from', None)
        if date_from:
            queryset = queryset.filter(created_at__date__gte=date_from)
        
        date_to = self.request.query_params.get('date_to', None)
        if date_to:
            queryset = queryset.filter(created_at__date__lte=date_to)
        
        return queryset.order_by('-created_at')
    
    @action(detail=True, methods=['get'])
    def status(self, request, pk=None):
        """Get processing job status and results"""
        try:
            job = self.get_object()
            
            response_data = {
                'job_id': str(job.id),
                'status': job.status,
                'file_info': DataFileSerializer(job.data_file).data,
                'run_anomalies': job.run_anomalies,
                'requested_anomalies': job.requested_anomalies,
                'processing_duration': job.processing_duration,
                'is_duplicate_content': job.is_duplicate_content,
                'existing_job_id': str(job.existing_job.id) if job.existing_job else None,
                'created_at': job.created_at,
                'started_at': job.started_at,
                'completed_at': job.completed_at,
            }
            
            # Add results if completed
            if job.status == 'COMPLETED':
                response_data['analytics_results'] = job.analytics_results
                response_data['anomaly_results'] = job.anomaly_results
                response_data['message'] = 'Processing completed successfully'
            elif job.status == 'FAILED':
                response_data['error_message'] = job.error_message
                response_data['message'] = 'Processing failed'
            elif job.status == 'CELERY_ERROR':
                response_data['error_message'] = job.error_message
                response_data['message'] = 'Processing failed due to Celery connection error'
                response_data['can_retry'] = True
                response_data['retry_endpoint'] = f'/api/file-processing-jobs/{job.id}/retry/'
            elif job.status == 'SKIPPED':
                response_data['message'] = 'File content already processed'
            else:
                response_data['message'] = 'Processing in progress'
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error getting job status: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['post'])
    def retry(self, request, pk=None):
        """Retry a failed or Celery error job"""
        try:
            job = self.get_object()
            
            # Check if job can be retried
            if job.status not in ['FAILED', 'CELERY_ERROR']:
                return Response({
                    'error': f'Job cannot be retried. Current status: {job.status}',
                    'allowed_statuses': ['FAILED', 'CELERY_ERROR']
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # Reset job status and clear error message
            job.status = 'PENDING'
            job.error_message = None
            job.started_at = None
            job.completed_at = None
            job.processing_duration = None
            job.analytics_results = {}
            job.anomaly_results = {}
            job.save()
            
            # Job is already in queue - no direct Celery call needed
            return Response({
                'job_id': str(job.id),
                'status': 'QUEUED',
                'message': 'Job retry initiated successfully - job added to queue',
                'status_endpoint': f'/api/file-processing-jobs/{job.id}/status/'
            }, status=status.HTTP_202_ACCEPTED)
            
        except Exception as e:
            logger.error(f"Error retrying job: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class MLModelTrainingViewSet(viewsets.ModelViewSet):
    """ViewSet for ML Model Training management"""
    
    queryset = MLModelTraining.objects.all()
    serializer_class = MLModelTrainingSerializer
    
    def get_serializer_class(self):
        if self.action == 'list':
            return MLModelTrainingListSerializer
        return MLModelTrainingSerializer
    
    @action(detail=False, methods=['post'])
    def train_models(self, request):
        """Train ML models on historical data"""
        try:
            serializer = MLModelTrainingRequestSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Create training session
            training_session = MLModelTraining.objects.create(
                session_name=serializer.validated_data['session_name'],
                description=serializer.validated_data.get('description', ''),
                model_type=serializer.validated_data['model_type'],
                training_parameters=serializer.validated_data.get('training_parameters', {}),
                status='PENDING'
            )
            
            # Create ML training job in queue instead of direct Celery call
            # The ML training will be picked up by the queue processor
            return Response({
                'training_id': str(training_session.id),
                'status': 'QUEUED',
                'message': 'ML model training queued for processing'
            }, status=status.HTTP_202_ACCEPTED)
            
        except Exception as e:
            logger.error(f"Error starting ML model training: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['get'])
    def status(self, request, pk=None):
        """Get training status"""
        try:
            training_session = self.get_object()
            return Response(training_session.get_training_summary())
        except MLModelTraining.DoesNotExist:
            return Response(
                {'error': 'Training session not found'},
                status=status.HTTP_404_NOT_FOUND
            )
    
    @action(detail=False, methods=['get'])
    def model_info(self, request):
        """Get information about trained models"""
        try:
            ml_detector = MLAnomalyDetector()
            ml_detector.load_models_from_memory()
            
            # Get latest performance metrics
            latest_training = MLModelTraining.objects.filter(
                status='COMPLETED'
            ).order_by('-created_at').first()
            
            latest_performance = None
            if latest_training:
                latest_performance = latest_training.performance_metrics
            
            # Get training history
            training_history = MLModelTraining.objects.filter(
                status='COMPLETED'
            ).order_by('-created_at')[:10]
            
            model_info = ml_detector.get_model_info()
            model_info['latest_performance'] = latest_performance
            model_info['training_history'] = MLModelTrainingListSerializer(
                training_history, many=True
            ).data
            
            serializer = MLModelInfoSerializer(model_info)
            return Response(serializer.data)
            
        except Exception as e:
            logger.error(f"Error getting model info: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['post'])
    def predict_anomalies(self, request):
        """Predict anomalies using trained ML models"""
        try:
            # Get transaction IDs from request
            transaction_ids = request.data.get('transaction_ids', [])
            use_ensemble = request.data.get('use_ensemble', True)
            
            if not transaction_ids:
                return Response(
                    {'error': 'No transaction IDs provided'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get transactions
            transactions = SAPGLPosting.objects.filter(id__in=transaction_ids)
            
            if not transactions:
                return Response(
                    {'error': 'No transactions found'},
                    status=status.HTTP_404_NOT_FOUND
                )
            
                    # Load ML detector
            ml_detector = MLAnomalyDetector()
            if not ml_detector.load_models_from_memory():
                return Response(
                    {'error': 'No trained models available. Please train models first.'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Make predictions
            import time
            start_time = time.time()
            
            if use_ensemble:
                predictions = ml_detector.ensemble_predict(transactions)
                individual_predictions = {}
            else:
                individual_predictions = ml_detector.predict_anomalies(transactions)
                predictions = []
            
            processing_time = time.time() - start_time
            
            # Prepare response
            response_data = {
                'total_anomalies': len(predictions),
                'processing_time': processing_time,
                'model_performance': {},
                'ensemble_predictions': predictions if use_ensemble else [],
            }
            
            # Add individual model predictions if not using ensemble
            if not use_ensemble:
                for model_name, model_predictions in individual_predictions.items():
                    response_data[model_name] = model_predictions
                    response_data['total_anomalies'] += len(model_predictions)
            
            serializer = MLAnomalyResultsSerializer(response_data)
            return Response(serializer.data)
            
        except Exception as e:
            logger.error(f"Error in ML anomaly prediction: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=False, methods=['post'])
    def retrain_models(self, request):
        """Retrain models with new data"""
        try:
            serializer = MLModelTrainingRequestSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            
            # Create retraining session
            training_session = MLModelTraining.objects.create(
                session_name=f"Retraining - {serializer.validated_data['session_name']}",
                description=serializer.validated_data.get('description', 'Model retraining session'),
                model_type=serializer.validated_data['model_type'],
                training_parameters=serializer.validated_data.get('training_parameters', {}),
                status='PENDING'
            )
            
            # Create ML retraining job in queue instead of direct Celery call
            # The ML retraining will be picked up by the queue processor
            return Response({
                'training_id': str(training_session.id),
                'status': 'QUEUED',
                'message': 'ML model retraining queued for processing'
            }, status=status.HTTP_202_ACCEPTED)
            
        except Exception as e:
            logger.error(f"Error starting ML model retraining: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AllFilesListView(generics.ListAPIView):
    """Comprehensive view to get list of all uploaded files with processing status"""
    
    def get(self, request, *args, **kwargs):
        """Get all files with comprehensive information"""
        try:
            # Get all DataFiles
            data_files = DataFile.objects.all().order_by('-uploaded_at')
            
            # Get all FileProcessingJobs
            processing_jobs = FileProcessingJob.objects.all().order_by('-created_at')
            
            # Create a mapping of data_file.id to processing_job for quick lookup
            job_map = {job.data_file_id: job for job in processing_jobs}
            
            files_data = []
            
            for data_file in data_files:
                # Get the corresponding processing job
                processing_job = job_map.get(data_file.id)
                
                # Get transactions for this file (based on upload date range)
                transactions = SAPGLPosting.objects.filter(
                    posting_date__gte=data_file.audit_start_date,
                    posting_date__lte=data_file.audit_end_date,
                    fiscal_year=data_file.fiscal_year
                ) if data_file.audit_start_date and data_file.audit_end_date else []
                
                # Get related analysis sessions
                related_sessions = AnalysisSession.objects.filter(
                    created_at__gte=data_file.uploaded_at
                ).order_by('-created_at')[:5]  # Limit to 5 most recent
                
                file_data = {
                    'file_id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'file_size': data_file.file_size,
                    'engagement_id': data_file.engagement_id,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'audit_start_date': data_file.audit_start_date.isoformat() if data_file.audit_start_date else None,
                    'audit_end_date': data_file.audit_end_date.isoformat() if data_file.audit_end_date else None,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'status': data_file.status,
                    'total_records': data_file.total_records or 0,
                    'processed_records': data_file.processed_records or 0,
                    'failed_records': data_file.failed_records or 0,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None,
                    
                    # Processing job information
                    'processing_job': {
                        'job_id': str(processing_job.id) if processing_job else None,
                        'status': processing_job.status if processing_job else None,
                        'run_anomalies': processing_job.run_anomalies if processing_job else False,
                        'requested_anomalies': processing_job.requested_anomalies if processing_job else [],
                        'created_at': processing_job.created_at.isoformat() if processing_job and processing_job.created_at else None,
                        'started_at': processing_job.started_at.isoformat() if processing_job and processing_job.started_at else None,
                        'completed_at': processing_job.completed_at.isoformat() if processing_job and processing_job.completed_at else None,
                        'processing_duration': processing_job.processing_duration if processing_job else None,
                        'error_message': processing_job.error_message if processing_job else None,
                        'analytics_results': processing_job.analytics_results if processing_job else {},
                        'anomaly_results': processing_job.anomaly_results if processing_job else {},
                    } if processing_job else None,
                    
                    # Transaction information
                    'transactions': {
                        'count': len(transactions),
                        'total_amount': float(sum(t.amount_local_currency for t in transactions)) if transactions else 0,
                        'unique_accounts': len(set(t.gl_account for t in transactions)) if transactions else 0,
                        'unique_users': len(set(t.user_name for t in transactions)) if transactions else 0,
                    },
                    
                    # Analysis sessions
                    'analysis_sessions': [
                        {
                            'id': str(session.id),
                            'session_name': session.session_name,
                            'status': session.status,
                            'created_at': session.created_at.isoformat() if session.created_at else None,
                            'started_at': session.started_at.isoformat() if session.started_at else None,
                            'completed_at': session.completed_at.isoformat() if session.completed_at else None,
                            'total_transactions': session.total_transactions or 0,
                            'flagged_transactions': session.flagged_transactions or 0,
                            'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions and session.total_transactions > 0 else 0
                        }
                        for session in related_sessions
                    ],
                    
                    # File hash for duplicate detection
                    'file_hash': processing_job.file_hash if processing_job else None,
                }
                
                files_data.append(file_data)
            
            # Calculate summary statistics
            total_files = len(files_data)
            total_records = sum(file['total_records'] for file in files_data)
            total_processed = sum(file['processed_records'] for file in files_data)
            total_failed = sum(file['failed_records'] for file in files_data)
            
            # Count files by status
            status_counts = {}
            for file in files_data:
                status = file['status']
                status_counts[status] = status_counts.get(status, 0) + 1
            
            # Count processing jobs by status
            job_status_counts = {}
            for job in processing_jobs:
                status = job.status
                job_status_counts[status] = job_status_counts.get(status, 0) + 1
            
            # Get recent activity
            recent_files = files_data[:10]  # Last 10 files
            
            response_data = {
                'files': files_data,
                'summary': {
                    'total_files': total_files,
                    'total_records': total_records,
                    'total_processed': total_processed,
                    'total_failed': total_failed,
                    'success_rate': round((total_processed / total_records * 100), 2) if total_records > 0 else 0,
                    'file_status_counts': status_counts,
                    'job_status_counts': job_status_counts,
                    'recent_activity': {
                        'files_uploaded_today': len([f for f in files_data if f['uploaded_at'] and f['uploaded_at'].startswith(timezone.now().date().isoformat())]),
                        'files_processed_today': len([f for f in files_data if f['processed_at'] and f['processed_at'].startswith(timezone.now().date().isoformat())]),
                        'processing_jobs_completed': job_status_counts.get('COMPLETED', 0),
                        'processing_jobs_failed': job_status_counts.get('FAILED', 0) + job_status_counts.get('CELERY_ERROR', 0),
                    }
                },
                'filters': {
                    'available_statuses': list(status_counts.keys()),
                    'available_clients': list(set(f['client_name'] for f in files_data if f['client_name'])),
                    'available_companies': list(set(f['company_name'] for f in files_data if f['company_name'])),
                    'fiscal_years': sorted(list(set(f['fiscal_year'] for f in files_data if f['fiscal_year'])), reverse=True),
                }
            }
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error in AllFilesListView: {e}")
            from rest_framework import status
            return Response(
                {'error': f'Error retrieving files: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AnalysisAPIView(generics.GenericAPIView):
    """Comprehensive API to retrieve analysis results for files, jobs, and transactions"""
    
    def get(self, request, *args, **kwargs):
        """Get analysis results based on query parameters"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            from django.db.models import Q
            
            # Get query parameters
            analysis_type = request.query_params.get('type', 'all')  # file, job, transaction, all
            file_id = request.query_params.get('file_id')
            job_id = request.query_params.get('job_id')
            transaction_id = request.query_params.get('transaction_id')
            
            # Clean UUID parameters (remove trailing slashes)
            if file_id:
                file_id = file_id.rstrip('/')
            if job_id:
                job_id = job_id.rstrip('/')
            if transaction_id:
                transaction_id = transaction_id.rstrip('/')
            date_from = request.query_params.get('date_from')
            date_to = request.query_params.get('date_to')
            anomaly_type = request.query_params.get('anomaly_type')
            risk_level = request.query_params.get('risk_level')
            
            response_data = {}
            
            # Get file analysis
            if analysis_type in ['file', 'all']:
                if file_id:
                    # Get specific file analysis
                    data_file = get_object_or_404(DataFile, id=file_id)
                    processing_job = FileProcessingJob.objects.filter(data_file=data_file).first()
                    
                    file_analysis = {
                        'file_info': {
                            'id': str(data_file.id),
                            'file_name': data_file.file_name,
                            'client_name': data_file.client_name,
                            'company_name': data_file.company_name,
                            'fiscal_year': data_file.fiscal_year,
                            'status': data_file.status,
                            'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                            'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None,
                            'total_records': data_file.total_records,
                            'processed_records': data_file.processed_records,
                            'failed_records': data_file.failed_records,
                        },
                        'processing_job': {
                            'job_id': str(processing_job.id) if processing_job else None,
                            'status': processing_job.status if processing_job else None,
                            'run_anomalies': processing_job.run_anomalies if processing_job else False,
                            'requested_anomalies': processing_job.requested_anomalies if processing_job else [],
                            'analytics_results': processing_job.analytics_results if processing_job else {},
                            'anomaly_results': processing_job.anomaly_results if processing_job else {},
                            'processing_duration': processing_job.processing_duration if processing_job else None,
                            'error_message': processing_job.error_message if processing_job else None,
                        } if processing_job else None,
                        'analysis_sessions': [
                            {
                                'id': str(session.id),
                                'session_name': session.session_name,
                                'status': session.status,
                                'total_transactions': session.total_transactions,
                                'flagged_transactions': session.flagged_transactions,
                                'high_value_transactions': session.high_value_transactions,
                                'created_at': session.created_at.isoformat() if session.created_at else None,
                                'completed_at': session.completed_at.isoformat() if session.completed_at else None,
                            }
                            for session in (AnalysisSession.objects.filter(
                                date_from__gte=data_file.audit_start_date,
                                date_to__lte=data_file.audit_end_date
                            ) if data_file.audit_start_date and data_file.audit_end_date else [])
                        ]
                    }
                    response_data['file_analysis'] = file_analysis
                else:
                    # Get all files analysis summary
                    files = DataFile.objects.all()
                    if date_from:
                        files = files.filter(uploaded_at__date__gte=date_from)
                    if date_to:
                        files = files.filter(uploaded_at__date__lte=date_to)
                    
                    files_analysis = {
                        'total_files': files.count(),
                        'files_by_status': {},
                        'files_by_client': {},
                        'files_by_fiscal_year': {},
                        'processing_summary': {
                            'total_processed': 0,
                            'total_failed': 0,
                            'total_pending': 0,
                            'success_rate': 0,
                        }
                    }
                    
                    for file in files:
                        # Status breakdown
                        status = file.status
                        files_analysis['files_by_status'][status] = files_analysis['files_by_status'].get(status, 0) + 1
                        
                        # Client breakdown
                        if file.client_name:
                            files_analysis['files_by_client'][file.client_name] = files_analysis['files_by_client'].get(file.client_name, 0) + 1
                        
                        # Fiscal year breakdown
                        if file.fiscal_year:
                            files_analysis['files_by_fiscal_year'][file.fiscal_year] = files_analysis['files_by_fiscal_year'].get(file.fiscal_year, 0) + 1
                        
                        # Processing summary
                        files_analysis['processing_summary']['total_processed'] += file.processed_records
                        files_analysis['processing_summary']['total_failed'] += file.failed_records
                        if file.status == 'PENDING':
                            files_analysis['processing_summary']['total_pending'] += 1
                    
                    # Calculate success rate
                    total_records = files_analysis['processing_summary']['total_processed'] + files_analysis['processing_summary']['total_failed']
                    if total_records > 0:
                        files_analysis['processing_summary']['success_rate'] = round(
                            (files_analysis['processing_summary']['total_processed'] / total_records) * 100, 2
                        )
                    
                    response_data['files_analysis'] = files_analysis
            
            # Get job analysis
            if analysis_type in ['job', 'all']:
                if job_id:
                    # Get specific job analysis
                    processing_job = get_object_or_404(FileProcessingJob, id=job_id)
                    
                    job_analysis = {
                        'job_info': {
                            'id': str(processing_job.id),
                            'status': processing_job.status,
                            'run_anomalies': processing_job.run_anomalies,
                            'requested_anomalies': processing_job.requested_anomalies,
                            'created_at': processing_job.created_at.isoformat() if processing_job.created_at else None,
                            'started_at': processing_job.started_at.isoformat() if processing_job.started_at else None,
                            'completed_at': processing_job.completed_at.isoformat() if processing_job.completed_at else None,
                            'processing_duration': processing_job.processing_duration,
                            'error_message': processing_job.error_message,
                        },
                        'analytics_results': processing_job.analytics_results or {},
                        'anomaly_results': processing_job.anomaly_results or {},
                        'file_info': {
                            'id': str(processing_job.data_file.id),
                            'file_name': processing_job.data_file.file_name,
                            'client_name': processing_job.data_file.client_name,
                            'company_name': processing_job.data_file.company_name,
                        } if processing_job.data_file else None,
                    }
                    response_data['job_analysis'] = job_analysis
                else:
                    # Get all jobs analysis summary
                    jobs = FileProcessingJob.objects.all()
                    if date_from:
                        jobs = jobs.filter(created_at__date__gte=date_from)
                    if date_to:
                        jobs = jobs.filter(created_at__date__lte=date_to)
                    
                    jobs_analysis = {
                        'total_jobs': jobs.count(),
                        'jobs_by_status': {},
                        'anomaly_requests': {
                            'total_with_anomalies': 0,
                            'anomaly_types_requested': {},
                            'successful_anomaly_detection': 0,
                        },
                        'processing_performance': {
                            'avg_duration': 0,
                            'max_duration': 0,
                            'min_duration': 0,
                        }
                    }
                    
                    durations = []
                    for job in jobs:
                        # Status breakdown
                        status = job.status
                        jobs_analysis['jobs_by_status'][status] = jobs_analysis['jobs_by_status'].get(status, 0) + 1
                        
                        # Anomaly requests
                        if job.run_anomalies:
                            jobs_analysis['anomaly_requests']['total_with_anomalies'] += 1
                            for anomaly_type in job.requested_anomalies:
                                jobs_analysis['anomaly_requests']['anomaly_types_requested'][anomaly_type] = \
                                    jobs_analysis['anomaly_requests']['anomaly_types_requested'].get(anomaly_type, 0) + 1
                        
                        # Processing performance
                        if job.processing_duration:
                            durations.append(job.processing_duration)
                    
                    if durations:
                        jobs_analysis['processing_performance']['avg_duration'] = round(sum(durations) / len(durations), 2)
                        jobs_analysis['processing_performance']['max_duration'] = max(durations)
                        jobs_analysis['processing_performance']['min_duration'] = min(durations)
                    
                    response_data['jobs_analysis'] = jobs_analysis
            
            # Get transaction analysis
            if analysis_type in ['transaction', 'all']:
                if transaction_id:
                    # Get specific transaction analysis
                    transaction = get_object_or_404(SAPGLPosting, id=transaction_id)
                    analysis = TransactionAnalysis.objects.filter(transaction=transaction).first()
                    
                    transaction_analysis = {
                        'transaction_info': {
                            'id': str(transaction.id),
                            'document_number': transaction.document_number,
                            'posting_date': transaction.posting_date.isoformat() if transaction.posting_date else None,
                            'gl_account': transaction.gl_account,
                            'amount': float(transaction.amount_local_currency),
                            'transaction_type': transaction.transaction_type,
                            'user_name': transaction.user_name,
                            'text': transaction.text,
                        },
                        'analysis': {
                            'risk_score': analysis.risk_score if analysis else 0,
                            'risk_level': analysis.risk_level if analysis else 'LOW',
                            'amount_anomaly': analysis.amount_anomaly if analysis else False,
                            'timing_anomaly': analysis.timing_anomaly if analysis else False,
                            'user_anomaly': analysis.user_anomaly if analysis else False,
                            'account_anomaly': analysis.account_anomaly if analysis else False,
                            'pattern_anomaly': analysis.pattern_anomaly if analysis else False,
                            'analysis_details': analysis.analysis_details if analysis else {},
                        } if analysis else None,
                    }
                    response_data['transaction_analysis'] = transaction_analysis
                else:
                    # Get all transactions analysis summary
                    transactions = SAPGLPosting.objects.all()
                    analyses = TransactionAnalysis.objects.all()
                    
                    # Apply filters
                    if date_from:
                        transactions = transactions.filter(posting_date__gte=date_from)
                    if date_to:
                        transactions = transactions.filter(posting_date__lte=date_to)
                    if anomaly_type:
                        if anomaly_type == 'amount':
                            analyses = analyses.filter(amount_anomaly=True)
                        elif anomaly_type == 'timing':
                            analyses = analyses.filter(timing_anomaly=True)
                        elif anomaly_type == 'user':
                            analyses = analyses.filter(user_anomaly=True)
                        elif anomaly_type == 'account':
                            analyses = analyses.filter(account_anomaly=True)
                        elif anomaly_type == 'pattern':
                            analyses = analyses.filter(pattern_anomaly=True)
                    if risk_level:
                        analyses = analyses.filter(risk_level=risk_level.upper())
                    
                    transaction_analysis = {
                        'total_transactions': transactions.count(),
                        'analyzed_transactions': analyses.count(),
                        'risk_distribution': {},
                        'anomaly_distribution': {
                            'amount_anomalies': analyses.filter(amount_anomaly=True).count(),
                            'timing_anomalies': analyses.filter(timing_anomaly=True).count(),
                            'user_anomalies': analyses.filter(user_anomaly=True).count(),
                            'account_anomalies': analyses.filter(account_anomaly=True).count(),
                            'pattern_anomalies': analyses.filter(pattern_anomaly=True).count(),
                        },
                        'top_high_risk_transactions': [
                            {
                                'id': str(analysis.transaction.id),
                                'document_number': analysis.transaction.document_number,
                                'risk_score': analysis.risk_score,
                                'risk_level': analysis.risk_level,
                                'posting_date': analysis.transaction.posting_date.isoformat() if analysis.transaction.posting_date else None,
                                'amount': float(analysis.transaction.amount_local_currency),
                                'user_name': analysis.transaction.user_name,
                            }
                            for analysis in analyses.order_by('-risk_score')[:10]
                        ],
                    }
                    
                    # Risk distribution
                    for analysis in analyses:
                        risk_level = analysis.risk_level
                        transaction_analysis['risk_distribution'][risk_level] = \
                            transaction_analysis['risk_distribution'].get(risk_level, 0) + 1
                    
                    response_data['transactions_analysis'] = transaction_analysis
            
            # Get ML model analysis
            if analysis_type in ['ml', 'all']:
                ml_models = MLModelTraining.objects.all()
                if date_from:
                    ml_models = ml_models.filter(created_at__date__gte=date_from)
                if date_to:
                    ml_models = ml_models.filter(created_at__date__lte=date_to)
                
                ml_analysis = {
                    'total_models': ml_models.count(),
                    'models_by_type': {},
                    'models_by_status': {},
                    'performance_summary': {
                        'avg_accuracy': 0,
                        'best_performing_model': None,
                        'recent_training_sessions': [],
                    },
                    'training_history': [
                        {
                            'id': str(model.id),
                            'session_name': model.session_name,
                            'model_type': model.model_type,
                            'status': model.status,
                            'training_data_size': model.training_data_size,
                            'performance_metrics': model.performance_metrics,
                            'training_duration': model.training_duration,
                            'created_at': model.created_at.isoformat() if model.created_at else None,
                            'completed_at': model.completed_at.isoformat() if model.completed_at else None,
                        }
                        for model in ml_models.order_by('-created_at')[:10]
                    ]
                }
                
                accuracies = []
                for model in ml_models:
                    # Type breakdown
                    model_type = model.model_type
                    ml_analysis['models_by_type'][model_type] = ml_analysis['models_by_type'].get(model_type, 0) + 1
                    
                    # Status breakdown
                    status = model.status
                    ml_analysis['models_by_status'][status] = ml_analysis['models_by_status'].get(status, 0) + 1
                    
                    # Performance metrics
                    if model.performance_metrics and 'accuracy' in model.performance_metrics:
                        accuracy = model.performance_metrics['accuracy']
                        accuracies.append(accuracy)
                        
                        # Track best performing model
                        if not ml_analysis['performance_summary']['best_performing_model'] or \
                           accuracy > ml_analysis['performance_summary']['best_performing_model']['accuracy']:
                            ml_analysis['performance_summary']['best_performing_model'] = {
                                'id': str(model.id),
                                'session_name': model.session_name,
                                'model_type': model.model_type,
                                'accuracy': accuracy,
                            }
                
                if accuracies:
                    ml_analysis['performance_summary']['avg_accuracy'] = round(sum(accuracies) / len(accuracies), 4)
                
                response_data['ml_analysis'] = ml_analysis
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error in AnalysisAPIView: {e}")
            from rest_framework import status
            return Response(
                {'error': f'Error retrieving analysis: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class FileAnalysisView(generics.RetrieveAPIView):
    """Dedicated API to get analysis results for a specific file by ID"""
    
    def get(self, request, file_id, *args, **kwargs):
        """Get comprehensive analysis for a specific file"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            import uuid
            
            # Clean file_id (remove trailing slashes)
            file_id = file_id.rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get the file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Get the processing job for this file
            processing_job = FileProcessingJob.objects.filter(data_file=data_file).first()
            
            # Get transactions for this file (based on audit date range)
            transactions = []
            if data_file.audit_start_date and data_file.audit_end_date:
                transactions = SAPGLPosting.objects.filter(
                    posting_date__gte=data_file.audit_start_date,
                    posting_date__lte=data_file.audit_end_date,
                    fiscal_year=data_file.fiscal_year
                )
            
            # Get analysis sessions for this file
            analysis_sessions = []
            if data_file.audit_start_date and data_file.audit_end_date:
                analysis_sessions = AnalysisSession.objects.filter(
                    date_from__gte=data_file.audit_start_date,
                    date_to__lte=data_file.audit_end_date
                )
            
            # Get transaction analyses for transactions in this file
            transaction_analyses = []
            if transactions:
                transaction_analyses = TransactionAnalysis.objects.filter(
                    transaction__in=transactions
                ).select_related('transaction')
            
            # Build comprehensive analysis response
            analysis_data = {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'file_size': data_file.file_size,
                    'engagement_id': data_file.engagement_id,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'audit_start_date': data_file.audit_start_date.isoformat() if data_file.audit_start_date else None,
                    'audit_end_date': data_file.audit_end_date.isoformat() if data_file.audit_end_date else None,
                    'status': data_file.status,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None,
                    'error_message': data_file.error_message,
                    'total_records': data_file.total_records,
                    'processed_records': data_file.processed_records,
                    'failed_records': data_file.failed_records,
                    'success_rate': round((data_file.processed_records / data_file.total_records * 100), 2) if data_file.total_records > 0 else 0,
                },
                
                'processing_job': {
                    'job_id': str(processing_job.id) if processing_job else None,
                    'status': processing_job.status if processing_job else None,
                    'run_anomalies': processing_job.run_anomalies if processing_job else False,
                    'requested_anomalies': processing_job.requested_anomalies if processing_job else [],
                    'created_at': processing_job.created_at.isoformat() if processing_job and processing_job.created_at else None,
                    'started_at': processing_job.started_at.isoformat() if processing_job and processing_job.started_at else None,
                    'completed_at': processing_job.completed_at.isoformat() if processing_job and processing_job.completed_at else None,
                    'processing_duration': processing_job.processing_duration if processing_job else None,
                    'error_message': processing_job.error_message if processing_job else None,
                    'file_hash': processing_job.file_hash if processing_job else None,
                    'is_duplicate_content': processing_job.is_duplicate_content if processing_job else False,
                } if processing_job else None,
                
                'analytics_results': processing_job.analytics_results if processing_job else {},
                'anomaly_results': processing_job.anomaly_results if processing_job else {},
                
                'transactions_summary': {
                    'total_transactions': len(transactions),
                    'total_amount': float(sum(t.amount_local_currency for t in transactions)) if transactions else 0,
                    'unique_accounts': len(set(t.gl_account for t in transactions)) if transactions else 0,
                    'unique_users': len(set(t.user_name for t in transactions)) if transactions else 0,
                    'unique_documents': len(set(t.document_number for t in transactions)) if transactions else 0,
                    'date_range': {
                        'min_date': min(t.posting_date for t in transactions).isoformat() if transactions else None,
                        'max_date': max(t.posting_date for t in transactions).isoformat() if transactions else None,
                    } if transactions else None,
                    'amount_range': {
                        'min_amount': float(min(t.amount_local_currency for t in transactions)) if transactions else None,
                        'max_amount': float(max(t.amount_local_currency for t in transactions)) if transactions else None,
                    } if transactions else None,
                },
                
                'analysis_sessions': [
                    {
                        'id': str(session.id),
                        'session_name': session.session_name,
                        'description': session.description,
                        'status': session.status,
                        'created_at': session.created_at.isoformat() if session.created_at else None,
                        'started_at': session.started_at.isoformat() if session.started_at else None,
                        'completed_at': session.completed_at.isoformat() if session.completed_at else None,
                        'total_transactions': session.total_transactions,
                        'total_amount': float(session.total_amount) if session.total_amount else 0,
                        'flagged_transactions': session.flagged_transactions,
                        'high_value_transactions': session.high_value_transactions,
                        'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions and session.total_transactions > 0 else 0,
                    }
                    for session in analysis_sessions
                ],
                
                'transaction_analyses': [
                    {
                        'transaction_id': str(analysis.transaction.id),
                        'document_number': analysis.transaction.document_number,
                        'posting_date': analysis.transaction.posting_date.isoformat() if analysis.transaction.posting_date else None,
                        'gl_account': analysis.transaction.gl_account,
                        'amount': float(analysis.transaction.amount_local_currency),
                        'user_name': analysis.transaction.user_name,
                        'risk_score': analysis.risk_score,
                        'risk_level': analysis.risk_level,
                        'amount_anomaly': analysis.amount_anomaly,
                        'timing_anomaly': analysis.timing_anomaly,
                        'user_anomaly': analysis.user_anomaly,
                        'account_anomaly': analysis.account_anomaly,
                        'pattern_anomaly': analysis.pattern_anomaly,
                        'analysis_details': analysis.analysis_details,
                    }
                    for analysis in transaction_analyses.order_by('-risk_score')
                ],
                
                'risk_summary': {
                    'total_analyzed': len(transaction_analyses),
                    'risk_distribution': {
                        'LOW': len([a for a in transaction_analyses if a.risk_level == 'LOW']),
                        'MEDIUM': len([a for a in transaction_analyses if a.risk_level == 'MEDIUM']),
                        'HIGH': len([a for a in transaction_analyses if a.risk_level == 'HIGH']),
                        'CRITICAL': len([a for a in transaction_analyses if a.risk_level == 'CRITICAL']),
                    },
                    'anomaly_distribution': {
                        'amount_anomalies': len([a for a in transaction_analyses if a.amount_anomaly]),
                        'timing_anomalies': len([a for a in transaction_analyses if a.timing_anomaly]),
                        'user_anomalies': len([a for a in transaction_analyses if a.user_anomaly]),
                        'account_anomalies': len([a for a in transaction_analyses if a.account_anomaly]),
                        'pattern_anomalies': len([a for a in transaction_analyses if a.pattern_anomaly]),
                    },
                    'top_high_risk_transactions': [
                        {
                            'transaction_id': str(analysis.transaction.id),
                            'document_number': analysis.transaction.document_number,
                            'risk_score': analysis.risk_score,
                            'risk_level': analysis.risk_level,
                            'amount': float(analysis.transaction.amount_local_currency),
                            'user_name': analysis.transaction.user_name,
                        }
                        for analysis in transaction_analyses.filter(risk_level__in=['HIGH', 'CRITICAL']).order_by('-risk_score')[:10]
                    ],
                },
                
                                            'ml_model_info': {
                                'has_trained_models': MLModelTraining.objects.filter(status='COMPLETED').exists(),
                                'total_training_sessions': MLModelTraining.objects.count(),
                                'recent_training_sessions': [
                                    {
                                        'id': str(model.id),
                                        'session_name': model.session_name,
                                        'model_type': model.model_type,
                                        'status': model.status,
                                        'training_data_size': model.training_data_size,
                                        'performance_metrics': model.performance_metrics,
                                        'created_at': model.created_at.isoformat() if model.created_at else None,
                                    }
                                    for model in MLModelTraining.objects.order_by('-created_at')[:5]
                                ],
                            },
                            
                            'comprehensive_expense_analytics': {
                                'status': 'completed',
                                'message': 'Analytics data available in database',
                                'transaction_count': len(transactions) if transactions else 0,
                                'analysis_sessions_count': len(analysis_sessions) if analysis_sessions else 0,
                                'transaction_analyses_count': len(transaction_analyses) if transaction_analyses else 0
                            },
            }
            
            return Response(analysis_data)
            
        except Exception as e:
            logger.error(f"Error in FileAnalysisView: {e}")
            from rest_framework import status
            return Response(
                {'error': f'Error retrieving file analysis: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

class AnalyticsDashboardView(generics.GenericAPIView):
    """Comprehensive analytics dashboard with detailed statistics and visualizations"""
    
    def get(self, request, *args, **kwargs):
        """Get comprehensive analytics dashboard data"""
        try:
            from rest_framework import status
            from django.db.models import Sum, Count, Avg, Max, Min, Q
            from django.db.models.functions import TruncMonth, TruncYear
            from decimal import Decimal
            import json
            
            # Get query parameters for filtering
            date_from = request.query_params.get('date_from')
            date_to = request.query_params.get('date_to')
            client_name = request.query_params.get('client_name')
            fiscal_year = request.query_params.get('fiscal_year')
            file_id = request.query_params.get('file_id')
            
            # Base queryset with filters
            transactions = SAPGLPosting.objects.all()
            if date_from:
                transactions = transactions.filter(posting_date__gte=date_from)
            if date_to:
                transactions = transactions.filter(posting_date__lte=date_to)
            if fiscal_year:
                transactions = transactions.filter(fiscal_year=fiscal_year)
            
            # Filter by file ID if provided
            if file_id:
                try:
                    import uuid
                    file_id = file_id.rstrip('/')
                    uuid.UUID(file_id)  # Validate UUID format
                    data_file = DataFile.objects.get(id=file_id)
                    if data_file.audit_start_date and data_file.audit_end_date:
                        transactions = transactions.filter(
                            posting_date__gte=data_file.audit_start_date,
                            posting_date__lte=data_file.audit_end_date,
                            fiscal_year=data_file.fiscal_year
                        )
                except (ValueError, DataFile.DoesNotExist):
                    return Response(
                        {'error': 'Invalid file ID or file not found'}, 
                        status=status.HTTP_400_BAD_REQUEST
                    )
            
            # File filtering
            files = DataFile.objects.all()
            if client_name:
                files = files.filter(client_name__icontains=client_name)
            if date_from:
                files = files.filter(uploaded_at__date__gte=date_from)
            if date_to:
                files = files.filter(uploaded_at__date__lte=date_to)
            if file_id:
                try:
                    files = files.filter(id=file_id)
                except ValueError:
                    pass  # Already handled above
            
            # 1. OVERALL ANALYTICS
            overall_analytics = {
                'summary_statistics': {
                    'total_transactions': transactions.count(),
                    'total_amount': float(transactions.aggregate(total=Sum('amount_local_currency'))['total'] or 0),
                    'unique_users': transactions.values('user_name').distinct().count(),
                    'unique_accounts': transactions.values('gl_account').distinct().count(),
                    'unique_documents': transactions.values('document_number').distinct().count(),
                    'unique_profit_centers': transactions.exclude(profit_center__isnull=True).values('profit_center').distinct().count(),
                    'average_transaction_amount': float(transactions.aggregate(avg=Avg('amount_local_currency'))['avg'] or 0),
                    'max_transaction_amount': float(transactions.aggregate(max=Max('amount_local_currency'))['max'] or 0),
                    'min_transaction_amount': float(transactions.aggregate(min=Min('amount_local_currency'))['min'] or 0),
                },
                
                'monthly_trend_analysis': self._generate_monthly_trend_analysis(transactions),
                'amount_distribution': self._generate_amount_distribution(transactions),
                'department_expenses': self._generate_department_expenses(transactions),
                
                'top_gl_accounts': self._generate_top_gl_accounts(transactions),
                'top_users_by_amount': self._generate_top_users_by_amount(transactions),
                'employee_expenses': self._generate_employee_expenses(transactions),
                
                'transaction_type_analysis': {
                    'debit_transactions': transactions.filter(transaction_type='DEBIT').count(),
                    'credit_transactions': transactions.filter(transaction_type='CREDIT').count(),
                    'debit_amount': float(transactions.filter(transaction_type='DEBIT').aggregate(total=Sum('amount_local_currency'))['total'] or 0),
                    'credit_amount': float(transactions.filter(transaction_type='CREDIT').aggregate(total=Sum('amount_local_currency'))['total'] or 0),
                },
                
                'document_type_analysis': list(transactions.exclude(document_type__isnull=True)
                    .values('document_type')
                    .annotate(count=Count('id'), total_amount=Sum('amount_local_currency'))
                    .order_by('-total_amount')[:10]),
                
                'fiscal_year_analysis': list(transactions.values('fiscal_year')
                    .annotate(
                        count=Count('id'),
                        total_amount=Sum('amount_local_currency'),
                        avg_amount=Avg('amount_local_currency')
                    )
                    .order_by('fiscal_year')),
            }
            
            # 2. ANOMALY ANALYTICS
            anomaly_analytics = self._generate_anomaly_analytics(transactions, files)
            
            # 3. FILE PROCESSING ANALYTICS
            file_analytics = {
                'file_summary': {
                    'total_files': files.count(),
                    'completed_files': files.filter(status='COMPLETED').count(),
                    'processing_files': files.filter(status='PROCESSING').count(),
                    'failed_files': files.filter(status='FAILED').count(),
                    'total_records_processed': files.aggregate(total=Sum('processed_records'))['total'] or 0,
                    'total_records_failed': files.aggregate(total=Sum('failed_records'))['total'] or 0,
                    'success_rate': self._calculate_success_rate(files),
                },
                
                'client_analysis': list(files.values('client_name')
                    .annotate(
                        file_count=Count('id'),
                        total_records=Sum('processed_records'),
                        total_amount=Sum('file_size')
                    )
                    .order_by('-file_count')[:10]),
                
                'processing_performance': {
                    'avg_processing_time': self._calculate_avg_processing_time(),
                    'processing_jobs_by_status': self._get_processing_jobs_by_status(),
                }
            }
            
            # 4. RISK ANALYTICS
            risk_analytics = self._generate_risk_analytics(transactions)
            
            response_data = {
                'overall_analytics': overall_analytics,
                'anomaly_analytics': anomaly_analytics,
                'file_analytics': file_analytics,
                'risk_analytics': risk_analytics,
                'filters_applied': {
                    'date_from': date_from,
                    'date_to': date_to,
                    'client_name': client_name,
                    'fiscal_year': fiscal_year,
                    'file_id': file_id,
                }
            }
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error in AnalyticsDashboardView: {e}")
            from rest_framework import status
            return Response(
                {'error': f'Error retrieving analytics: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_monthly_trend_analysis(self, transactions):
        """Generate monthly trend analysis data"""
        monthly_data = list(transactions.annotate(
            month=TruncMonth('posting_date')
        ).values('month').annotate(
            count=Count('id'),
            total_amount=Sum('amount_local_currency'),
            avg_amount=Avg('amount_local_currency')
        ).order_by('month'))
        
        return {
            'data': [
                {
                    'month': item['month'].strftime('%Y-%m') if item['month'] else None,
                    'count': item['count'],
                    'total_amount': float(item['total_amount'] or 0),
                    'avg_amount': float(item['avg_amount'] or 0),
                }
                for item in monthly_data
            ],
            'summary': {
                'total_months': len(monthly_data),
                'avg_monthly_transactions': sum(item['count'] for item in monthly_data) / len(monthly_data) if monthly_data else 0,
                'avg_monthly_amount': sum(float(item['total_amount'] or 0) for item in monthly_data) / len(monthly_data) if monthly_data else 0,
            }
        }
    
    def _generate_amount_distribution(self, transactions):
        """Generate amount distribution analysis"""
        # Define amount ranges
        ranges = [
            (0, 1000, '0-1K'),
            (1000, 10000, '1K-10K'),
            (10000, 100000, '10K-100K'),
            (100000, 1000000, '100K-1M'),
            (1000000, float('inf'), '1M+')
        ]
        
        distribution = []
        for min_val, max_val, label in ranges:
            if max_val == float('inf'):
                count = transactions.filter(amount_local_currency__gte=min_val).count()
            else:
                count = transactions.filter(
                    amount_local_currency__gte=min_val,
                    amount_local_currency__lt=max_val
                ).count()
            
            distribution.append({
                'range': label,
                'count': count,
                'percentage': round((count / transactions.count()) * 100, 2) if transactions.count() > 0 else 0
            })
        
        return distribution
    
    def _generate_department_expenses(self, transactions):
        """Generate department expenses analysis"""
        dept_data = list(transactions.exclude(profit_center__isnull=True)
            .values('profit_center')
            .annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency'),
                avg_amount=Avg('amount_local_currency')
            )
            .order_by('-total_amount')[:10])
        
        return [
            {
                'department': item['profit_center'],
                'count': item['count'],
                'total_amount': float(item['total_amount'] or 0),
                'avg_amount': float(item['avg_amount'] or 0),
            }
            for item in dept_data
        ]
    
    def _generate_top_gl_accounts(self, transactions):
        """Generate top GL accounts analysis"""
        accounts_data = list(transactions.values('gl_account')
            .annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency'),
                trial_balance=Sum('amount_local_currency')
            )
            .order_by('-total_amount')[:20])
        
        # Calculate totals
        total_accounts = len(accounts_data)
        total_trial_balance = sum(float(item['total_amount'] or 0) for item in accounts_data)
        total_debits = float(transactions.filter(transaction_type='DEBIT').aggregate(total=Sum('amount_local_currency'))['total'] or 0)
        total_credits = float(transactions.filter(transaction_type='CREDIT').aggregate(total=Sum('amount_local_currency'))['total'] or 0)
        
        return {
            'summary': {
                'total_accounts': total_accounts,
                'total_trial_balance': total_trial_balance,
                'total_trading_equity': total_trial_balance,
                'total_debits': total_debits,
                'total_credits': total_credits,
                'currency': 'SAR'
            },
            'accounts': [
                {
                    'account': item['gl_account'],
                    'amount': float(item['total_amount'] or 0),
                    'transactions': item['count'],
                    'trial_balance': float(item['trial_balance'] or 0),
                }
                for item in accounts_data
            ]
        }
    
    def _generate_top_users_by_amount(self, transactions):
        """Generate top users by amount analysis"""
        users_data = list(transactions.values('user_name')
            .annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency'),
                avg_amount=Avg('amount_local_currency')
            )
            .order_by('-total_amount')[:10])
        
        return [
            {
                'user': item['user_name'],
                'amount': float(item['total_amount'] or 0),
                'transactions': item['count'],
                'avg_amount': float(item['avg_amount'] or 0),
            }
            for item in users_data
        ]
    
    def _generate_employee_expenses(self, transactions):
        """Generate employee expenses for chart visualization"""
        users_data = list(transactions.values('user_name')
            .annotate(total_amount=Sum('amount_local_currency'))
            .order_by('-total_amount')[:5])
        
        return {
            'employees': [
                {
                    'name': item['user_name'],
                    'amount': float(item['total_amount'] or 0),
                }
                for item in users_data
            ],
            'total_employees': len(users_data),
            'total_expenses': sum(float(item['total_amount'] or 0) for item in users_data)
        }
    
    def _generate_anomaly_analytics(self, transactions, files):
        """Generate comprehensive anomaly analytics"""
        # Get processing jobs with anomaly results
        processing_jobs = FileProcessingJob.objects.filter(run_anomalies=True)
        
        # Get transaction analyses
        transaction_analyses = TransactionAnalysis.objects.all()
        if transactions.exists():
            transaction_analyses = transaction_analyses.filter(transaction__in=transactions)
        
        # Calculate anomaly statistics
        total_analyzed = transaction_analyses.count()
        flagged_transactions = transaction_analyses.filter(
            Q(amount_anomaly=True) | Q(timing_anomaly=True) | 
            Q(user_anomaly=True) | Q(account_anomaly=True) | Q(pattern_anomaly=True)
        ).count()
        
        # Risk distribution
        risk_distribution = {
            'LOW': transaction_analyses.filter(risk_level='LOW').count(),
            'MEDIUM': transaction_analyses.filter(risk_level='MEDIUM').count(),
            'HIGH': transaction_analyses.filter(risk_level='HIGH').count(),
            'CRITICAL': transaction_analyses.filter(risk_level='CRITICAL').count(),
        }
        
        # Anomaly types
        anomaly_types = {
            'amount_anomalies': transaction_analyses.filter(amount_anomaly=True).count(),
            'timing_anomalies': transaction_analyses.filter(timing_anomaly=True).count(),
            'user_anomalies': transaction_analyses.filter(user_anomaly=True).count(),
            'account_anomalies': transaction_analyses.filter(account_anomaly=True).count(),
            'pattern_anomalies': transaction_analyses.filter(pattern_anomaly=True).count(),
        }
        
        # Duplicate analysis (from processing jobs)
        duplicate_analysis = self._analyze_duplicates(processing_jobs)
        
        # Backdated analysis
        backdated_analysis = self._analyze_backdated_entries(transactions)
        
        # User anomaly analysis
        user_anomaly_analysis = self._analyze_user_anomalies(transactions)
        
        return {
            'summary': {
                'total_analyzed': total_analyzed,
                'flagged_transactions': flagged_transactions,
                'flag_rate': round((flagged_transactions / total_analyzed * 100), 2) if total_analyzed > 0 else 0,
                'overall_risk_score': self._calculate_overall_risk_score(transaction_analyses),
            },
            
            'risk_distribution': risk_distribution,
            'anomaly_types': anomaly_types,
            
            'duplicate_analysis': duplicate_analysis,
            'backdated_analysis': backdated_analysis,
            'user_anomaly_analysis': user_anomaly_analysis,
            
            'top_anomalies': self._get_top_anomalies(transaction_analyses),
            'anomaly_trends': self._generate_anomaly_trends(transactions),
            
            'ml_model_performance': self._get_ml_model_performance(),
        }
    
    def _analyze_duplicates(self, processing_jobs):
        """Analyze duplicate entries from processing jobs"""
        total_duplicates = 0
        duplicate_amount = 0
        duplicate_details = []
        
        for job in processing_jobs:
            if job.anomaly_results and 'duplicate_analysis' in job.anomaly_results:
                dup_data = job.anomaly_results['duplicate_analysis']
                total_duplicates += dup_data.get('total_duplicates', 0)
                duplicate_amount += dup_data.get('duplicate_amount', 0)
                
                if 'duplicate_transactions' in dup_data:
                    duplicate_details.extend(dup_data['duplicate_transactions'])
        
        return {
            'total_duplicates': total_duplicates,
            'duplicate_amount': duplicate_amount,
            'duplicate_transactions': duplicate_details[:20],  # Top 20
            'duplicate_rate': round((total_duplicates / SAPGLPosting.objects.count()) * 100, 2) if SAPGLPosting.objects.count() > 0 else 0,
        }
    
    def _analyze_backdated_entries(self, transactions):
        """Analyze backdated entries"""
        # Simple backdated detection (entries dated before current period)
        from datetime import datetime, timedelta
        current_date = datetime.now().date()
        three_months_ago = current_date - timedelta(days=90)
        
        backdated_transactions = transactions.filter(posting_date__lt=three_months_ago)
        
        return {
            'total_backdated': backdated_transactions.count(),
            'backdated_amount': float(backdated_transactions.aggregate(total=Sum('amount_local_currency'))['total'] or 0),
            'backdated_transactions': list(backdated_transactions.values(
                'document_number', 'posting_date', 'amount_local_currency', 'user_name'
            )[:20]),
            'backdated_rate': round((backdated_transactions.count() / transactions.count()) * 100, 2) if transactions.count() > 0 else 0,
        }
    
    def _analyze_user_anomalies(self, transactions):
        """Analyze user anomalies"""
        # Find users with unusual patterns
        user_stats = list(transactions.values('user_name')
            .annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency'),
                avg_amount=Avg('amount_local_currency'),
                max_amount=Max('amount_local_currency')
            ))
        
        # Calculate thresholds for anomaly detection
        amounts = [float(user['total_amount'] or 0) for user in user_stats]
        if amounts:
            mean_amount = sum(amounts) / len(amounts)
            std_amount = (sum((x - mean_amount) ** 2 for x in amounts) / len(amounts)) ** 0.5
            threshold = mean_amount + (2 * std_amount)  # 2 standard deviations
            
            anomalous_users = [
                {
                    'user': user['user_name'],
                    'total_amount': float(user['total_amount'] or 0),
                    'transaction_count': user['count'],
                    'avg_amount': float(user['avg_amount'] or 0),
                    'max_amount': float(user['max_amount'] or 0),
                    'anomaly_score': round((float(user['total_amount'] or 0) - mean_amount) / std_amount, 2) if std_amount > 0 else 0,
                }
                for user in user_stats
                if float(user['total_amount'] or 0) > threshold
            ]
        else:
            anomalous_users = []
        
        return {
            'total_anomalous_users': len(anomalous_users),
            'anomalous_users': anomalous_users[:10],  # Top 10
        }
    
    def _get_top_anomalies(self, transaction_analyses):
        """Get top anomalies by risk score"""
        return list(transaction_analyses.order_by('-risk_score')[:10].values(
            'transaction__document_number',
            'transaction__posting_date',
            'transaction__amount_local_currency',
            'transaction__user_name',
            'risk_score',
            'risk_level',
            'amount_anomaly',
            'timing_anomaly',
            'user_anomaly'
        ))
    
    def _generate_anomaly_trends(self, transactions):
        """Generate anomaly trends over time"""
        # This would require historical anomaly data
        # For now, return basic structure
        return {
            'monthly_anomalies': [],
            'anomaly_types_trend': {},
            'risk_level_trend': {},
        }
    
    def _get_ml_model_performance(self):
        """Get ML model performance metrics"""
        ml_models = MLModelTraining.objects.filter(status='COMPLETED').order_by('-created_at')
        
        if ml_models.exists():
            latest_model = ml_models.first()
            return {
                'has_trained_models': True,
                'latest_model': {
                    'id': str(latest_model.id),
                    'session_name': latest_model.session_name,
                    'model_type': latest_model.model_type,
                    'performance_metrics': latest_model.performance_metrics,
                    'training_data_size': latest_model.training_data_size,
                },
                'total_models': ml_models.count(),
                'avg_accuracy': self._calculate_avg_model_accuracy(ml_models),
            }
        else:
            return {
                'has_trained_models': False,
                'total_models': 0,
                'avg_accuracy': 0,
            }
    
    def _calculate_avg_model_accuracy(self, ml_models):
        """Calculate average model accuracy"""
        accuracies = []
        for model in ml_models:
            if model.performance_metrics and 'accuracy' in model.performance_metrics:
                accuracies.append(model.performance_metrics['accuracy'])
        
        return round(sum(accuracies) / len(accuracies), 4) if accuracies else 0
    
    def _generate_risk_analytics(self, transactions):
        """Generate risk analytics"""
        transaction_analyses = TransactionAnalysis.objects.all()
        if transactions.exists():
            transaction_analyses = transaction_analyses.filter(transaction__in=transactions)
        
        total_analyzed = transaction_analyses.count()
        
        # Risk distribution
        risk_distribution = {
            'LOW': transaction_analyses.filter(risk_level='LOW').count(),
            'MEDIUM': transaction_analyses.filter(risk_level='MEDIUM').count(),
            'HIGH': transaction_analyses.filter(risk_level='HIGH').count(),
            'CRITICAL': transaction_analyses.filter(risk_level='CRITICAL').count(),
        }
        
        # Calculate percentages
        risk_percentages = {}
        for level, count in risk_distribution.items():
            risk_percentages[level] = round((count / total_analyzed * 100), 2) if total_analyzed > 0 else 0
        
        # High-risk transactions
        high_risk_transactions = list(transaction_analyses.filter(
            risk_level__in=['HIGH', 'CRITICAL']
        ).order_by('-risk_score')[:20].values(
            'transaction__document_number',
            'transaction__posting_date',
            'transaction__amount_local_currency',
            'transaction__user_name',
            'risk_score',
            'risk_level'
        ))
        
        return {
            'summary': {
                'total_analyzed': total_analyzed,
                'high_risk_count': risk_distribution['HIGH'] + risk_distribution['CRITICAL'],
                'critical_risk_count': risk_distribution['CRITICAL'],
                'overall_risk_score': self._calculate_overall_risk_score(transaction_analyses),
            },
            'risk_distribution': risk_distribution,
            'risk_percentages': risk_percentages,
            'high_risk_transactions': high_risk_transactions,
            'risk_trends': self._generate_risk_trends(transactions),
        }
    
    def _calculate_overall_risk_score(self, transaction_analyses):
        """Calculate overall risk score"""
        if not transaction_analyses.exists():
            return 0
        
        total_score = sum(analysis.risk_score for analysis in transaction_analyses)
        return round(total_score / transaction_analyses.count(), 2)
    
    def _generate_risk_trends(self, transactions):
        """Generate risk trends over time"""
        # This would require historical risk data
        return {
            'monthly_risk_scores': [],
            'risk_level_trends': {},
        }
    
    def _calculate_success_rate(self, files):
        """Calculate overall success rate"""
        total_processed = files.aggregate(total=Sum('processed_records'))['total'] or 0
        total_failed = files.aggregate(total=Sum('failed_records'))['total'] or 0
        total_records = total_processed + total_failed
        
        return round((total_processed / total_records * 100), 2) if total_records > 0 else 0
    
    def _calculate_avg_processing_time(self):
        """Calculate average processing time"""
        jobs = FileProcessingJob.objects.exclude(processing_duration__isnull=True)
        if jobs.exists():
            avg_time = jobs.aggregate(avg=Avg('processing_duration'))['avg']
            return round(avg_time, 2) if avg_time else 0
        return 0
    
    def _get_processing_jobs_by_status(self):
        """Get processing jobs grouped by status"""
        return dict(FileProcessingJob.objects.values('status').annotate(
            count=Count('id')
        ).values_list('status', 'count'))

class DetailedAnomalyView(generics.GenericAPIView):
    """Detailed anomaly analysis and statistics"""
    
    def get(self, request, *args, **kwargs):
        """Get detailed anomaly analysis"""
        try:
            from rest_framework import status
            from django.db.models import Q, Count, Sum, Avg
            
            # Get query parameters
            anomaly_type = request.query_params.get('type')  # duplicate, backdated, user, amount, timing, pattern
            risk_level = request.query_params.get('risk_level')  # LOW, MEDIUM, HIGH, CRITICAL
            date_from = request.query_params.get('date_from')
            date_to = request.query_params.get('date_to')
            user_name = request.query_params.get('user_name')
            gl_account = request.query_params.get('gl_account')
            file_id = request.query_params.get('file_id')
            
            # Base queryset
            transactions = SAPGLPosting.objects.all()
            if date_from:
                transactions = transactions.filter(posting_date__gte=date_from)
            if date_to:
                transactions = transactions.filter(posting_date__lte=date_to)
            if user_name:
                transactions = transactions.filter(user_name__icontains=user_name)
            if gl_account:
                transactions = transactions.filter(gl_account=gl_account)
            
            # Filter by file ID if provided
            if file_id:
                try:
                    import uuid
                    file_id = file_id.rstrip('/')
                    uuid.UUID(file_id)  # Validate UUID format
                    data_file = DataFile.objects.get(id=file_id)
                    if data_file.audit_start_date and data_file.audit_end_date:
                        transactions = transactions.filter(
                            posting_date__gte=data_file.audit_start_date,
                            posting_date__lte=data_file.audit_end_date,
                            fiscal_year=data_file.fiscal_year
                        )
                except (ValueError, DataFile.DoesNotExist):
                    return Response(
                        {'error': 'Invalid file ID or file not found'}, 
                        status=status.HTTP_400_BAD_REQUEST
                    )
            
            # Get transaction analyses
            analyses = TransactionAnalysis.objects.all()
            if transactions.exists():
                analyses = analyses.filter(transaction__in=transactions)
            
            # Filter by anomaly type
            if anomaly_type:
                if anomaly_type == 'amount':
                    analyses = analyses.filter(amount_anomaly=True)
                elif anomaly_type == 'timing':
                    analyses = analyses.filter(timing_anomaly=True)
                elif anomaly_type == 'user':
                    analyses = analyses.filter(user_anomaly=True)
                elif anomaly_type == 'account':
                    analyses = analyses.filter(account_anomaly=True)
                elif anomaly_type == 'pattern':
                    analyses = analyses.filter(pattern_anomaly=True)
            
            # Filter by risk level
            if risk_level:
                analyses = analyses.filter(risk_level=risk_level.upper())
            
            # Generate detailed anomaly analysis
            anomaly_analysis = {
                'summary': {
                    'total_anomalies': analyses.count(),
                    'total_amount': float(analyses.aggregate(
                        total=Sum('transaction__amount_local_currency')
                    )['total'] or 0),
                    'avg_risk_score': float(analyses.aggregate(avg=Avg('risk_score'))['avg'] or 0),
                    'high_risk_count': analyses.filter(risk_level__in=['HIGH', 'CRITICAL']).count(),
                },
                
                'anomaly_breakdown': {
                    'amount_anomalies': {
                        'count': analyses.filter(amount_anomaly=True).count(),
                        'amount': float(analyses.filter(amount_anomaly=True).aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                        'avg_risk_score': float(analyses.filter(amount_anomaly=True).aggregate(
                            avg=Avg('risk_score')
                        )['avg'] or 0),
                    },
                    'timing_anomalies': {
                        'count': analyses.filter(timing_anomaly=True).count(),
                        'amount': float(analyses.filter(timing_anomaly=True).aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                        'avg_risk_score': float(analyses.filter(timing_anomaly=True).aggregate(
                            avg=Avg('risk_score')
                        )['avg'] or 0),
                    },
                    'user_anomalies': {
                        'count': analyses.filter(user_anomaly=True).count(),
                        'amount': float(analyses.filter(user_anomaly=True).aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                        'avg_risk_score': float(analyses.filter(user_anomaly=True).aggregate(
                            avg=Avg('risk_score')
                        )['avg'] or 0),
                    },
                    'account_anomalies': {
                        'count': analyses.filter(account_anomaly=True).count(),
                        'amount': float(analyses.filter(account_anomaly=True).aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                        'avg_risk_score': float(analyses.filter(account_anomaly=True).aggregate(
                            avg=Avg('risk_score')
                        )['avg'] or 0),
                    },
                    'pattern_anomalies': {
                        'count': analyses.filter(pattern_anomaly=True).count(),
                        'amount': float(analyses.filter(pattern_anomaly=True).aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                        'avg_risk_score': float(analyses.filter(pattern_anomaly=True).aggregate(
                            avg=Avg('risk_score')
                        )['avg'] or 0),
                    },
                },
                
                'risk_level_breakdown': {
                    'LOW': {
                        'count': analyses.filter(risk_level='LOW').count(),
                        'amount': float(analyses.filter(risk_level='LOW').aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                    },
                    'MEDIUM': {
                        'count': analyses.filter(risk_level='MEDIUM').count(),
                        'amount': float(analyses.filter(risk_level='MEDIUM').aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                    },
                    'HIGH': {
                        'count': analyses.filter(risk_level='HIGH').count(),
                        'amount': float(analyses.filter(risk_level='HIGH').aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                    },
                    'CRITICAL': {
                        'count': analyses.filter(risk_level='CRITICAL').count(),
                        'amount': float(analyses.filter(risk_level='CRITICAL').aggregate(
                            total=Sum('transaction__amount_local_currency')
                        )['total'] or 0),
                    },
                },
                
                'top_anomalous_users': list(analyses.values('transaction__user_name')
                    .annotate(
                        count=Count('id'),
                        total_amount=Sum('transaction__amount_local_currency'),
                        avg_risk_score=Avg('risk_score')
                    )
                    .order_by('-total_amount')[:10]),
                
                'top_anomalous_accounts': list(analyses.values('transaction__gl_account')
                    .annotate(
                        count=Count('id'),
                        total_amount=Sum('transaction__amount_local_currency'),
                        avg_risk_score=Avg('risk_score')
                    )
                    .order_by('-total_amount')[:10]),
                
                'anomaly_details': list(analyses.order_by('-risk_score')[:50].values(
                    'transaction__document_number',
                    'transaction__posting_date',
                    'transaction__gl_account',
                    'transaction__amount_local_currency',
                    'transaction__user_name',
                    'transaction__text',
                    'risk_score',
                    'risk_level',
                    'amount_anomaly',
                    'timing_anomaly',
                    'user_anomaly',
                    'account_anomaly',
                    'pattern_anomaly',
                    'analysis_details'
                )),
                
                'anomaly_trends': self._generate_detailed_anomaly_trends(analyses),
                'ml_predictions': self._get_ml_predictions(analyses),
            }
            
            return Response(anomaly_analysis)
            
        except Exception as e:
            logger.error(f"Error in DetailedAnomalyView: {e}")
            from rest_framework import status
            return Response(
                {'error': f'Error retrieving anomaly analysis: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_detailed_anomaly_trends(self, analyses):
        """Generate detailed anomaly trends"""
        # Monthly anomaly trends
        monthly_trends = list(analyses.annotate(
            month=TruncMonth('transaction__posting_date')
        ).values('month').annotate(
            count=Count('id'),
            total_amount=Sum('transaction__amount_local_currency'),
            avg_risk_score=Avg('risk_score')
        ).order_by('month'))
        
        return {
            'monthly_trends': [
                {
                    'month': item['month'].strftime('%Y-%m') if item['month'] else None,
                    'count': item['count'],
                    'total_amount': float(item['total_amount'] or 0),
                    'avg_risk_score': float(item['avg_risk_score'] or 0),
                }
                for item in monthly_trends
            ],
            'anomaly_type_trends': {
                'amount_anomalies': analyses.filter(amount_anomaly=True).count(),
                'timing_anomalies': analyses.filter(timing_anomaly=True).count(),
                'user_anomalies': analyses.filter(user_anomaly=True).count(),
                'account_anomalies': analyses.filter(account_anomaly=True).count(),
                'pattern_anomalies': analyses.filter(pattern_anomaly=True).count(),
            }
        }
    
    def _get_ml_predictions(self, analyses):
        """Get ML model predictions for anomalies"""
        # This would integrate with the ML models to get predictions
        return {
            'model_confidence': 0.85,
            'prediction_accuracy': 0.92,
            'false_positive_rate': 0.08,
            'false_negative_rate': 0.05,
        }

class FileAnalyticsView(generics.GenericAPIView):
    """Comprehensive analytics for a specific file by ID"""
    
    def get(self, request, file_id, *args, **kwargs):
        """Get comprehensive analytics for a specific file"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            from django.db.models import Sum, Count, Avg, Max, Min, Q
            from django.db.models.functions import TruncMonth
            import uuid
            
            # Clean file_id (remove trailing slashes)
            file_id = file_id.rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get the file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Get the processing job for this file
            processing_job = FileProcessingJob.objects.filter(data_file=data_file).first()
            
            # Get transactions for this file (based on audit date range)
            transactions = []
            if data_file.audit_start_date and data_file.audit_end_date:
                transactions = SAPGLPosting.objects.filter(
                    posting_date__gte=data_file.audit_start_date,
                    posting_date__lte=data_file.audit_end_date,
                    fiscal_year=data_file.fiscal_year
                )
            
            # Get transaction analyses for this file
            transaction_analyses = []
            if transactions:
                transaction_analyses = TransactionAnalysis.objects.filter(
                    transaction__in=transactions
                ).select_related('transaction')
            
            # Build comprehensive file analytics
            file_analytics = {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'file_size': data_file.file_size,
                    'engagement_id': data_file.engagement_id,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'audit_start_date': data_file.audit_start_date.isoformat() if data_file.audit_start_date else None,
                    'audit_end_date': data_file.audit_end_date.isoformat() if data_file.audit_end_date else None,
                    'status': data_file.status,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None,
                    'error_message': data_file.error_message,
                    'total_records': data_file.total_records,
                    'processed_records': data_file.processed_records,
                    'failed_records': data_file.failed_records,
                    'success_rate': round((data_file.processed_records / data_file.total_records * 100), 2) if data_file.total_records > 0 else 0,
                },
                
                'processing_job': {
                    'job_id': str(processing_job.id) if processing_job else None,
                    'status': processing_job.status if processing_job else None,
                    'run_anomalies': processing_job.run_anomalies if processing_job else False,
                    'requested_anomalies': processing_job.requested_anomalies if processing_job else [],
                    'created_at': processing_job.created_at.isoformat() if processing_job and processing_job.created_at else None,
                    'started_at': processing_job.started_at.isoformat() if processing_job and processing_job.started_at else None,
                    'completed_at': processing_job.completed_at.isoformat() if processing_job and processing_job.completed_at else None,
                    'processing_duration': processing_job.processing_duration if processing_job else None,
                    'error_message': processing_job.error_message if processing_job else None,
                    'file_hash': processing_job.file_hash if processing_job else None,
                    'is_duplicate_content': processing_job.is_duplicate_content if processing_job else False,
                } if processing_job else None,
                
                'analytics_results': processing_job.analytics_results if processing_job else {},
                'anomaly_results': processing_job.anomaly_results if processing_job else {},
                
                'transaction_summary': {
                    'total_transactions': len(transactions),
                    'total_amount': float(sum(t.amount_local_currency for t in transactions)) if transactions else 0,
                    'unique_accounts': len(set(t.gl_account for t in transactions)) if transactions else 0,
                    'unique_users': len(set(t.user_name for t in transactions)) if transactions else 0,
                    'unique_documents': len(set(t.document_number for t in transactions)) if transactions else 0,
                    'unique_profit_centers': len(set(t.profit_center for t in transactions if t.profit_center)) if transactions else 0,
                    'average_transaction_amount': float(sum(t.amount_local_currency for t in transactions) / len(transactions)) if transactions else 0,
                    'max_transaction_amount': float(max(t.amount_local_currency for t in transactions)) if transactions else 0,
                    'min_transaction_amount': float(min(t.amount_local_currency for t in transactions)) if transactions else 0,
                    'date_range': {
                        'min_date': min(t.posting_date for t in transactions).isoformat() if transactions else None,
                        'max_date': max(t.posting_date for t in transactions).isoformat() if transactions else None,
                    } if transactions else None,
                    'amount_range': {
                        'min_amount': float(min(t.amount_local_currency for t in transactions)) if transactions else None,
                        'max_amount': float(max(t.amount_local_currency for t in transactions)) if transactions else None,
                    } if transactions else None,
                },
                
                'monthly_trend_analysis': self._generate_file_monthly_trends(transactions),
                'amount_distribution': self._generate_file_amount_distribution(transactions),
                'department_expenses': self._generate_file_department_expenses(transactions),
                
                'top_gl_accounts': self._generate_file_top_gl_accounts(transactions),
                'top_users_by_amount': self._generate_file_top_users(transactions),
                'employee_expenses': self._generate_file_employee_expenses(transactions),
                
                'transaction_type_analysis': {
                    'debit_transactions': len([t for t in transactions if t.transaction_type == 'DEBIT']),
                    'credit_transactions': len([t for t in transactions if t.transaction_type == 'CREDIT']),
                    'debit_amount': float(sum(t.amount_local_currency for t in transactions if t.transaction_type == 'DEBIT')),
                    'credit_amount': float(sum(t.amount_local_currency for t in transactions if t.transaction_type == 'CREDIT')),
                },
                
                'document_type_analysis': self._generate_file_document_type_analysis(transactions),
                
                'anomaly_summary': {
                    'total_analyzed': len(transaction_analyses),
                    'flagged_transactions': len([a for a in transaction_analyses if any([
                        a.amount_anomaly, a.timing_anomaly, a.user_anomaly, 
                        a.account_anomaly, a.pattern_anomaly
                    ])]),
                    'flag_rate': round((len([a for a in transaction_analyses if any([
                        a.amount_anomaly, a.timing_anomaly, a.user_anomaly, 
                        a.account_anomaly, a.pattern_anomaly
                    ])]) / len(transaction_analyses) * 100), 2) if transaction_analyses else 0,
                    'overall_risk_score': round(sum(a.risk_score for a in transaction_analyses) / len(transaction_analyses), 2) if transaction_analyses else 0,
                },
                
                'risk_distribution': {
                    'LOW': len([a for a in transaction_analyses if a.risk_level == 'LOW']),
                    'MEDIUM': len([a for a in transaction_analyses if a.risk_level == 'MEDIUM']),
                    'HIGH': len([a for a in transaction_analyses if a.risk_level == 'HIGH']),
                    'CRITICAL': len([a for a in transaction_analyses if a.risk_level == 'CRITICAL']),
                },
                
                'anomaly_types': {
                    'amount_anomalies': len([a for a in transaction_analyses if a.amount_anomaly]),
                    'timing_anomalies': len([a for a in transaction_analyses if a.timing_anomaly]),
                    'user_anomalies': len([a for a in transaction_analyses if a.user_anomaly]),
                    'account_anomalies': len([a for a in transaction_analyses if a.account_anomaly]),
                    'pattern_anomalies': len([a for a in transaction_analyses if a.pattern_anomaly]),
                },
                
                'duplicate_analysis': self._generate_file_duplicate_analysis(processing_job),
                'backdated_analysis': self._generate_file_backdated_analysis(transactions),
                'user_anomaly_analysis': self._generate_file_user_anomaly_analysis(transactions),
                
                'top_anomalies': [
                    {
                        'transaction_id': str(analysis.transaction.id),
                        'document_number': analysis.transaction.document_number,
                        'posting_date': analysis.transaction.posting_date.isoformat() if analysis.transaction.posting_date else None,
                        'gl_account': analysis.transaction.gl_account,
                        'amount': float(analysis.transaction.amount_local_currency),
                        'user_name': analysis.transaction.user_name,
                        'risk_score': analysis.risk_score,
                        'risk_level': analysis.risk_level,
                        'amount_anomaly': analysis.amount_anomaly,
                        'timing_anomaly': analysis.timing_anomaly,
                        'user_anomaly': analysis.user_anomaly,
                        'account_anomaly': analysis.account_anomaly,
                        'pattern_anomaly': analysis.pattern_anomaly,
                        'analysis_details': analysis.analysis_details,
                    }
                    for analysis in sorted(transaction_analyses, key=lambda x: x.risk_score, reverse=True)[:20]
                ],
                
                'high_risk_transactions': [
                    {
                        'transaction_id': str(analysis.transaction.id),
                        'document_number': analysis.transaction.document_number,
                        'posting_date': analysis.transaction.posting_date.isoformat() if analysis.transaction.posting_date else None,
                        'gl_account': analysis.transaction.gl_account,
                        'amount': float(analysis.transaction.amount_local_currency),
                        'user_name': analysis.transaction.user_name,
                        'risk_score': analysis.risk_score,
                        'risk_level': analysis.risk_level,
                    }
                    for analysis in transaction_analyses if analysis.risk_level in ['HIGH', 'CRITICAL']
                ],
                
                'ml_model_info': {
                    'has_trained_models': MLModelTraining.objects.filter(status='COMPLETED').exists(),
                    'total_training_sessions': MLModelTraining.objects.count(),
                    'recent_training_sessions': [
                        {
                            'id': str(model.id),
                            'session_name': model.session_name,
                            'model_type': model.model_type,
                            'status': model.status,
                            'training_data_size': model.training_data_size,
                            'performance_metrics': model.performance_metrics,
                            'created_at': model.created_at.isoformat() if model.created_at else None,
                        }
                        for model in MLModelTraining.objects.order_by('-created_at')[:5]
                    ],
                },
                
                'analysis_sessions': [
                    {
                        'id': str(session.id),
                        'session_name': session.session_name,
                        'description': session.description,
                        'status': session.status,
                        'created_at': session.created_at.isoformat() if session.created_at else None,
                        'started_at': session.started_at.isoformat() if session.started_at else None,
                        'completed_at': session.completed_at.isoformat() if session.completed_at else None,
                        'total_transactions': session.total_transactions,
                        'total_amount': float(session.total_amount) if session.total_amount else 0,
                        'flagged_transactions': session.flagged_transactions,
                        'high_value_transactions': session.high_value_transactions,
                        'flag_rate': round((session.flagged_transactions / session.total_transactions * 100), 2) if session.total_transactions and session.total_transactions > 0 else 0,
                    }
                    for session in (AnalysisSession.objects.filter(
                        date_from__gte=data_file.audit_start_date,
                        date_to__lte=data_file.audit_end_date
                    ) if data_file.audit_start_date and data_file.audit_end_date else AnalysisSession.objects.none())
                ],
            }
            
            return Response(file_analytics)
            
        except Exception as e:
            logger.error(f"Error in FileAnalyticsView: {e}")
            from rest_framework import status
            return Response(
                {'error': f'Error retrieving file analytics: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


class ComprehensiveFileAnalyticsView(generics.GenericAPIView):
    """General charts and stats API that returns essential analytics data for a specific file"""
    
    def get(self, request, file_id, *args, **kwargs):
        """Get general charts and stats for a specific file"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            from django.db.models import Sum, Count, Avg, Max, Min, Q
            import uuid
            
            # Clean file_id (remove trailing slashes)
            file_id = file_id.rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get the data file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Get related transactions
            transactions = SAPGLPosting.objects.filter(
                created_at__gte=data_file.uploaded_at
            ).order_by('created_at')
            
            if not transactions.exists():
                return Response(
                    {'error': 'No transactions found for this file'}, 
                    status=status.HTTP_404_NOT_FOUND
                )
            
            # Get risk data from ML model response
            risk_data = self._get_risk_data_from_ml_response(data_file)
            
            # Prepare general charts and stats data
            analytics_data = {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'status': data_file.status,
                    'total_records': data_file.total_records,
                    'processed_records': data_file.processed_records,
                    'failed_records': data_file.failed_records,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None
                },
                'general_stats': self._generate_general_stats(transactions),
                'charts': self._generate_general_charts(transactions),
                'summary': self._generate_summary_data(transactions),
                'risk_data': risk_data
            }
            
            return Response(analytics_data)
            
        except Exception as e:
            logger.error(f"Error in ComprehensiveFileAnalyticsView: {e}")
            return Response(
                {'error': f'Error retrieving general charts and stats: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_general_stats(self, transactions):
        """Generate general statistics for the file"""
        try:
            total_amount = sum(float(t.amount_local_currency) for t in transactions)
            avg_amount = total_amount / len(transactions) if transactions else 0
            
            # Amount statistics
            amounts = [float(t.amount_local_currency) for t in transactions]
            min_amount = min(amounts) if amounts else 0
            max_amount = max(amounts) if amounts else 0
            
            # Date statistics
            dates = [t.posting_date for t in transactions if t.posting_date]
            min_date = min(dates) if dates else None
            max_date = max(dates) if dates else None
            
            # User and account statistics
            unique_users = transactions.values('user_name').distinct().count()
            unique_accounts = transactions.values('gl_account').distinct().count()
            
            # Document type statistics
            document_types = transactions.values('document_type').annotate(count=Count('id')).order_by('-count')[:5]
            
            return {
                'total_transactions': len(transactions),
                'total_amount': total_amount,
                'average_amount': avg_amount,
                'min_amount': min_amount,
                'max_amount': max_amount,
                'unique_users': unique_users,
                'unique_accounts': unique_accounts,
                'date_range': {
                    'min_date': min_date.isoformat() if min_date else None,
                    'max_date': max_date.isoformat() if max_date else None
                },
                'top_document_types': [
                    {'type': dt['document_type'], 'count': dt['count']} 
                    for dt in document_types
                ]
            }
        except Exception as e:
            logger.error(f"Error generating general stats: {e}")
            return {}
    
    def _generate_general_charts(self, transactions):
        """Generate general charts data"""
        try:
            charts = {
                'monthly_trends': self._generate_monthly_trends(transactions),
                'amount_distribution': self._generate_amount_distribution(transactions),
                'top_users': self._generate_top_users(transactions),
                'top_accounts': self._generate_top_accounts(transactions),
                'transaction_types': self._generate_transaction_types(transactions),
                'daily_activity': self._generate_daily_activity(transactions)
            }
            return charts
        except Exception as e:
            logger.error(f"Error generating general charts: {e}")
            return {}
    
    def _generate_summary_data(self, transactions):
        """Generate summary data"""
        try:
            # High value transactions summary
            high_value_transactions = [t for t in transactions if float(t.amount_local_currency) > 1000000]
            
            # User activity summary
            user_activity = transactions.values('user_name').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:10]
            
            # Account summary
            account_summary = transactions.values('gl_account').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-total_amount')[:10]
            
            return {
                'high_value_transactions': len(high_value_transactions),
                'high_value_amount': sum(float(t.amount_local_currency) for t in high_value_transactions),
                'top_active_users': [
                    {
                        'user_name': ua['user_name'],
                        'transaction_count': ua['count'],
                        'total_amount': float(ua['total_amount'])
                    }
                    for ua in user_activity
                ],
                'top_accounts': [
                    {
                        'gl_account': acc['gl_account'],
                        'transaction_count': acc['count'],
                        'total_amount': float(acc['total_amount'])
                    }
                    for acc in account_summary
                ]
            }
        except Exception as e:
            logger.error(f"Error generating summary data: {e}")
            return {}
    
    def _generate_monthly_trends(self, transactions):
        """Generate monthly trends chart data"""
        try:
            monthly_data = {}
            for transaction in transactions:
                if transaction.posting_date:
                    month_key = transaction.posting_date.strftime('%Y-%m')
                    if month_key not in monthly_data:
                        monthly_data[month_key] = {
                            'count': 0,
                            'total_amount': 0,
                            'avg_amount': 0
                        }
                    monthly_data[month_key]['count'] += 1
                    monthly_data[month_key]['total_amount'] += float(transaction.amount_local_currency)
            
            # Calculate averages
            for month in monthly_data:
                if monthly_data[month]['count'] > 0:
                    monthly_data[month]['avg_amount'] = monthly_data[month]['total_amount'] / monthly_data[month]['count']
            
            # Convert to sorted list
            chart_data = []
            for month in sorted(monthly_data.keys()):
                chart_data.append({
                    'month': month,
                    'transaction_count': monthly_data[month]['count'],
                    'total_amount': monthly_data[month]['total_amount'],
                    'average_amount': monthly_data[month]['avg_amount']
                })
            
            return chart_data
        except Exception as e:
            logger.error(f"Error generating monthly trends: {e}")
            return []
    
    def _generate_amount_distribution(self, transactions):
        """Generate amount distribution chart data"""
        try:
            amounts = [float(t.amount_local_currency) for t in transactions]
            
            # Define amount ranges
            ranges = [
                {'min': 0, 'max': 1000, 'label': '0-1K'},
                {'min': 1000, 'max': 10000, 'label': '1K-10K'},
                {'min': 10000, 'max': 100000, 'label': '10K-100K'},
                {'min': 100000, 'max': 1000000, 'label': '100K-1M'},
                {'min': 1000000, 'max': float('inf'), 'label': '1M+'}
            ]
            
            distribution = []
            for range_def in ranges:
                count = len([a for a in amounts if range_def['min'] <= a < range_def['max']])
                distribution.append({
                    'range': range_def['label'],
                    'count': count,
                    'percentage': (count / len(amounts)) * 100 if amounts else 0
                })
            
            return distribution
        except Exception as e:
            logger.error(f"Error generating amount distribution: {e}")
            return []
    
    def _generate_top_users(self, transactions):
        """Generate top users chart data"""
        try:
            user_stats = transactions.values('user_name').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency'),
                avg_amount=Avg('amount_local_currency')
            ).order_by('-total_amount')[:10]
            
            chart_data = []
            for user in user_stats:
                chart_data.append({
                    'user_name': user['user_name'],
                    'transaction_count': user['count'],
                    'total_amount': float(user['total_amount']),
                    'average_amount': float(user['avg_amount'])
                })
            
            return chart_data
        except Exception as e:
            logger.error(f"Error generating top users: {e}")
            return []
    
    def _generate_top_accounts(self, transactions):
        """Generate top accounts chart data"""
        try:
            account_stats = transactions.values('gl_account').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency'),
                avg_amount=Avg('amount_local_currency')
            ).order_by('-total_amount')[:10]
            
            chart_data = []
            for account in account_stats:
                chart_data.append({
                    'gl_account': account['gl_account'],
                    'transaction_count': account['count'],
                    'total_amount': float(account['total_amount']),
                    'average_amount': float(account['avg_amount'])
                })
            
            return chart_data
        except Exception as e:
            logger.error(f"Error generating top accounts: {e}")
            return []
    
    def _generate_transaction_types(self, transactions):
        """Generate transaction types chart data"""
        try:
            type_stats = transactions.values('document_type').annotate(
                count=Count('id'),
                total_amount=Sum('amount_local_currency')
            ).order_by('-count')[:10]
            
            chart_data = []
            for type_stat in type_stats:
                chart_data.append({
                    'document_type': type_stat['document_type'],
                    'count': type_stat['count'],
                    'total_amount': float(type_stat['total_amount'])
                })
            
            return chart_data
        except Exception as e:
            logger.error(f"Error generating transaction types: {e}")
            return []
    
    def _generate_daily_activity(self, transactions):
        """Generate daily activity chart data"""
        try:
            daily_data = {}
            for transaction in transactions:
                if transaction.posting_date:
                    day_key = transaction.posting_date.strftime('%Y-%m-%d')
                    if day_key not in daily_data:
                        daily_data[day_key] = {
                            'count': 0,
                            'total_amount': 0
                        }
                    daily_data[day_key]['count'] += 1
                    daily_data[day_key]['total_amount'] += float(transaction.amount_local_currency)
            
            # Convert to sorted list (last 30 days)
            chart_data = []
            sorted_days = sorted(daily_data.keys())[-30:]  # Last 30 days
            for day in sorted_days:
                chart_data.append({
                    'date': day,
                    'transaction_count': daily_data[day]['count'],
                    'total_amount': daily_data[day]['total_amount']
                })
            
            return chart_data
        except Exception as e:
            logger.error(f"Error generating daily activity: {e}")
            return []
    
    def _get_risk_data_from_ml_response(self, data_file):
        """Get risk data from ML model response stored in FileProcessingJob"""
        try:
            from core.models import FileProcessingJob
            
            # Get the latest processing job for this file
            processing_job = FileProcessingJob.objects.filter(
                data_file=data_file,
                status='COMPLETED'
            ).order_by('-created_at').first()
            
            if not processing_job:
                logger.warning(f"No completed processing job found for file {data_file.id}")
                return self._get_default_risk_data()
            
            # Get anomaly results from the processing job
            anomaly_results = processing_job.anomaly_results
            
            if not anomaly_results:
                logger.warning(f"No anomaly results found in processing job {processing_job.id}")
                return self._get_default_risk_data()
            
            # Compile risk data from the ML model response
            return self._compile_risk_data_from_ml_response(anomaly_results)
                
        except Exception as e:
            logger.error(f"Error getting risk data from ML response: {e}")
            return self._get_default_risk_data()
    
    def _compile_risk_data_from_ml_response(self, anomaly_results):
        """Compile risk data from ML model response"""
        try:
            risk_data = {
                'risk_stats': {},
                'risk_charts': {}
            }
            
            # Extract duplicates data from ML response
            duplicates = anomaly_results.get('duplicate', {})
            
            if isinstance(duplicates, dict):
                # Calculate summary from details
                details = duplicates.get('details', [])
                anomalies_found = duplicates.get('anomalies_found', 0)
                
                # Calculate totals from details
                total_amount_involved = 0
                total_risk_score = 0
                unique_accounts = set()
                unique_users = set()
                risk_level_distribution = {'LOW': 0, 'MEDIUM': 0, 'HIGH': 0, 'CRITICAL': 0}
                
                for detail in details:
                    amount = detail.get('debit_amount', 0) + detail.get('credit_amount', 0)
                    total_amount_involved += amount
                    total_risk_score += detail.get('risk_score', 0)
                    
                    # Track unique accounts and users
                    unique_accounts.add(detail.get('gl_account', ''))
                    for transaction in detail.get('transactions', []):
                        unique_users.add(transaction.get('user_name', ''))
                    
                    # Categorize risk levels
                    risk_score = detail.get('risk_score', 0)
                    if risk_score >= 80:
                        risk_level_distribution['CRITICAL'] += 1
                    elif risk_score >= 60:
                        risk_level_distribution['HIGH'] += 1
                    elif risk_score >= 40:
                        risk_level_distribution['MEDIUM'] += 1
                    else:
                        risk_level_distribution['LOW'] += 1
                
                # Create risk statistics
                risk_data['risk_stats'] = {
                    'duplicates': {
                        'total_patterns': anomalies_found,
                        'total_amount': total_amount_involved,
                        'average_risk_score': total_risk_score / len(details) if details else 0,
                        'unique_accounts': len(unique_accounts),
                        'unique_users': len(unique_users),
                        'risk_levels': risk_level_distribution
                    }
                }
                
                # Create risk charts (only 2 charts)
                risk_data['risk_charts'] = {
                    'duplicate_types': self._get_duplicate_types_chart(details),
                    'risk_levels': self._get_risk_levels_chart(risk_level_distribution)
                }
            
            # Add other anomaly types if available
            for anomaly_type in ['backdated_entries', 'user_anomalies', 'closing_entries', 'unusual_days', 'holiday_entries']:
                if anomaly_type in anomaly_results and anomaly_results[anomaly_type]:
                    anomaly_data = anomaly_results[anomaly_type]
                    if isinstance(anomaly_data, list):
                        risk_data['risk_stats'][anomaly_type] = {
                            'total_count': len(anomaly_data),
                            'total_amount': sum(item.get('amount', 0) for item in anomaly_data if isinstance(item, dict)),
                            'unique_accounts': len(set(item.get('gl_account', '') for item in anomaly_data if isinstance(item, dict))),
                            'unique_users': len(set(item.get('user_name', '') for item in anomaly_data if isinstance(item, dict)))
                        }
                    elif isinstance(anomaly_data, dict):
                        risk_data['risk_stats'][anomaly_type] = {
                            'total_count': anomaly_data.get('count', 0),
                            'total_amount': anomaly_data.get('total_amount', 0),
                            'unique_accounts': anomaly_data.get('unique_accounts', 0),
                            'unique_users': anomaly_data.get('unique_users', 0)
                        }
            
            return risk_data
            
        except Exception as e:
            logger.error(f"Error compiling risk data from ML response: {e}")
            return self._get_default_risk_data()
    
    def _get_duplicate_types_chart(self, details):
        """Create duplicate types chart data"""
        try:
            type_counts = {}
            for detail in details:
                dup_type = detail.get('type', 'Unknown')
                if dup_type not in type_counts:
                    type_counts[dup_type] = {
                        'type': dup_type,
                        'count': 0,
                        'total_amount': 0,
                        'avg_risk_score': 0
                    }
                type_counts[dup_type]['count'] += 1
                type_counts[dup_type]['total_amount'] += detail.get('debit_amount', 0) + detail.get('credit_amount', 0)
                type_counts[dup_type]['avg_risk_score'] += detail.get('risk_score', 0)
            
            # Calculate averages
            for dup_type in type_counts:
                count = type_counts[dup_type]['count']
                if count > 0:
                    type_counts[dup_type]['avg_risk_score'] = type_counts[dup_type]['avg_risk_score'] / count
            
            return list(type_counts.values())
            
        except Exception as e:
            logger.error(f"Error creating duplicate types chart: {e}")
            return []
    
    def _get_risk_levels_chart(self, risk_level_distribution):
        """Create risk levels chart data"""
        try:
            chart_data = []
            for level, count in risk_level_distribution.items():
                if count > 0:
                    chart_data.append({
                        'risk_level': level,
                        'count': count,
                        'percentage': (count / sum(risk_level_distribution.values())) * 100 if sum(risk_level_distribution.values()) > 0 else 0
                    })
            return chart_data
            
        except Exception as e:
            logger.error(f"Error creating risk levels chart: {e}")
            return []
    
    def _determine_overall_risk_level(self, risk_summary):
        """Determine overall risk level based on risk summary"""
        try:
            avg_risk_score = risk_summary.get('average_risk_score', 0)
            total_duplicates = risk_summary.get('total_duplicates', 0)
            
            if avg_risk_score >= 80 or total_duplicates >= 50:
                return 'CRITICAL'
            elif avg_risk_score >= 60 or total_duplicates >= 20:
                return 'HIGH'
            elif avg_risk_score >= 40 or total_duplicates >= 10:
                return 'MEDIUM'
            else:
                return 'LOW'
        except Exception as e:
            logger.error(f"Error determining overall risk level: {e}")
            return 'LOW'
    
    def _extract_key_risk_factors_from_ml(self, anomaly_results):
        """Extract key risk factors from ML model response"""
        try:
            risk_factors = []
            
            # Check for high-risk duplicates
            duplicates = anomaly_results.get('duplicate', {})
            if isinstance(duplicates, dict):
                details = duplicates.get('details', [])
                for detail in details:
                    risk_score = detail.get('risk_score', 0)
                    if risk_score >= 70:
                        risk_factors.append({
                            'type': 'High Risk Duplicate',
                            'description': f"{detail.get('type', 'Duplicate')} with risk score {risk_score}",
                            'amount': detail.get('debit_amount', 0) + detail.get('credit_amount', 0),
                            'account': detail.get('gl_account', 'Unknown')
                        })
                
                # Check for large amounts involved
                total_amount = sum(detail.get('debit_amount', 0) + detail.get('credit_amount', 0) for detail in details)
                if total_amount > 10000000:  # 10M threshold
                    risk_factors.append({
                        'type': 'Large Amount Involved',
                        'description': f"Total amount involved: {total_amount:,.2f}",
                        'severity': 'HIGH'
                    })
                
                # Check for multiple duplicate types
                if len(details) > 5:
                    risk_factors.append({
                        'type': 'Multiple Duplicate Types',
                        'description': f"Found {len(details)} different duplicate patterns",
                        'count': len(details),
                        'severity': 'HIGH'
                    })
            
            # Check for other anomaly types
            for anomaly_type in ['backdated_entries', 'user_anomalies', 'closing_entries']:
                if anomaly_type in anomaly_results and anomaly_results[anomaly_type]:
                    count = len(anomaly_results[anomaly_type]) if isinstance(anomaly_results[anomaly_type], list) else 0
                    if count > 0:
                        risk_factors.append({
                            'type': f'{anomaly_type.replace("_", " ").title()}',
                            'description': f"Found {count} {anomaly_type.replace('_', ' ')}",
                            'count': count,
                            'severity': 'MEDIUM' if count < 10 else 'HIGH'
                        })
            
            return risk_factors[:5]  # Top 5 risk factors
            
        except Exception as e:
            logger.error(f"Error extracting key risk factors from ML: {e}")
            return []
    
    def _extract_risk_trends_from_ml(self, anomaly_results):
        """Extract risk trends from ML model response"""
        try:
            trends = []
            
            # Analyze duplicates risk distribution
            duplicates = anomaly_results.get('duplicate', {})
            if isinstance(duplicates, dict):
                details = duplicates.get('details', [])
                if details:
                    # Count by duplicate type
                    type_counts = {}
                    for detail in details:
                        dup_type = detail.get('type', 'Unknown')
                        type_counts[dup_type] = type_counts.get(dup_type, 0) + 1
                    
                    for dup_type, count in type_counts.items():
                        trends.append({
                            'trend': f"{dup_type}",
                            'count': count,
                            'trend_direction': 'increasing' if count > 2 else 'stable'
                        })
                    
                    # Overall duplicates trend
                    total_duplicates = len(details)
                    trends.append({
                        'trend': 'Total Duplicate Patterns',
                        'count': total_duplicates,
                        'trend_direction': 'increasing' if total_duplicates > 5 else 'stable'
                    })
            
            # Analyze other anomaly types
            for anomaly_type in ['backdated_entries', 'user_anomalies', 'closing_entries']:
                if anomaly_type in anomaly_results and anomaly_results[anomaly_type]:
                    count = len(anomaly_results[anomaly_type]) if isinstance(anomaly_results[anomaly_type], list) else 0
                    if count > 0:
                        trends.append({
                            'trend': f"{anomaly_type.replace('_', ' ').title()}",
                            'count': count,
                            'trend_direction': 'increasing' if count > 5 else 'stable'
                        })
            
            return trends
            
        except Exception as e:
            logger.error(f"Error extracting risk trends from ML: {e}")
            return []
    
    def _generate_action_items(self, risk_summary):
        """Generate action items based on risk summary"""
        try:
            action_items = []
            
            total_duplicates = risk_summary.get('total_duplicates', 0)
            avg_risk_score = risk_summary.get('average_risk_score', 0)
            
            if total_duplicates > 0:
                action_items.append({
                    'priority': 'HIGH' if avg_risk_score >= 60 else 'MEDIUM',
                    'action': 'Review duplicate transactions',
                    'description': f"Investigate {total_duplicates} duplicate transactions"
                })
            
            if avg_risk_score >= 70:
                action_items.append({
                    'priority': 'CRITICAL',
                    'action': 'Immediate risk assessment required',
                    'description': f"High average risk score: {avg_risk_score}"
                })
            
            if total_duplicates >= 20:
                action_items.append({
                    'priority': 'HIGH',
                    'action': 'Implement duplicate prevention measures',
                    'description': f"Large number of duplicates: {total_duplicates}"
                })
            
            return action_items
            
        except Exception as e:
            logger.error(f"Error generating action items: {e}")
            return []
    
    def _get_default_risk_data(self):
        """Get default risk data when duplicate analysis is not available"""
        return {
            'risk_stats': {
                'duplicates': {
                    'total_patterns': 0,
                    'total_amount': 0,
                    'average_risk_score': 0,
                    'unique_accounts': 0,
                    'unique_users': 0,
                    'risk_levels': {'LOW': 0, 'MEDIUM': 0, 'HIGH': 0, 'CRITICAL': 0}
                }
            },
            'risk_charts': {
                'duplicate_types': [],
                'risk_levels': []
            }
        }


    def _generate_file_monthly_trends(self, transactions):
        """Generate monthly trends for file transactions"""
        if not transactions:
            return {'data': [], 'summary': {}}
        
        # Group by month
        monthly_data = {}
        for transaction in transactions:
            month_key = transaction.posting_date.strftime('%Y-%m') if transaction.posting_date else 'Unknown'
            if month_key not in monthly_data:
                monthly_data[month_key] = {
                    'count': 0,
                    'total_amount': 0,
                    'avg_amount': 0
                }
            monthly_data[month_key]['count'] += 1
            monthly_data[month_key]['total_amount'] += float(transaction.amount_local_currency)
        
        # Calculate averages and format data
        data = []
        for month, stats in sorted(monthly_data.items()):
            stats['avg_amount'] = stats['total_amount'] / stats['count'] if stats['count'] > 0 else 0
            data.append({
                'month': month,
                'count': stats['count'],
                'total_amount': stats['total_amount'],
                'avg_amount': stats['avg_amount'],
            })
        
        # Calculate summary
        total_months = len(data)
        avg_monthly_transactions = sum(item['count'] for item in data) / total_months if total_months > 0 else 0
        avg_monthly_amount = sum(item['total_amount'] for item in data) / total_months if total_months > 0 else 0
        
        return {
            'data': data,
            'summary': {
                'total_months': total_months,
                'avg_monthly_transactions': avg_monthly_transactions,
                'avg_monthly_amount': avg_monthly_amount,
            }
        }
    
    def _generate_file_amount_distribution(self, transactions):
        """Generate amount distribution for file transactions"""
        if not transactions:
            return []
        
        # Define amount ranges
        ranges = [
            (0, 1000, '0-1K'),
            (1000, 10000, '1K-10K'),
            (10000, 100000, '10K-100K'),
            (100000, 1000000, '100K-1M'),
            (1000000, float('inf'), '1M+')
        ]
        
        distribution = []
        total_transactions = len(transactions)
        
        for min_val, max_val, label in ranges:
            if max_val == float('inf'):
                count = len([t for t in transactions if float(t.amount_local_currency) >= min_val])
            else:
                count = len([t for t in transactions if min_val <= float(t.amount_local_currency) < max_val])
            
            distribution.append({
                'range': label,
                'count': count,
                'percentage': round((count / total_transactions * 100), 2) if total_transactions > 0 else 0
            })
        
        return distribution
    
    def _generate_file_department_expenses(self, transactions):
        """Generate department expenses for file transactions"""
        if not transactions:
            return []
        
        # Group by profit center
        dept_data = {}
        for transaction in transactions:
            dept = transaction.profit_center or 'Unknown'
            if dept not in dept_data:
                dept_data[dept] = {
                    'count': 0,
                    'total_amount': 0,
                    'avg_amount': 0
                }
            dept_data[dept]['count'] += 1
            dept_data[dept]['total_amount'] += float(transaction.amount_local_currency)
        
        # Calculate averages and format data
        result = []
        for dept, stats in sorted(dept_data.items(), key=lambda x: x[1]['total_amount'], reverse=True)[:10]:
            stats['avg_amount'] = stats['total_amount'] / stats['count'] if stats['count'] > 0 else 0
            result.append({
                'department': dept,
                'count': stats['count'],
                'total_amount': stats['total_amount'],
                'avg_amount': stats['avg_amount'],
            })
        
        return result
    
    def _generate_file_top_gl_accounts(self, transactions):
        """Generate top GL accounts for file transactions"""
        if not transactions:
            return {'summary': {}, 'accounts': []}
        
        # Group by GL account
        account_data = {}
        for transaction in transactions:
            account = transaction.gl_account
            if account not in account_data:
                account_data[account] = {
                    'count': 0,
                    'total_amount': 0,
                    'trial_balance': 0
                }
            account_data[account]['count'] += 1
            account_data[account]['total_amount'] += float(transaction.amount_local_currency)
            account_data[account]['trial_balance'] += float(transaction.amount_local_currency)
        
        # Calculate totals
        total_accounts = len(account_data)
        total_trial_balance = sum(stats['total_amount'] for stats in account_data.values())
        total_debits = sum(float(t.amount_local_currency) for t in transactions if t.transaction_type == 'DEBIT')
        total_credits = sum(float(t.amount_local_currency) for t in transactions if t.transaction_type == 'CREDIT')
        
        # Format accounts data
        accounts = []
        for account, stats in sorted(account_data.items(), key=lambda x: x[1]['total_amount'], reverse=True)[:20]:
            accounts.append({
                'account': account,
                'amount': stats['total_amount'],
                'transactions': stats['count'],
                'trial_balance': stats['trial_balance'],
            })
        
        return {
            'summary': {
                'total_accounts': total_accounts,
                'total_trial_balance': total_trial_balance,
                'total_trading_equity': total_trial_balance,
                'total_debits': total_debits,
                'total_credits': total_credits,
                'currency': 'SAR'
            },
            'accounts': accounts
        }
    
    def _generate_file_top_users(self, transactions):
        """Generate top users for file transactions"""
        if not transactions:
            return []
        
        # Group by user
        user_data = {}
        for transaction in transactions:
            user = transaction.user_name
            if user not in user_data:
                user_data[user] = {
                    'count': 0,
                    'total_amount': 0,
                    'avg_amount': 0
                }
            user_data[user]['count'] += 1
            user_data[user]['total_amount'] += float(transaction.amount_local_currency)
        
        # Calculate averages and format data
        result = []
        for user, stats in sorted(user_data.items(), key=lambda x: x[1]['total_amount'], reverse=True)[:10]:
            stats['avg_amount'] = stats['total_amount'] / stats['count'] if stats['count'] > 0 else 0
            result.append({
                'user': user,
                'amount': stats['total_amount'],
                'transactions': stats['count'],
                'avg_amount': stats['avg_amount'],
            })
        
        return result
    
    def _generate_file_employee_expenses(self, transactions):
        """Generate employee expenses for file transactions"""
        if not transactions:
            return {'employees': [], 'total_employees': 0, 'total_expenses': 0}
        
        # Group by user
        user_data = {}
        for transaction in transactions:
            user = transaction.user_name
            if user not in user_data:
                user_data[user] = 0
            user_data[user] += float(transaction.amount_local_currency)
        
        # Format data
        employees = []
        for user, amount in sorted(user_data.items(), key=lambda x: x[1], reverse=True)[:5]:
            employees.append({
                'name': user,
                'amount': amount,
            })
        
        return {
            'employees': employees,
            'total_employees': len(employees),
            'total_expenses': sum(emp['amount'] for emp in employees)
        }
    
    def _generate_file_document_type_analysis(self, transactions):
        """Generate document type analysis for file transactions"""
        if not transactions:
            return []
        
        # Group by document type
        doc_data = {}
        for transaction in transactions:
            doc_type = transaction.document_type or 'Unknown'
            if doc_type not in doc_data:
                doc_data[doc_type] = {
                    'count': 0,
                    'total_amount': 0
                }
            doc_data[doc_type]['count'] += 1
            doc_data[doc_type]['total_amount'] += float(transaction.amount_local_currency)
        
        # Format data
        result = []
        for doc_type, stats in sorted(doc_data.items(), key=lambda x: x[1]['total_amount'], reverse=True)[:10]:
            result.append({
                'document_type': doc_type,
                'count': stats['count'],
                'total_amount': stats['total_amount']
            })
        
        return result
    
    def _generate_file_duplicate_analysis(self, processing_job):
        """Generate duplicate analysis for file"""
        if not processing_job or not processing_job.anomaly_results:
            return {
                'total_duplicates': 0,
                'duplicate_amount': 0,
                'duplicate_transactions': [],
                'duplicate_rate': 0
            }
        
        duplicate_data = processing_job.anomaly_results.get('duplicate_analysis', {})
        return {
            'total_duplicates': duplicate_data.get('total_duplicates', 0),
            'duplicate_amount': duplicate_data.get('duplicate_amount', 0),
            'duplicate_transactions': duplicate_data.get('duplicate_transactions', [])[:20],
            'duplicate_rate': duplicate_data.get('duplicate_rate', 0)
        }
    
    def _generate_file_backdated_analysis(self, transactions):
        """Generate backdated analysis for file transactions"""
        if not transactions:
            return {
                'total_backdated': 0,
                'backdated_amount': 0,
                'backdated_transactions': [],
                'backdated_rate': 0
            }
        
        # Simple backdated detection (entries dated before current period)
        from datetime import datetime, timedelta
        current_date = datetime.now().date()
        three_months_ago = current_date - timedelta(days=90)
        
        backdated_transactions = [t for t in transactions if t.posting_date and t.posting_date < three_months_ago]
        
        return {
            'total_backdated': len(backdated_transactions),
            'backdated_amount': sum(float(t.amount_local_currency) for t in backdated_transactions),
            'backdated_transactions': [
                {
                    'document_number': t.document_number,
                    'posting_date': t.posting_date.isoformat() if t.posting_date else None,
                    'amount_local_currency': float(t.amount_local_currency),
                    'user_name': t.user_name,
                }
                for t in backdated_transactions[:20]
            ],
            'backdated_rate': round((len(backdated_transactions) / len(transactions) * 100), 2) if transactions else 0
        }
    
    def _generate_file_user_anomaly_analysis(self, transactions):
        """Generate user anomaly analysis for file transactions"""
        if not transactions:
            return {
                'total_anomalous_users': 0,
                'anomalous_users': []
            }
        
        # Group by user
        user_stats = {}
        for transaction in transactions:
            user = transaction.user_name
            if user not in user_stats:
                user_stats[user] = {
                    'count': 0,
                    'total_amount': 0,
                    'avg_amount': 0,
                    'max_amount': 0
                }
            user_stats[user]['count'] += 1
            user_stats[user]['total_amount'] += float(transaction.amount_local_currency)
            user_stats[user]['max_amount'] = max(user_stats[user]['max_amount'], float(transaction.amount_local_currency))
        
        # Calculate averages
        for user in user_stats:
            user_stats[user]['avg_amount'] = user_stats[user]['total_amount'] / user_stats[user]['count']
        
        # Calculate thresholds for anomaly detection
        amounts = [stats['total_amount'] for stats in user_stats.values()]
        if amounts:
            mean_amount = sum(amounts) / len(amounts)
            std_amount = (sum((x - mean_amount) ** 2 for x in amounts) / len(amounts)) ** 0.5
            threshold = mean_amount + (2 * std_amount)  # 2 standard deviations
            
            anomalous_users = [
                {
                    'user': user,
                    'total_amount': stats['total_amount'],
                    'transaction_count': stats['count'],
                    'avg_amount': stats['avg_amount'],
                    'max_amount': stats['max_amount'],
                    'anomaly_score': round((stats['total_amount'] - mean_amount) / std_amount, 2) if std_amount > 0 else 0,
                }
                for user, stats in user_stats.items()
                if stats['total_amount'] > threshold
            ]
        else:
            anomalous_users = []
        
        return {
            'total_anomalous_users': len(anomalous_users),
            'anomalous_users': anomalous_users[:10]  # Top 10
        }
    


class CeleryDebugViewSet(viewsets.ViewSet):
    """ViewSet for debugging and monitoring Celery worker health"""
    
    def list(self, request):
        """Get Celery worker status and health information"""
        try:
            from celery import current_app
            inspect = current_app.control.inspect()
            
            # Get worker stats
            stats = inspect.stats()
            active_tasks = inspect.active()
            registered_tasks = inspect.registered()
            
            # Test Celery connection
            connection_status = {
                'connected': bool(stats),
                'workers': list(stats.keys()) if stats else [],
                'worker_count': len(stats) if stats else 0
            }
            
            # Get active tasks info
            active_tasks_info = {}
            if active_tasks:
                for worker, tasks in active_tasks.items():
                    active_tasks_info[worker] = {
                        'task_count': len(tasks),
                        'tasks': [{'id': task['id'], 'name': task['name']} for task in tasks]
                    }
            
            # Get registered tasks info
            registered_tasks_info = {}
            if registered_tasks:
                for worker, tasks in registered_tasks.items():
                    registered_tasks_info[worker] = {
                        'task_count': len(tasks),
                        'tasks': tasks
                    }
            
            response_data = {
                'connection_status': connection_status,
                'active_tasks': active_tasks_info,
                'registered_tasks': registered_tasks_info,
                'timestamp': timezone.now().isoformat()
            }
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error getting Celery debug info: {e}")
            return Response({
                'error': str(e),
                'error_type': type(e).__name__,
                'timestamp': timezone.now().isoformat()
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def create(self, request):
        """Trigger debug tasks and health checks"""
        try:
            # Handle both DRF request and regular request
            if hasattr(request, 'data'):
                action = request.data.get('action', 'debug')
            else:
                import json
                action = json.loads(request.body.decode('utf-8')).get('action', 'debug')
            
            if action == 'debug':
                # Queue debug task instead of direct call
                return Response({
                    'action': 'debug_task',
                    'message': 'Debug task queued for processing',
                    'note': 'Using queue system - no direct Celery calls'
                }, status=status.HTTP_202_ACCEPTED)
                
            elif action == 'health_check':
                # Queue health check task instead of direct call
                return Response({
                    'action': 'health_check',
                    'message': 'Worker health check queued for processing',
                    'note': 'Using queue system - no direct Celery calls'
                }, status=status.HTTP_202_ACCEPTED)
                
            elif action == 'performance_monitor':
                # Queue performance monitoring task instead of direct call
                return Response({
                    'action': 'performance_monitor',
                    'message': 'Performance monitoring queued for processing',
                    'note': 'Using queue system - no direct Celery calls'
                }, status=status.HTTP_202_ACCEPTED)
                
            else:
                return Response({
                    'error': f'Unknown action: {action}',
                    'available_actions': ['debug', 'health_check', 'performance_monitor']
                }, status=status.HTTP_400_BAD_REQUEST)
                
        except Exception as e:
            logger.error(f"Error triggering Celery debug task: {e}")
            return Response({
                'error': str(e),
                'error_type': type(e).__name__
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def retrieve(self, request, pk=None):
        """Get task result by ID"""
        try:
            from celery.result import AsyncResult
            from analytics.celery import app
            
            task_result = AsyncResult(pk, app=app)
            
            response_data = {
                'task_id': pk,
                'status': task_result.status,
                'ready': task_result.ready(),
                'successful': task_result.successful(),
                'failed': task_result.failed(),
                'info': task_result.info if task_result.ready() else None,
                'traceback': task_result.traceback if task_result.failed() else None,
                'timestamp': timezone.now().isoformat()
            }
            
            return Response(response_data)
            
        except Exception as e:
            logger.error(f"Error getting task result: {e}")
            return Response({
                'error': str(e),
                'error_type': type(e).__name__
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class ComprehensiveDuplicateAnalysisView(generics.GenericAPIView):
    """
    Retrieve Existing ML Duplicate Analysis View
    
    This API retrieves already existing duplicate analysis results from ML model for a specific file.
    No new analysis is performed - only retrieves saved analysis data.
    """
    
    def get(self, request, file_id, *args, **kwargs):
        """Get existing ML duplicate analysis for a specific file"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            from django.core.cache import cache
            import uuid
            
            # Clean file_id
            file_id = file_id.rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Check cache first
            cache_key = f"ml_duplicate_analysis_{file_id}"
            cached_result = cache.get(cache_key)
            if cached_result:
                return Response(cached_result)
            
            # Get the data file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Retrieve existing ML analysis results
            analysis_result = self._get_existing_ml_analysis(data_file)
            
            # Cache the result for 1 hour
            cache.set(cache_key, analysis_result, 3600)
            
            return Response(analysis_result)
            
        except Exception as e:
            logger.error(f"Error in ComprehensiveDuplicateAnalysisView: {e}")
            return Response(
                {'error': f'Error retrieving ML duplicate analysis: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _get_existing_ml_analysis(self, data_file):
        """Retrieve existing ML duplicate analysis for the specific file"""
        try:
            from core.models import FileProcessingJob
            from core.ml_models import MLAnomalyDetector
            
            # Get the latest processing job for this file that has duplicate analysis
            processing_job = FileProcessingJob.objects.filter(
                data_file=data_file
            ).order_by('-created_at').first()
            
            # If no job found, try to find any job with duplicate analysis for this file
            if not processing_job:
                processing_job = FileProcessingJob.objects.filter(
                    data_file=data_file,
                    anomaly_results__isnull=False
                ).exclude(anomaly_results={}).order_by('-created_at').first()
            
            if not processing_job:
                return self._get_no_analysis_response(data_file, "No processing job found for this file")
            
            # Get anomaly results from the processing job
            anomaly_results = processing_job.anomaly_results
            
            if not anomaly_results:
                return self._get_no_analysis_response(data_file, "No anomaly results found in processing job")
            
            # Initialize ML detector to get saved duplicate analysis
            ml_detector = MLAnomalyDetector()
            
            # Try to get from processing job results first (most reliable)
            saved_analysis = anomaly_results.get('duplicate_analysis', {})
            if saved_analysis:
                print(f"Retrieved duplicate analysis from processing job for file {data_file.id}")
            else:
                # Try to get from ML model as fallback
                if ml_detector.duplicate_model and ml_detector.duplicate_model.has_saved_analysis(str(data_file.id)):
                    saved_analysis = ml_detector.duplicate_model.get_saved_analysis(str(data_file.id))
                    print(f"Retrieved saved ML analysis for file {data_file.id}")
                else:
                    return self._get_no_analysis_response(data_file, "No duplicate analysis found in processing job or ML model")
            
            # Generate response with existing analysis data
            return self._generate_ml_analysis_response(data_file, saved_analysis, processing_job)
            
        except Exception as e:
            logger.error(f"Error retrieving ML analysis for file {data_file.id}: {e}")
            return self._get_no_analysis_response(data_file, f"Error retrieving ML analysis: {str(e)}")
    
    def _get_no_analysis_response(self, data_file, message):
        """Return response when no ML analysis is found"""
        return {
            'file_info': {
                'id': str(data_file.id),
                'file_name': data_file.file_name,
                'client_name': data_file.client_name,
                'company_name': data_file.company_name,
                'fiscal_year': data_file.fiscal_year,
                'status': data_file.status,
                'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None
            },
            'analysis_status': {
                'status': 'NOT_FOUND',
                'message': message,
                'analysis_date': datetime.now().isoformat()
            },
            'duplicate_analysis': {
                'total_duplicates': 0,
                'duplicate_amount': 0,
                'duplicate_list': [],
                'duplicate_types': {},
                'top_risk_items': []
            },
            'ml_model_info': {
                'model_used': 'MLAnomalyDetector',
                'analysis_available': False,
                'message': message
            }
        }
    
    def _generate_ml_analysis_response(self, data_file, saved_analysis, processing_job):
        """Generate response with existing ML analysis data"""
        try:
            # Extract duplicate analysis data
            duplicate_list = saved_analysis.get('duplicate_list', [])
            chart_data = saved_analysis.get('chart_data', {})
            breakdowns = saved_analysis.get('breakdowns', {})
            summary_table = saved_analysis.get('summary_table', [])
            
            # Calculate key metrics
            total_duplicates = len(duplicate_list)
            total_amount = sum(item.get('amount', 0) for item in duplicate_list)
            avg_risk_score = sum(item.get('risk_score', 0) for item in duplicate_list) / len(duplicate_list) if duplicate_list else 0
            
            # Get top risk items (top 10 by risk score)
            top_risk_items = sorted(
                duplicate_list, 
                key=lambda x: x.get('risk_score', 0), 
                reverse=True
            )[:10]
            
            # Get duplicate type breakdown
            duplicate_types = {}
            for item in duplicate_list:
                dup_type = item.get('duplicate_type', 'Unknown')
                if dup_type not in duplicate_types:
                    duplicate_types[dup_type] = {'count': 0, 'amount': 0}
                duplicate_types[dup_type]['count'] += 1
                duplicate_types[dup_type]['amount'] += item.get('amount', 0)
            
            return {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'status': data_file.status,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None
                },
                'analysis_status': {
                    'status': 'FOUND',
                    'message': 'Existing ML analysis retrieved successfully',
                    'analysis_date': datetime.now().isoformat(),
                    'processing_job_id': str(processing_job.id) if processing_job else None,
                    'job_completed_at': processing_job.completed_at.isoformat() if processing_job and processing_job.completed_at else None
                },
                'duplicate_analysis': {
                    'total_duplicates': total_duplicates,
                    'duplicate_amount': total_amount,
                    'duplicate_percentage': (total_duplicates / len(duplicate_list) * 100) if duplicate_list else 0,
                    'average_risk_score': round(avg_risk_score, 2),
                    'duplicate_list': duplicate_list[:50],  # Limit to top 50
                    'duplicate_types': duplicate_types,
                    'top_risk_items': [
                        {
                            'id': item.get('id'),
                            'gl_account': item.get('gl_account'),
                            'amount': item.get('amount'),
                            'user_name': item.get('user_name'),
                            'posting_date': item.get('posting_date'),
                            'duplicate_type': item.get('duplicate_type'),
                            'risk_score': item.get('risk_score'),
                            'document_number': item.get('document_number'),
                            'text': item.get('text', '')[:100] if item.get('text') else ''
                        }
                        for item in top_risk_items
                    ]
                },
                'chart_data': chart_data,
                'breakdowns': breakdowns,
                'summary_table': summary_table[:20],  # Limit to top 20
                'ml_model_info': {
                    'model_used': 'MLAnomalyDetector',
                    'analysis_available': True,
                    'analysis_source': 'saved_ml_model' if 'duplicate_list' in saved_analysis else 'processing_job',
                    'total_analysis_items': len(duplicate_list)
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating ML analysis response: {e}")
            return self._get_no_analysis_response(data_file, f"Error processing ML analysis data: {str(e)}")
    
    def _detect_duplicates_efficiently(self, base_query):
        """Detect duplicates using efficient database queries instead of pandas"""
        from django.db.models import Count, Sum, Q
        
        duplicate_results = {
            'duplicate_groups': [],
            'total_duplicates': 0,
            'duplicate_amount': 0,
            'duplicate_types': {},
            'top_risk_items': []
        }
        
        # Type 1: Account + Amount duplicates
        type1_duplicates = base_query.values('gl_account', 'amount_local_currency').annotate(
            count=Count('id'),
            total_amount=Sum('amount_local_currency'),
            unique_users=Count('user_name', distinct=True),
            unique_docs=Count('document_number', distinct=True)
        ).filter(count__gte=2).order_by('-count')[:50]  # Limit to top 50
        
        for dup in type1_duplicates:
            duplicate_results['duplicate_groups'].append({
                'type': 'Type 1 Duplicate',
                'criteria': 'Account Number + Amount',
                'gl_account': dup['gl_account'],
                'amount': float(dup['amount_local_currency']),
                'count': dup['count'],
                'total_amount': float(dup['total_amount']),
                'unique_users': dup['unique_users'],
                'unique_docs': dup['unique_docs'],
                'risk_score': min(dup['count'] * 10, 100)
            })
        
        # Type 2: Account + Source + Amount duplicates
        type2_duplicates = base_query.values('gl_account', 'document_type', 'amount_local_currency').annotate(
            count=Count('id'),
            total_amount=Sum('amount_local_currency'),
            unique_users=Count('user_name', distinct=True)
        ).filter(count__gte=2).order_by('-count')[:30]
        
        for dup in type2_duplicates:
            duplicate_results['duplicate_groups'].append({
                'type': 'Type 2 Duplicate',
                'criteria': 'Account Number + Source + Amount',
                'gl_account': dup['gl_account'],
                'source': dup['document_type'],
                'amount': float(dup['amount_local_currency']),
                'count': dup['count'],
                'total_amount': float(dup['total_amount']),
                'unique_users': dup['unique_users'],
                'risk_score': min(dup['count'] * 12, 100)
            })
        
        # Type 3: Account + User + Amount duplicates
        type3_duplicates = base_query.values('gl_account', 'user_name', 'amount_local_currency').annotate(
            count=Count('id'),
            total_amount=Sum('amount_local_currency')
        ).filter(count__gte=2).order_by('-count')[:30]
        
        for dup in type3_duplicates:
            duplicate_results['duplicate_groups'].append({
                'type': 'Type 3 Duplicate',
                'criteria': 'Account Number + User + Amount',
                'gl_account': dup['gl_account'],
                'user_name': dup['user_name'],
                'amount': float(dup['amount_local_currency']),
                'count': dup['count'],
                'total_amount': float(dup['total_amount']),
                'risk_score': min(dup['count'] * 12, 100)
            })
        
        # Calculate summary statistics
        if duplicate_results['duplicate_groups']:
            duplicate_results['total_duplicates'] = sum(group['count'] for group in duplicate_results['duplicate_groups'])
            duplicate_results['duplicate_amount'] = sum(group['total_amount'] for group in duplicate_results['duplicate_groups'])
            
            # Get top risk items
            duplicate_results['top_risk_items'] = sorted(
                duplicate_results['duplicate_groups'], 
                key=lambda x: x['risk_score'], 
                reverse=True
            )[:10]
            
            # Get duplicate type breakdown
            for group in duplicate_results['duplicate_groups']:
                dup_type = group['type']
                if dup_type not in duplicate_results['duplicate_types']:
                    duplicate_results['duplicate_types'][dup_type] = {
                        'count': 0, 
                        'amount': 0, 
                        'groups': 0
                    }
                duplicate_results['duplicate_types'][dup_type]['count'] += group['count']
                duplicate_results['duplicate_types'][dup_type]['amount'] += group['total_amount']
                duplicate_results['duplicate_types'][dup_type]['groups'] += 1
        
        return duplicate_results
    
    def _generate_optimized_response(self, data_file, total_transactions, total_amount_result, duplicate_analysis):
        """Generate optimized response with minimal data"""
        from django.db.models import Count, Sum
        
        # Get top users by duplicate count
        top_users = SAPGLPosting.objects.filter(
            created_at__gte=data_file.uploaded_at,
            posting_date__gte=data_file.audit_start_date,
            posting_date__lte=data_file.audit_end_date,
            fiscal_year=data_file.fiscal_year
        ).values('user_name').annotate(
            total_amount=Sum('amount_local_currency'),
            transaction_count=Count('id')
        ).order_by('-total_amount')[:5]
        
        # Get top accounts by duplicate count
        top_accounts = SAPGLPosting.objects.filter(
            created_at__gte=data_file.uploaded_at,
            posting_date__gte=data_file.audit_start_date,
            posting_date__lte=data_file.audit_end_date,
            fiscal_year=data_file.fiscal_year
        ).values('gl_account').annotate(
            total_amount=Sum('amount_local_currency'),
            transaction_count=Count('id')
        ).order_by('-total_amount')[:5]
        
        # Generate insights
        insights = self._generate_insights(duplicate_analysis, total_transactions)
        
        return {
            # File information
            'file_info': {
                'id': str(data_file.id),
                'file_name': data_file.file_name,
                'client_name': data_file.client_name,
                'company_name': data_file.company_name,
                'fiscal_year': data_file.fiscal_year,
                'total_records': data_file.total_records,
                'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None
            },
            
            # Key metrics
            'summary_metrics': {
                'total_transactions': total_transactions,
                'total_duplicates': duplicate_analysis['total_duplicates'],
                'duplicate_percentage': (duplicate_analysis['total_duplicates'] / total_transactions * 100) if total_transactions > 0 else 0,
                'total_amount_involved': float(total_amount_result['total_amount'] or 0),
                'duplicate_amount': duplicate_analysis['duplicate_amount'],
                'average_amount': float(total_amount_result['avg_amount'] or 0),
                'min_amount': float(total_amount_result['min_amount'] or 0),
                'max_amount': float(total_amount_result['max_amount'] or 0),
                'analysis_date': datetime.now().isoformat()
            },
            
            # Duplicate analysis
            'duplicate_analysis': {
                'total_duplicate_groups': len(duplicate_analysis['duplicate_groups']),
                'duplicate_type_breakdown': duplicate_analysis['duplicate_types'],
                'top_risk_items': duplicate_analysis['top_risk_items'][:10],  # Top 10 only
                'duplicate_groups': duplicate_analysis['duplicate_groups'][:20]  # Top 20 only
            },
            
            # Top performers
            'top_performers': {
                'top_users': list(top_users),
                'top_accounts': list(top_accounts)
            },
            
            # Insights and recommendations
            'insights': insights,
            
            # Performance info
            'performance_info': {
                'analysis_type': 'optimized',
                'cached': False,
                'response_size_reduction': '90%+',
                'processing_time': 'fast'
            }
        }
    
    def _generate_insights(self, duplicate_analysis, total_transactions):
        """Generate actionable insights based on duplicate analysis"""
        insights = {
            'immediate_actions': [],
            'investigation_priorities': [],
            'risk_assessment': {},
            'recommendations': []
        }
        
        if not duplicate_analysis['duplicate_groups']:
            insights['immediate_actions'].append("No duplicates detected - good internal controls")
            insights['risk_assessment'] = {'level': 'LOW', 'score': 0}
            return insights
        
        # Calculate risk level
        total_duplicates = duplicate_analysis['total_duplicates']
        duplicate_percentage = (total_duplicates / total_transactions * 100) if total_transactions > 0 else 0
        
        if duplicate_percentage > 10:
            risk_level = 'HIGH'
            risk_score = 80
        elif duplicate_percentage > 5:
            risk_level = 'MEDIUM'
            risk_score = 50
        else:
            risk_level = 'LOW'
            risk_score = 20
        
        insights['risk_assessment'] = {
            'level': risk_level,
            'score': risk_score,
            'duplicate_percentage': round(duplicate_percentage, 2)
        }
        
        # Generate recommendations based on duplicate types
        type_counts = duplicate_analysis['duplicate_types']
        
        if 'Type 1 Duplicate' in type_counts and type_counts['Type 1 Duplicate']['groups'] > 5:
            insights['immediate_actions'].append("Investigate Type 1 duplicates (Account + Amount) - potential control weakness")
        
        if 'Type 3 Duplicate' in type_counts and type_counts['Type 3 Duplicate']['groups'] > 3:
            insights['immediate_actions'].append("Review user-specific duplicate patterns - possible user training needed")
        
        # Add investigation priorities
        for group in duplicate_analysis['top_risk_items'][:3]:
            insights['investigation_priorities'].append({
                'type': group['type'],
                'gl_account': group['gl_account'],
                'amount': group['amount'],
                'count': group['count'],
                'risk_score': group['risk_score']
            })
        
        # Add general recommendations
        insights['recommendations'] = [
            "Implement automated duplicate detection controls",
            "Review and strengthen approval workflows",
            "Consider implementing transaction limits",
            "Enhance user training on proper transaction entry"
        ]
        
        return insights
    
    def _get_empty_response(self, data_file):
        """Return empty response structure"""
        return {
            'file_info': {
                'id': str(data_file.id),
                'file_name': data_file.file_name,
                'client_name': data_file.client_name,
                'company_name': data_file.company_name,
                'fiscal_year': data_file.fiscal_year,
                'total_records': data_file.total_records,
                'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None
            },
            'summary_metrics': {
                'total_transactions': 0,
                'total_duplicates': 0,
                'duplicate_percentage': 0,
                'total_amount_involved': 0,
                'duplicate_amount': 0,
                'average_amount': 0,
                'min_amount': 0,
                'max_amount': 0,
                'analysis_date': datetime.now().isoformat()
            },
            'duplicate_analysis': {
                'total_duplicate_groups': 0,
                'duplicate_type_breakdown': {},
                'top_risk_items': [],
                'duplicate_groups': []
            },
            'top_performers': {
                'top_users': [],
                'top_accounts': []
            },
            'insights': {
                'immediate_actions': ['No transactions found for analysis'],
                'investigation_priorities': [],
                'risk_assessment': {'level': 'N/A', 'score': 0},
                'recommendations': []
            },
            'performance_info': {
                'analysis_type': 'optimized',
                'cached': False,
                'response_size_reduction': '90%+',
                'processing_time': 'fast'
            }
        }
    
    def _generate_ml_optimized_response(self, analysis_result, transactions, data_file, ml_detector):
        """Generate optimized response with only meaningful data from ML model"""
        
        # Extract essential data
        duplicate_list = analysis_result.get('duplicate_list', [])
        
        # Calculate key metrics
        total_transactions = len(transactions)
        total_duplicates = len(duplicate_list)
        total_amount = sum(item.get('amount', 0) for item in duplicate_list)
        avg_risk_score = sum(item.get('risk_score', 0) for item in duplicate_list) / len(duplicate_list) if duplicate_list else 0
        
        # Get top risk items (top 10 by risk score)
        top_risk_items = sorted(
            duplicate_list, 
            key=lambda x: x.get('risk_score', 0), 
            reverse=True
        )[:10]
        
        # Get duplicate type breakdown
        duplicate_types = {}
        for item in duplicate_list:
            dup_type = item.get('duplicate_type', 'Unknown')
            if dup_type not in duplicate_types:
                duplicate_types[dup_type] = {'count': 0, 'amount': 0}
            duplicate_types[dup_type]['count'] += 1
            duplicate_types[dup_type]['amount'] += item.get('amount', 0)
        
        # Get top users by duplicate count
        user_duplicates = {}
        for item in duplicate_list:
            user = item.get('user_name', 'Unknown')
            if user not in user_duplicates:
                user_duplicates[user] = {'count': 0, 'amount': 0}
            user_duplicates[user]['count'] += 1
            user_duplicates[user]['amount'] += item.get('amount', 0)
        
        top_users = sorted(
            user_duplicates.items(), 
            key=lambda x: x[1]['count'], 
            reverse=True
        )[:5]
        
        # Get top accounts by duplicate count
        account_duplicates = {}
        for item in duplicate_list:
            account = item.get('gl_account', 'Unknown')
            if account not in account_duplicates:
                account_duplicates[account] = {'count': 0, 'amount': 0}
            account_duplicates[account]['count'] += 1
            account_duplicates[account]['amount'] += item.get('amount', 0)
        
        top_accounts = sorted(
            account_duplicates.items(), 
            key=lambda x: x[1]['count'], 
            reverse=True
        )[:5]
        
        # Generate actionable insights
        insights = self._generate_actionable_insights(duplicate_list, total_transactions)
        
        # Build optimized response
        optimized_result = {
            # File information
            'file_info': {
                'id': str(data_file.id),
                'file_name': data_file.file_name,
                'client_name': data_file.client_name,
                'company_name': data_file.company_name,
                'fiscal_year': data_file.fiscal_year,
                'total_records': data_file.total_records,
                'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None
            },
            
            # Key metrics
            'summary_metrics': {
                'total_transactions': total_transactions,
                'total_duplicates': total_duplicates,
                'duplicate_percentage': (total_duplicates / total_transactions * 100) if total_transactions > 0 else 0,
                'total_amount_involved': total_amount,
                'average_risk_score': round(avg_risk_score, 2),
                'analysis_date': datetime.now().isoformat()
            },
            
            # Top risk items (most important)
            'top_risk_items': [
                {
                    'id': item.get('id'),
                    'gl_account': item.get('gl_account'),
                    'amount': item.get('amount'),
                    'user_name': item.get('user_name'),
                    'posting_date': item.get('posting_date'),
                    'duplicate_type': item.get('duplicate_type'),
                    'risk_score': item.get('risk_score'),
                    'document_number': item.get('document_number'),
                    'text': item.get('text', '')[:100] if item.get('text') else ''
                }
                for item in top_risk_items
            ],
            
            # Breakdowns
            'breakdowns': {
                'duplicate_types': duplicate_types,
                'top_users': [
                    {
                        'user_name': user,
                        'duplicate_count': data['count'],
                        'total_amount': data['amount']
                    }
                    for user, data in top_users
                ],
                'top_accounts': [
                    {
                        'gl_account': account,
                        'duplicate_count': data['count'],
                        'total_amount': data['amount']
                    }
                    for account, data in top_accounts
                ]
            },
            
            # Actionable insights
            'insights': insights,
            
            # ML model information
            'ml_model_info': {
                'model_used': 'MLAnomalyDetector',
                'duplicate_model_available': ml_detector.duplicate_model is not None,
                'model_trained': ml_detector.duplicate_model.is_trained() if ml_detector.duplicate_model else False,
                'analysis_saved': bool(analysis_result.get('analysis_result'))
            },
            
            # Risk distribution
            'risk_distribution': {
                'low_risk': len([item for item in duplicate_list if item.get('risk_score', 0) < 40]),
                'medium_risk': len([item for item in duplicate_list if 40 <= item.get('risk_score', 0) < 70]),
                'high_risk': len([item for item in duplicate_list if 70 <= item.get('risk_score', 0) < 90]),
                'critical_risk': len([item for item in duplicate_list if item.get('risk_score', 0) >= 90])
            }
        }
        
        return optimized_result
    
    def _generate_actionable_insights(self, duplicate_list, total_transactions):
        """Generate actionable insights from duplicate analysis"""
        insights = {
            'immediate_actions': [],
            'investigation_priorities': [],
            'risk_mitigation': []
        }
        
        if not duplicate_list:
            insights['immediate_actions'].append("No duplicates found - data appears clean")
            return insights
        
        # Calculate metrics for insights
        high_risk_count = len([item for item in duplicate_list if item.get('risk_score', 0) >= 70])
        high_value_duplicates = [item for item in duplicate_list if item.get('amount', 0) > 10000]
        frequent_users = {}
        
        for item in duplicate_list:
            user = item.get('user_name', 'Unknown')
            frequent_users[user] = frequent_users.get(user, 0) + 1
        
        # Immediate actions
        if high_risk_count > 0:
            insights['immediate_actions'].append(f"Review {high_risk_count} high-risk duplicate transactions immediately")
        
        if high_value_duplicates:
            insights['immediate_actions'].append(f"Investigate {len(high_value_duplicates)} high-value duplicates (>$10,000)")
        
        # Investigation priorities
        top_frequent_user = max(frequent_users.items(), key=lambda x: x[1]) if frequent_users else None
        if top_frequent_user and top_frequent_user[1] > 5:
            insights['investigation_priorities'].append(f"Focus investigation on user '{top_frequent_user[0]}' with {top_frequent_user[1]} duplicates")
        
        # Risk mitigation
        duplicate_percentage = (len(duplicate_list) / total_transactions * 100) if total_transactions > 0 else 0
        if duplicate_percentage > 10:
            insights['risk_mitigation'].append("High duplicate rate detected - implement additional controls")
        elif duplicate_percentage > 5:
            insights['risk_mitigation'].append("Moderate duplicate rate - review existing controls")
        else:
            insights['risk_mitigation'].append("Low duplicate rate - maintain current controls")
        
        return insights
    
    def _optimize_analysis_response(self, analysis_result):
        """Optimize analysis response by removing duplicates and irrelevant data"""
        if not analysis_result:
            return analysis_result
        
        optimized_result = {}
        
        # Keep essential analysis info
        if 'analysis_info' in analysis_result:
            optimized_result['analysis_info'] = analysis_result['analysis_info']
        
        # Keep file info
        if 'file_info' in analysis_result:
            optimized_result['file_info'] = analysis_result['file_info']
        
        # Keep ML model info
        if 'ml_model_info' in analysis_result:
            optimized_result['ml_model_info'] = analysis_result['ml_model_info']
        
        # Optimize duplicate list - remove redundant fields and limit size
        if 'duplicate_list' in analysis_result:
            optimized_duplicates = []
            seen_combinations = set()
            
            for item in analysis_result['duplicate_list']:
                # Create unique key to avoid duplicates
                key = f"{item.get('gl_account', '')}_{item.get('amount', 0)}_{item.get('user_name', '')}_{item.get('posting_date', '')}"
                
                if key not in seen_combinations:
                    seen_combinations.add(key)
                    
                    # Keep only essential fields
                    optimized_item = {
                        'id': item.get('id'),
                        'gl_account': item.get('gl_account'),
                        'amount': item.get('amount'),
                        'user_name': item.get('user_name'),
                        'posting_date': item.get('posting_date'),
                        'duplicate_type': item.get('duplicate_type'),
                        'risk_score': item.get('risk_score'),
                        'document_number': item.get('document_number'),
                        'text': item.get('text', '')[:100] if item.get('text') else ''  # Limit text length
                    }
                    optimized_duplicates.append(optimized_item)
            
            # Limit to top 100 most relevant duplicates
            optimized_result['duplicate_list'] = sorted(
                optimized_duplicates, 
                key=lambda x: x.get('risk_score', 0), 
                reverse=True
            )[:100]
        
        # Optimize chart data - keep only essential charts
        if 'chart_data' in analysis_result:
            chart_data = analysis_result['chart_data']
            optimized_result['chart_data'] = {
                'duplicate_type_chart': chart_data.get('duplicate_type_chart', []),
                'monthly_trend_chart': chart_data.get('monthly_trend_chart', [])[:12],  # Limit to 12 months
                'user_breakdown_chart': chart_data.get('user_breakdown_chart', [])[:10],  # Top 10 users
                'fs_line_chart': chart_data.get('fs_line_chart', [])[:10],  # Top 10 FS lines
                'amount_distribution_chart': chart_data.get('amount_distribution_chart', []),
                'risk_level_chart': chart_data.get('risk_level_chart', [])
            }
        
        # Optimize breakdowns - keep only essential breakdowns
        if 'breakdowns' in analysis_result:
            breakdowns = analysis_result['breakdowns']
            optimized_result['breakdowns'] = {
                'duplicate_flags': breakdowns.get('duplicate_flags', {}),
                'user_breakdown': dict(list(breakdowns.get('user_breakdown', {}).items())[:10]),  # Top 10 users
                'fs_line_breakdown': dict(list(breakdowns.get('fs_line_breakdown', {}).items())[:10]),  # Top 10 FS lines
                'type_breakdown': breakdowns.get('type_breakdown', {}),
                'risk_breakdown': breakdowns.get('risk_breakdown', {})
            }
        
        # Optimize slicer filters - keep only essential filters
        if 'slicer_filters' in analysis_result:
            slicer_filters = analysis_result['slicer_filters']
            optimized_result['slicer_filters'] = {
                'duplicate_types': slicer_filters.get('duplicate_types', []),
                'users': slicer_filters.get('users', [])[:20],  # Top 20 users
                'gl_accounts': slicer_filters.get('gl_accounts', [])[:20],  # Top 20 accounts
                'date_ranges': slicer_filters.get('date_ranges', []),
                'amount_ranges': slicer_filters.get('amount_ranges', []),
                'risk_levels': slicer_filters.get('risk_levels', [])
            }
        
        # Optimize summary table - use comprehensive summary from complete expense data
        if 'complete_expense_data' in analysis_result:
            complete_data = analysis_result['complete_expense_data']
            comprehensive_summary = complete_data.get('comprehensive_summary_table', [])
            
            # Use the comprehensive summary table if available, otherwise create from duplicate transactions
            if comprehensive_summary:
                optimized_result['summary_table'] = comprehensive_summary[:50]  # Limit to top 50 items
            else:
                duplicate_transactions = complete_data.get('duplicate_transactions', [])
                enhanced_summary = []
                for transaction in duplicate_transactions[:50]:  # Limit to top 50 items
                    enhanced_item = {
                        'journal_id': transaction.get('journal_id'),
                        'gl_account': transaction.get('gl_account'),
                        'amount': transaction.get('amount'),
                        'user_name': transaction.get('user_name'),
                        'posting_date': transaction.get('posting_date'),
                        'duplicate_type': transaction.get('duplicate_type'),
                        'risk_score': transaction.get('risk_score'),
                        'count': 1,  # Individual transaction count
                        'document_number': transaction.get('document_number'),
                        'document_type': transaction.get('document_type'),
                        'text': transaction.get('text'),
                        'source': transaction.get('source'),
                        'profit_center': transaction.get('profit_center'),
                        'cost_center': transaction.get('cost_center'),
                        'debit_amount': transaction.get('debit_amount'),
                        'credit_amount': transaction.get('credit_amount'),
                        'currency': transaction.get('currency'),
                        'is_duplicate': transaction.get('is_duplicate'),
                        'created_at': transaction.get('created_at'),
                        'updated_at': transaction.get('updated_at')
                    }
                    enhanced_summary.append(enhanced_item)
                
                optimized_result['summary_table'] = enhanced_summary
        elif 'summary_table' in analysis_result:
            # Fallback to original summary table if complete expense data not available
            summary_table = analysis_result['summary_table']
            optimized_summary = []
            
            for item in summary_table[:50]:  # Limit to top 50 items
                optimized_item = {
                    'journal_id': item.get('journal_id'),
                    'gl_account': item.get('gl_account'),
                    'amount': item.get('amount'),
                    'user_name': item.get('user_name'),
                    'posting_date': item.get('posting_date'),
                    'duplicate_type': item.get('duplicate_type'),
                    'risk_score': item.get('risk_score'),
                    'count': item.get('count')
                }
                optimized_summary.append(optimized_item)
            
            optimized_result['summary_table'] = optimized_summary
        
        # Keep detailed analysis but optimize it
        if 'detailed_analysis' in analysis_result:
            detailed_analysis = analysis_result['detailed_analysis']
            optimized_result['detailed_analysis'] = {
                'duplicate_detection_summary': detailed_analysis.get('duplicate_detection_summary', {}),
                'duplicate_type_analysis': detailed_analysis.get('duplicate_type_analysis', {}),
                'risk_analysis': detailed_analysis.get('risk_analysis', {}),
                'amount_analysis': detailed_analysis.get('amount_analysis', {}),
                'user_analysis': {
                    'top_users_by_duplicates': detailed_analysis.get('user_analysis', {}).get('top_users_by_duplicates', [])[:5],
                    'top_users_by_amount': detailed_analysis.get('user_analysis', {}).get('top_users_by_amount', [])[:5]
                },
                'account_analysis': {
                    'top_accounts_by_duplicates': detailed_analysis.get('account_analysis', {}).get('top_accounts_by_duplicates', [])[:5],
                    'top_accounts_by_amount': detailed_analysis.get('account_analysis', {}).get('top_accounts_by_amount', [])[:5]
                },
                'temporal_analysis': detailed_analysis.get('temporal_analysis', {})
            }
        
        # Keep detailed insights but optimize them
        if 'detailed_insights' in analysis_result:
            detailed_insights = analysis_result['detailed_insights']
            optimized_result['detailed_insights'] = {
                'duplicate_patterns': {
                    'most_common_patterns': detailed_insights.get('duplicate_patterns', {}).get('most_common_patterns', [])[:3],
                    'unusual_patterns': detailed_insights.get('duplicate_patterns', {}).get('unusual_patterns', [])[:3]
                },
                'anomaly_indicators': {
                    'high_value_duplicates': detailed_insights.get('anomaly_indicators', {}).get('high_value_duplicates', [])[:3],
                    'frequent_duplicates': detailed_insights.get('anomaly_indicators', {}).get('frequent_duplicates', [])[:3],
                    'time_based_anomalies': detailed_insights.get('anomaly_indicators', {}).get('time_based_anomalies', [])[:3]
                },
                'risk_assessment': {
                    'high_risk_groups': detailed_insights.get('risk_assessment', {}).get('high_risk_groups', [])[:5],
                    'risk_distribution': detailed_insights.get('risk_assessment', {}).get('risk_distribution', {}),
                    'mitigation_suggestions': detailed_insights.get('risk_assessment', {}).get('mitigation_suggestions', [])[:3]
                },
                'audit_recommendations': {
                    'immediate_actions': detailed_insights.get('audit_recommendations', {}).get('immediate_actions', [])[:2],
                    'investigation_priorities': detailed_insights.get('audit_recommendations', {}).get('investigation_priorities', [])[:3],
                    'control_improvements': detailed_insights.get('audit_recommendations', {}).get('control_improvements', [])[:2],
                    'monitoring_suggestions': detailed_insights.get('audit_recommendations', {}).get('monitoring_suggestions', [])[:3]
                },
                'trend_analysis': {
                    'temporal_trends': dict(list(detailed_insights.get('trend_analysis', {}).get('temporal_trends', {}).items())[:6]),  # Last 6 months
                    'amount_trends': detailed_insights.get('trend_analysis', {}).get('amount_trends', {})
                },
                'comparative_analysis': detailed_insights.get('comparative_analysis', {})
            }
        
        # Keep complete expense data but optimize it
        if 'complete_expense_data' in analysis_result:
            complete_expense_data = analysis_result['complete_expense_data']
            optimized_result['complete_expense_data'] = {
                'expense_summary': complete_expense_data.get('expense_summary', {}),
                'account_breakdown': dict(list(complete_expense_data.get('account_breakdown', {}).items())[:10]),  # Top 10 accounts
                'user_breakdown': dict(list(complete_expense_data.get('user_breakdown', {}).items())[:10]),  # Top 10 users
                'temporal_breakdown': dict(list(complete_expense_data.get('temporal_breakdown', {}).items())[:6]),  # Last 6 months
                'amount_analysis': complete_expense_data.get('amount_analysis', {}),
                'risk_analysis': complete_expense_data.get('risk_analysis', {}),
                'all_transactions_count': len(complete_expense_data.get('all_transactions', [])),
                'duplicate_transactions_count': len(complete_expense_data.get('duplicate_transactions', [])),
                'non_duplicate_transactions_count': len(complete_expense_data.get('non_duplicate_transactions', []))
            }
        
        # Keep suggestions and recommendations but optimize them
        if 'suggestions_and_recommendations' in analysis_result:
            suggestions = analysis_result['suggestions_and_recommendations']
            optimized_result['suggestions_and_recommendations'] = {
                'immediate_actions': suggestions.get('immediate_actions', [])[:3],
                'investigation_priorities': suggestions.get('investigation_priorities', [])[:3],
                'control_improvements': suggestions.get('control_improvements', [])[:3],
                'monitoring_suggestions': suggestions.get('monitoring_suggestions', [])[:3],
                'risk_mitigation': suggestions.get('risk_mitigation', [])[:3],
                'audit_recommendations': suggestions.get('audit_recommendations', [])[:3],
                'process_improvements': suggestions.get('process_improvements', [])[:3],
                'technology_recommendations': suggestions.get('technology_recommendations', [])[:3],
                'training_recommendations': suggestions.get('training_recommendations', [])[:3],
                'compliance_recommendations': suggestions.get('compliance_recommendations', [])[:3]
            }
        
        # Remove export_data to reduce response size
        # if 'export_data' in analysis_result:
        #     optimized_result['export_data'] = analysis_result['export_data']
        
        return optimized_result
    
    def _generate_complete_expense_data(self, transactions, duplicate_list):
        """Generate complete expense data with detailed transaction information"""
        try:
            print(f"Starting complete expense data generation with {len(transactions)} transactions and {len(duplicate_list)} duplicates")
            
            complete_expense_data = {
                'all_transactions': [],
                'duplicate_transactions': [],
                'non_duplicate_transactions': [],
                'expense_summary': {},
                'transaction_details': {},
                'account_breakdown': {},
                'user_breakdown': {},
                'temporal_breakdown': {},
                'amount_analysis': {},
                'risk_analysis': {}
            }
            
            print(f"Initialized complete_expense_data structure")
            
            # Get all transaction IDs that are duplicates
            duplicate_ids = set()
            for duplicate_item in duplicate_list:
                if duplicate_item.get('id'):
                    duplicate_ids.add(duplicate_item.get('id'))
                elif duplicate_item.get('document_number'):  # Fallback to document_number if id is not available
                    duplicate_ids.add(duplicate_item.get('document_number'))
            
            print(f"Found {len(duplicate_ids)} duplicate IDs")
            
            # Process all transactions
            print(f"Starting to process {len(transactions)} transactions")
            for i, transaction in enumerate(transactions):
                if i % 10 == 0:  # Print progress every 10 transactions
                    print(f"Processing transaction {i+1}/{len(transactions)}")
                transaction_data = {
                    'id': transaction.id,
                    'journal_id': transaction.document_number,  # Use document_number as journal_id
                    'gl_account': transaction.gl_account,
                    'amount': float(transaction.amount_local_currency) if transaction.amount_local_currency else 0,
                    'user_name': transaction.user_name,
                    'posting_date': transaction.posting_date.isoformat() if transaction.posting_date else None,
                    'document_date': transaction.document_date.isoformat() if transaction.document_date else None,
                    'document_number': transaction.document_number,
                    'document_type': transaction.document_type,
                    'text': transaction.text,
                    'source': transaction.local_currency,  # Use local_currency as source
                    'profit_center': transaction.profit_center,
                    'cost_center': transaction.cost_center,
                    'debit_amount': float(transaction.amount_local_currency) if transaction.transaction_type == 'DEBIT' and transaction.amount_local_currency else 0,
                    'credit_amount': float(transaction.amount_local_currency) if transaction.transaction_type == 'CREDIT' and transaction.amount_local_currency else 0,
                    'currency': transaction.local_currency,
                    'is_duplicate': False,  # Will be set below
                    'duplicate_type': None,
                    'risk_score': 0,
                    'created_at': transaction.created_at.isoformat() if transaction.created_at else None,
                    'updated_at': transaction.updated_at.isoformat() if transaction.updated_at else None
                }
                
                # Add duplicate information if this transaction is a duplicate
                is_duplicate = str(transaction.id) in duplicate_ids or transaction.document_number in duplicate_ids
                transaction_data['is_duplicate'] = is_duplicate
                
                if i == 0:  # Debug first transaction
                    print(f"First transaction - ID: {transaction.id} (type: {type(transaction.id)})")
                    print(f"First transaction - Doc: {transaction.document_number} (type: {type(transaction.document_number)})")
                    print(f"Duplicate IDs: {duplicate_ids}")
                    print(f"Duplicate ID types: {[type(did) for did in duplicate_ids]}")
                    print(f"Is duplicate: {is_duplicate}")
                    print(f"ID in duplicate_ids: {transaction.id in duplicate_ids}")
                    print(f"Doc in duplicate_ids: {transaction.document_number in duplicate_ids}")
                
                if is_duplicate:
                    for duplicate_item in duplicate_list:
                        if (duplicate_item.get('id') == str(transaction.id) or 
                            duplicate_item.get('document_number') == transaction.document_number):
                            transaction_data['duplicate_type'] = duplicate_item.get('duplicate_type')
                            transaction_data['risk_score'] = duplicate_item.get('risk_score', 0)
                            break
                    complete_expense_data['duplicate_transactions'].append(transaction_data)
                else:
                    complete_expense_data['non_duplicate_transactions'].append(transaction_data)
                
                complete_expense_data['all_transactions'].append(transaction_data)
            
            print(f"Processed {len(complete_expense_data['all_transactions'])} transactions")
            print(f"Found {len(complete_expense_data['duplicate_transactions'])} duplicate transactions")
            print(f"Found {len(complete_expense_data['non_duplicate_transactions'])} non-duplicate transactions")
            
            # Debug: Check if transactions are being added
            if len(complete_expense_data['all_transactions']) == 0:
                print("ERROR: No transactions were added to all_transactions!")
                print("This suggests there's an error in the transaction processing loop")
            else:
                print(f"SUCCESS: {len(complete_expense_data['all_transactions'])} transactions processed successfully")
            
            # Generate expense summary
            try:
                all_transactions = complete_expense_data['all_transactions']
                duplicate_transactions = complete_expense_data['duplicate_transactions']
                non_duplicate_transactions = complete_expense_data['non_duplicate_transactions']
                
                complete_expense_data['expense_summary'] = {
                    'total_transactions': len(all_transactions),
                    'total_amount': sum(t['amount'] for t in all_transactions),
                    'duplicate_transactions': len(duplicate_transactions),
                    'duplicate_amount': sum(t['amount'] for t in duplicate_transactions),
                    'non_duplicate_transactions': len(non_duplicate_transactions),
                    'non_duplicate_amount': sum(t['amount'] for t in non_duplicate_transactions),
                    'average_amount': sum(t['amount'] for t in all_transactions) / len(all_transactions) if all_transactions else 0,
                    'min_amount': min((t['amount'] for t in all_transactions), default=0),
                    'max_amount': max((t['amount'] for t in all_transactions), default=0)
                }
            except Exception as e:
                logger.error(f"Error generating expense summary: {e}")
                complete_expense_data['expense_summary'] = {
                    'total_transactions': 0,
                    'total_amount': 0,
                    'duplicate_transactions': 0,
                    'duplicate_amount': 0,
                    'non_duplicate_transactions': 0,
                    'non_duplicate_amount': 0,
                    'average_amount': 0,
                    'min_amount': 0,
                    'max_amount': 0
                }
            
            # Generate account breakdown
            account_groups = {}
            for transaction in complete_expense_data['all_transactions']:
                account = transaction['gl_account']
                if account not in account_groups:
                    account_groups[account] = {
                        'transactions': [],
                        'total_amount': 0,
                        'duplicate_count': 0,
                        'duplicate_amount': 0
                    }
                account_groups[account]['transactions'].append(transaction)
                account_groups[account]['total_amount'] += transaction['amount']
                if transaction['is_duplicate']:
                    account_groups[account]['duplicate_count'] += 1
                    account_groups[account]['duplicate_amount'] += transaction['amount']
            
            complete_expense_data['account_breakdown'] = {
                account: {
                    'total_transactions': len(data['transactions']),
                    'total_amount': data['total_amount'],
                    'duplicate_transactions': data['duplicate_count'],
                    'duplicate_amount': data['duplicate_amount'],
                    'duplicate_percentage': (data['duplicate_count'] / len(data['transactions'])) * 100 if data['transactions'] else 0,
                    'average_amount': data['total_amount'] / len(data['transactions']) if data['transactions'] else 0
                }
                for account, data in account_groups.items()
            }
            
            # Generate user breakdown
            user_groups = {}
            for transaction in complete_expense_data['all_transactions']:
                user = transaction['user_name'] or 'Unknown'
                if user not in user_groups:
                    user_groups[user] = {
                        'transactions': [],
                        'total_amount': 0,
                        'duplicate_count': 0,
                        'duplicate_amount': 0
                    }
                user_groups[user]['transactions'].append(transaction)
                user_groups[user]['total_amount'] += transaction['amount']
                if transaction['is_duplicate']:
                    user_groups[user]['duplicate_count'] += 1
                    user_groups[user]['duplicate_amount'] += transaction['amount']
            
            complete_expense_data['user_breakdown'] = {
                user: {
                    'total_transactions': len(data['transactions']),
                    'total_amount': data['total_amount'],
                    'duplicate_transactions': data['duplicate_count'],
                    'duplicate_amount': data['duplicate_amount'],
                    'duplicate_percentage': (data['duplicate_count'] / len(data['transactions'])) * 100 if data['transactions'] else 0,
                    'average_amount': data['total_amount'] / len(data['transactions']) if data['transactions'] else 0
                }
                for user, data in user_groups.items()
            }
            
            # Generate temporal breakdown
            temporal_groups = {}
            for transaction in complete_expense_data['all_transactions']:
                if transaction['posting_date']:
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(transaction['posting_date'].replace('Z', '+00:00'))
                        month_key = dt.strftime('%Y-%m')
                        day_key = dt.strftime('%Y-%m-%d')
                        
                        if month_key not in temporal_groups:
                            temporal_groups[month_key] = {
                                'transactions': [],
                                'total_amount': 0,
                                'duplicate_count': 0,
                                'duplicate_amount': 0,
                                'days': {}
                            }
                        
                        if day_key not in temporal_groups[month_key]['days']:
                            temporal_groups[month_key]['days'][day_key] = {
                                'transactions': [],
                                'total_amount': 0,
                                'duplicate_count': 0,
                                'duplicate_amount': 0
                            }
                        
                        temporal_groups[month_key]['transactions'].append(transaction)
                        temporal_groups[month_key]['total_amount'] += transaction['amount']
                        temporal_groups[month_key]['days'][day_key]['transactions'].append(transaction)
                        temporal_groups[month_key]['days'][day_key]['total_amount'] += transaction['amount']
                        
                        if transaction['is_duplicate']:
                            temporal_groups[month_key]['duplicate_count'] += 1
                            temporal_groups[month_key]['duplicate_amount'] += transaction['amount']
                            temporal_groups[month_key]['days'][day_key]['duplicate_count'] += 1
                            temporal_groups[month_key]['days'][day_key]['duplicate_amount'] += transaction['amount']
                    except:
                        pass
            
            complete_expense_data['temporal_breakdown'] = {
                month: {
                    'total_transactions': len(data['transactions']),
                    'total_amount': data['total_amount'],
                    'duplicate_transactions': data['duplicate_count'],
                    'duplicate_amount': data['duplicate_amount'],
                    'duplicate_percentage': (data['duplicate_count'] / len(data['transactions'])) * 100 if data['transactions'] else 0,
                    'average_amount': data['total_amount'] / len(data['transactions']) if data['transactions'] else 0,
                    'daily_breakdown': {
                        day: {
                            'total_transactions': len(day_data['transactions']),
                            'total_amount': day_data['total_amount'],
                            'duplicate_transactions': day_data['duplicate_count'],
                            'duplicate_amount': day_data['duplicate_amount'],
                            'duplicate_percentage': (day_data['duplicate_count'] / len(day_data['transactions'])) * 100 if day_data['transactions'] else 0
                        }
                        for day, day_data in data['days'].items()
                    }
                }
                for month, data in temporal_groups.items()
            }
            
            # Generate amount analysis
            amounts = [t['amount'] for t in complete_expense_data['all_transactions']]
            duplicate_amounts = [t['amount'] for t in complete_expense_data['duplicate_transactions']]
            
            complete_expense_data['amount_analysis'] = {
                'overall': {
                    'min_amount': min(amounts) if amounts else 0,
                    'max_amount': max(amounts) if amounts else 0,
                    'average_amount': sum(amounts) / len(amounts) if amounts else 0,
                    'median_amount': sorted(amounts)[len(amounts)//2] if amounts else 0,
                    'total_amount': sum(amounts),
                    'transaction_count': len(amounts)
                },
                'duplicates': {
                    'min_amount': min(duplicate_amounts) if duplicate_amounts else 0,
                    'max_amount': max(duplicate_amounts) if duplicate_amounts else 0,
                    'average_amount': sum(duplicate_amounts) / len(duplicate_amounts) if duplicate_amounts else 0,
                    'median_amount': sorted(duplicate_amounts)[len(duplicate_amounts)//2] if duplicate_amounts else 0,
                    'total_amount': sum(duplicate_amounts),
                    'transaction_count': len(duplicate_amounts)
                },
                'amount_ranges': {
                    'low': len([a for a in amounts if a < 1000]),
                    'medium': len([a for a in amounts if 1000 <= a < 10000]),
                    'high': len([a for a in amounts if 10000 <= a < 100000]),
                    'very_high': len([a for a in amounts if a >= 100000])
                }
            }
            
            # Generate risk analysis
            risk_scores = [t['risk_score'] for t in complete_expense_data['duplicate_transactions']]
            complete_expense_data['risk_analysis'] = {
                'average_risk_score': sum(risk_scores) / len(risk_scores) if risk_scores else 0,
                'max_risk_score': max(risk_scores) if risk_scores else 0,
                'min_risk_score': min(risk_scores) if risk_scores else 0,
                'risk_distribution': {
                    'low_risk': len([r for r in risk_scores if r < 40]),
                    'medium_risk': len([r for r in risk_scores if 40 <= r < 70]),
                    'high_risk': len([r for r in risk_scores if 70 <= r < 90]),
                    'critical_risk': len([r for r in risk_scores if r >= 90])
                },
                'high_risk_transactions': [
                    t for t in complete_expense_data['duplicate_transactions'] 
                    if t['risk_score'] >= 70
                ]
            }
            
            # Generate comprehensive summary table with all transaction details
            complete_expense_data['comprehensive_summary_table'] = self._generate_comprehensive_summary_table(
                complete_expense_data['duplicate_transactions']
            )
            
            return complete_expense_data
            
        except Exception as e:
            print(f"ERROR in _generate_complete_expense_data: {e}")
            import traceback
            traceback.print_exc()
            logger.error(f"Error generating complete expense data: {e}")
            return {
                'error': f'Error generating complete expense data: {str(e)}',
                'all_transactions': [],
                'duplicate_transactions': [],
                'non_duplicate_transactions': [],
                'expense_summary': {},
                'account_breakdown': {},
                'user_breakdown': {},
                'temporal_breakdown': {},
                'amount_analysis': {},
                'risk_analysis': {},
                'comprehensive_summary_table': []
            }
    
    def _generate_comprehensive_summary_table(self, duplicate_transactions):
        """Generate comprehensive summary table with complete transaction details"""
        try:
            comprehensive_summary = []
            
            for transaction in duplicate_transactions:
                # Create comprehensive summary entry with all available data
                summary_entry = {
                    # Basic transaction info
                    'id': transaction.get('id'),
                    'journal_id': transaction.get('journal_id'),
                    'gl_account': transaction.get('gl_account'),
                    'amount': transaction.get('amount'),
                    'user_name': transaction.get('user_name'),
                    'posting_date': transaction.get('posting_date'),
                    'document_date': transaction.get('document_date'),
                    
                    # Duplicate analysis info
                    'duplicate_type': transaction.get('duplicate_type'),
                    'risk_score': transaction.get('risk_score'),
                    'is_duplicate': transaction.get('is_duplicate'),
                    
                    # Document info
                    'document_number': transaction.get('document_number'),
                    'document_type': transaction.get('document_type'),
                    'text': transaction.get('text'),
                    'source': transaction.get('source'),
                    
                    # Financial info
                    'debit_amount': transaction.get('debit_amount'),
                    'credit_amount': transaction.get('credit_amount'),
                    'currency': transaction.get('currency'),
                    
                    # Organizational info
                    'profit_center': transaction.get('profit_center'),
                    'cost_center': transaction.get('cost_center'),
                    
                    # Metadata
                    'created_at': transaction.get('created_at'),
                    'updated_at': transaction.get('updated_at'),
                    
                    # Calculated fields
                    'count': 1,  # Individual transaction
                    'risk_level': self._get_risk_level(transaction.get('risk_score', 0)),
                    'amount_formatted': f"{transaction.get('amount', 0):,.2f}" if transaction.get('amount') else "0.00"
                }
                
                comprehensive_summary.append(summary_entry)
            
            return comprehensive_summary
            
        except Exception as e:
            logger.error(f"Error generating comprehensive summary table: {e}")
            return []
    
    def _get_risk_level(self, risk_score):
        """Get risk level from score"""
        if risk_score < 40:
            return 'LOW'
        elif risk_score < 70:
            return 'MEDIUM'
        elif risk_score < 90:
            return 'HIGH'
        else:
            return 'CRITICAL'
    
    def _generate_suggestions_and_recommendations(self, duplicate_list, detailed_analysis, transactions):
        """Generate comprehensive suggestions and recommendations based on analysis"""
        try:
            suggestions_and_recommendations = {
                'immediate_actions': [],
                'investigation_priorities': [],
                'control_improvements': [],
                'monitoring_suggestions': [],
                'risk_mitigation': [],
                'audit_recommendations': [],
                'process_improvements': [],
                'technology_recommendations': [],
                'training_recommendations': [],
                'compliance_recommendations': []
            }
            
            # Analyze duplicate patterns and generate recommendations
            if duplicate_list:
                total_duplicates = len(duplicate_list)
                total_amount = sum(item.get('amount', 0) for item in duplicate_list)
                avg_risk_score = sum(item.get('risk_score', 0) for item in duplicate_list) / len(duplicate_list)
                
                # Immediate actions based on risk level
                if avg_risk_score >= 90:
                    suggestions_and_recommendations['immediate_actions'].extend([
                        {
                            'priority': 'Critical',
                            'action': 'Immediate suspension of affected user accounts',
                            'description': 'High-risk duplicates detected require immediate user account suspension',
                            'impact': 'Prevents further fraudulent transactions'
                        },
                        {
                            'priority': 'Critical',
                            'action': 'Freeze affected GL accounts',
                            'description': 'Temporarily freeze GL accounts with high-risk duplicates',
                            'impact': 'Prevents additional duplicate postings'
                        }
                    ])
                elif avg_risk_score >= 70:
                    suggestions_and_recommendations['immediate_actions'].extend([
                        {
                            'priority': 'High',
                            'action': 'Enhanced monitoring of affected accounts',
                            'description': 'Implement real-time monitoring for accounts with high-risk duplicates',
                            'impact': 'Early detection of suspicious activities'
                        },
                        {
                            'priority': 'High',
                            'action': 'Review and approve all transactions above threshold',
                            'description': 'Implement manual approval for transactions above certain amounts',
                            'impact': 'Reduces risk of unauthorized transactions'
                        }
                    ])
                
                # Investigation priorities
                high_value_duplicates = [item for item in duplicate_list if item.get('amount', 0) > 10000]
                if high_value_duplicates:
                    suggestions_and_recommendations['investigation_priorities'].extend([
                        {
                            'priority': 'High',
                            'focus': 'High-value duplicate transactions',
                            'description': f'Investigate {len(high_value_duplicates)} high-value duplicates totaling {sum(item.get("amount", 0) for item in high_value_duplicates):,.2f}',
                            'transactions': [item.get('id') for item in high_value_duplicates[:5]]
                        }
                    ])
                
                frequent_users = {}
                for item in duplicate_list:
                    user = item.get('user_name')
                    if user:
                        frequent_users[user] = frequent_users.get(user, 0) + 1
                
                top_frequent_users = sorted(frequent_users.items(), key=lambda x: x[1], reverse=True)[:3]
                if top_frequent_users:
                    suggestions_and_recommendations['investigation_priorities'].extend([
                        {
                            'priority': 'Medium',
                            'focus': 'Users with frequent duplicates',
                            'description': f'Investigate users with highest duplicate frequency: {", ".join([f"{user} ({count})" for user, count in top_frequent_users])}',
                            'users': [user for user, count in top_frequent_users]
                        }
                    ])
                
                # Control improvements
                suggestions_and_recommendations['control_improvements'].extend([
                    {
                        'control': 'Duplicate Detection System',
                        'description': 'Implement real-time duplicate detection in the posting system',
                        'benefit': 'Prevents duplicates before they are posted',
                        'implementation': 'Medium complexity, 2-3 months'
                    },
                    {
                        'control': 'Approval Workflow',
                        'description': 'Implement multi-level approval for transactions above threshold',
                        'benefit': 'Reduces risk of unauthorized high-value transactions',
                        'implementation': 'High complexity, 4-6 months'
                    },
                    {
                        'control': 'User Access Controls',
                        'description': 'Implement role-based access controls and segregation of duties',
                        'benefit': 'Prevents single user from creating and approving transactions',
                        'implementation': 'Medium complexity, 3-4 months'
                    }
                ])
                
                # Monitoring suggestions
                suggestions_and_recommendations['monitoring_suggestions'].extend([
                    {
                        'monitoring_area': 'Real-time Transaction Monitoring',
                        'description': 'Monitor transactions in real-time for duplicate patterns',
                        'frequency': 'Continuous',
                        'thresholds': 'Amount > 1000, Same account + amount within 24 hours'
                    },
                    {
                        'monitoring_area': 'User Activity Monitoring',
                        'description': 'Monitor user posting patterns and frequency',
                        'frequency': 'Daily',
                        'thresholds': 'User posting > 50 transactions/day, High-value transactions'
                    },
                    {
                        'monitoring_area': 'Account Activity Monitoring',
                        'description': 'Monitor specific GL accounts for unusual activity',
                        'frequency': 'Daily',
                        'thresholds': 'Account with > 10 duplicates in a month'
                    }
                ])
                
                # Risk mitigation strategies
                suggestions_and_recommendations['risk_mitigation'].extend([
                    {
                        'risk': 'Duplicate Transaction Risk',
                        'mitigation': 'Implement automated duplicate detection and prevention',
                        'effectiveness': 'High',
                        'cost': 'Medium'
                    },
                    {
                        'risk': 'User Error Risk',
                        'mitigation': 'Provide training on proper transaction posting procedures',
                        'effectiveness': 'Medium',
                        'cost': 'Low'
                    },
                    {
                        'risk': 'System Error Risk',
                        'mitigation': 'Implement system validation and reconciliation procedures',
                        'effectiveness': 'High',
                        'cost': 'Medium'
                    }
                ])
                
                # Audit recommendations
                suggestions_and_recommendations['audit_recommendations'].extend([
                    {
                        'audit_type': 'Transaction Testing',
                        'description': 'Perform detailed testing of duplicate transactions',
                        'scope': f'Test {min(50, len(duplicate_list))} duplicate transactions',
                        'timeline': '2-3 weeks'
                    },
                    {
                        'audit_type': 'User Access Review',
                        'description': 'Review user access and posting permissions',
                        'scope': 'All users with duplicate transactions',
                        'timeline': '1-2 weeks'
                    },
                    {
                        'audit_type': 'System Controls Testing',
                        'description': 'Test effectiveness of duplicate detection controls',
                        'scope': 'All GL accounts with duplicates',
                        'timeline': '2-3 weeks'
                    }
                ])
                
                # Process improvements
                suggestions_and_recommendations['process_improvements'].extend([
                    {
                        'process': 'Transaction Posting Process',
                        'improvement': 'Add duplicate check step before posting',
                        'benefit': 'Prevents duplicates at source',
                        'effort': 'Medium'
                    },
                    {
                        'process': 'Approval Process',
                        'improvement': 'Implement mandatory approval for high-value transactions',
                        'benefit': 'Reduces risk of unauthorized transactions',
                        'effort': 'High'
                    },
                    {
                        'process': 'Reconciliation Process',
                        'improvement': 'Daily reconciliation of posted transactions',
                        'benefit': 'Early detection of discrepancies',
                        'effort': 'Medium'
                    }
                ])
                
                # Technology recommendations
                suggestions_and_recommendations['technology_recommendations'].extend([
                    {
                        'technology': 'AI/ML Duplicate Detection',
                        'description': 'Implement machine learning-based duplicate detection',
                        'benefit': 'Improved accuracy in detecting sophisticated duplicates',
                        'implementation': '6-12 months'
                    },
                    {
                        'technology': 'Blockchain for Transaction Integrity',
                        'description': 'Consider blockchain for critical transaction integrity',
                        'benefit': 'Immutable transaction records',
                        'implementation': '12-18 months'
                    },
                    {
                        'technology': 'Advanced Analytics Dashboard',
                        'description': 'Implement real-time analytics dashboard',
                        'benefit': 'Proactive monitoring and alerting',
                        'implementation': '3-4 months'
                    }
                ])
                
                # Training recommendations
                suggestions_and_recommendations['training_recommendations'].extend([
                    {
                        'training_area': 'Duplicate Prevention',
                        'description': 'Train users on how to avoid creating duplicate transactions',
                        'audience': 'All posting users',
                        'frequency': 'Quarterly'
                    },
                    {
                        'training_area': 'Fraud Awareness',
                        'description': 'Provide fraud awareness training',
                        'audience': 'All finance staff',
                        'frequency': 'Annually'
                    },
                    {
                        'training_area': 'System Procedures',
                        'description': 'Train users on proper system procedures',
                        'audience': 'New users and refresher for existing users',
                        'frequency': 'As needed'
                    }
                ])
                
                # Compliance recommendations
                suggestions_and_recommendations['compliance_recommendations'].extend([
                    {
                        'compliance_area': 'SOX Controls',
                        'description': 'Strengthen SOX controls around transaction posting',
                        'action': 'Document and test controls',
                        'timeline': 'Ongoing'
                    },
                    {
                        'compliance_area': 'Internal Audit',
                        'description': 'Increase internal audit coverage of transaction processing',
                        'action': 'Quarterly audit reviews',
                        'timeline': 'Quarterly'
                    },
                    {
                        'compliance_area': 'Regulatory Reporting',
                        'description': 'Ensure proper reporting of duplicate transactions',
                        'action': 'Monthly reporting to management',
                        'timeline': 'Monthly'
                    }
                ])
            
            # Add general recommendations if no duplicates found
            if not duplicate_list:
                suggestions_and_recommendations['monitoring_suggestions'].extend([
                    {
                        'monitoring_area': 'Preventive Monitoring',
                        'description': 'Continue monitoring for potential duplicates',
                        'frequency': 'Daily',
                        'thresholds': 'Standard duplicate detection rules'
                    }
                ])
                
                suggestions_and_recommendations['control_improvements'].extend([
                    {
                        'control': 'Maintain Current Controls',
                        'description': 'Current controls appear effective, maintain and monitor',
                        'benefit': 'Continued prevention of duplicates',
                        'implementation': 'Ongoing'
                    }
                ])
            
            return suggestions_and_recommendations
            
        except Exception as e:
            logger.error(f"Error generating suggestions and recommendations: {e}")
            return {
                'error': f'Error generating suggestions and recommendations: {str(e)}',
                'immediate_actions': [],
                'investigation_priorities': [],
                'control_improvements': [],
                'monitoring_suggestions': [],
                'risk_mitigation': [],
                'audit_recommendations': [],
                'process_improvements': [],
                'technology_recommendations': [],
                'training_recommendations': [],
                'compliance_recommendations': []
            }


class ProcessingResultsAPIView(generics.GenericAPIView):
    """API view to retrieve processing results from database"""
    
    def get(self, request, *args, **kwargs):
        """Get processing results for a specific file or job"""
        try:
            file_id = request.GET.get('file_id')
            job_id = request.GET.get('job_id')
            
            if not file_id and not job_id:
                return Response({
                    'error': 'Either file_id or job_id must be provided'
                }, status=400)
            
            if file_id:
                # Get results by file ID
                try:
                    data_file = DataFile.objects.get(id=file_id)
                    processing_summary = get_file_processing_summary(data_file)
                    
                    if 'error' in processing_summary:
                        return Response(processing_summary, status=404)
                    
                    return Response({
                        'success': True,
                        'processing_summary': processing_summary
                    })
                    
                except DataFile.DoesNotExist:
                    return Response({
                        'error': f'DataFile with ID {file_id} not found'
                    }, status=404)
            
            elif job_id:
                # Get results by job ID
                try:
                    processing_job = FileProcessingJob.objects.get(id=job_id)
                    from .analytics_db_saver import AnalyticsDBSaver
                    db_saver = AnalyticsDBSaver(processing_job)
                    processing_summary = db_saver.get_processing_summary()
                    
                    return Response({
                        'success': True,
                        'processing_summary': processing_summary
                    })
                    
                except FileProcessingJob.DoesNotExist:
                    return Response({
                        'error': f'FileProcessingJob with ID {job_id} not found'
                    }, status=404)
                    
        except Exception as e:
            logger.error(f"Error retrieving processing results: {e}")
            return Response({
                'error': f'Error retrieving processing results: {str(e)}'
            }, status=500)


class AnalyticsResultsAPIView(generics.GenericAPIView):
    """API view to retrieve specific analytics results from database"""
    
    def get(self, request, *args, **kwargs):
        """Get specific analytics results"""
        try:
            file_id = request.GET.get('file_id')
            analytics_type = request.GET.get('analytics_type', 'all')
            
            if not file_id:
                return Response({
                    'error': 'file_id must be provided'
                }, status=400)
            
            try:
                data_file = DataFile.objects.get(id=file_id)
            except DataFile.DoesNotExist:
                return Response({
                    'error': f'DataFile with ID {file_id} not found'
                }, status=404)
            
            # Get analytics results
            analytics_results = AnalyticsProcessingResult.objects.filter(
                data_file=data_file
            )
            
            if analytics_type != 'all':
                analytics_results = analytics_results.filter(analytics_type=analytics_type)
            
            results = []
            for result in analytics_results.order_by('-created_at'):
                results.append({
                    'id': str(result.id),
                    'analytics_type': result.analytics_type,
                    'processing_status': result.processing_status,
                    'summary': result.get_summary(),
                    'detailed_results': {
                        'trial_balance_data': result.trial_balance_data,
                        'expense_breakdown': result.expense_breakdown,
                        'user_patterns': result.user_patterns,
                        'account_patterns': result.account_patterns,
                        'temporal_patterns': result.temporal_patterns,
                        'risk_assessment': result.risk_assessment,
                        'chart_data': result.chart_data,
                        'export_data': result.export_data,
                    },
                    'created_at': result.created_at.isoformat(),
                    'processed_at': result.processed_at.isoformat() if result.processed_at else None,
                })
            
            return Response({
                'success': True,
                'file_name': data_file.file_name,
                'file_id': str(data_file.id),
                'analytics_type': analytics_type,
                'results_count': len(results),
                'results': results
            })
            
        except Exception as e:
            logger.error(f"Error retrieving analytics results: {e}")
            return Response({
                'error': f'Error retrieving analytics results: {str(e)}'
            }, status=500)


class MLProcessingResultsAPIView(generics.GenericAPIView):
    """API view to retrieve ML processing results from database"""
    
    def get(self, request, *args, **kwargs):
        """Get ML processing results"""
        try:
            file_id = request.GET.get('file_id')
            model_type = request.GET.get('model_type', 'all')
            
            if not file_id:
                return Response({
                    'error': 'file_id must be provided'
                }, status=400)
            
            try:
                data_file = DataFile.objects.get(id=file_id)
            except DataFile.DoesNotExist:
                return Response({
                    'error': f'DataFile with ID {file_id} not found'
                }, status=404)
            
            # Get ML processing results
            ml_results = MLModelProcessingResult.objects.filter(
                data_file=data_file
            )
            
            if model_type != 'all':
                ml_results = ml_results.filter(model_type=model_type)
            
            results = []
            for result in ml_results.order_by('-created_at'):
                results.append({
                    'id': str(result.id),
                    'model_type': result.model_type,
                    'processing_status': result.processing_status,
                    'summary': result.get_summary(),
                    'detailed_results': {
                        'detailed_results': result.detailed_results,
                        'model_metrics': result.model_metrics,
                        'feature_importance': result.feature_importance,
                    },
                    'created_at': result.created_at.isoformat(),
                    'processed_at': result.processed_at.isoformat() if result.processed_at else None,
                })
            
            return Response({
                'success': True,
                'file_name': data_file.file_name,
                'file_id': str(data_file.id),
                'model_type': model_type,
                'results_count': len(results),
                'results': results
            })
            
        except Exception as e:
            logger.error(f"Error retrieving ML processing results: {e}")
            return Response({
                'error': f'Error retrieving ML processing results: {str(e)}'
            }, status=500)


class ProcessingProgressAPIView(generics.GenericAPIView):
    """API view to get real-time processing progress"""
    
    def get(self, request, *args, **kwargs):
        """Get processing progress for a job"""
        try:
            job_id = request.GET.get('job_id')
            
            if not job_id:
                return Response({
                    'error': 'job_id must be provided'
                }, status=400)
            
            try:
                processing_job = FileProcessingJob.objects.get(id=job_id)
            except FileProcessingJob.DoesNotExist:
                return Response({
                    'error': f'FileProcessingJob with ID {job_id} not found'
                }, status=404)
            
            # Get job tracker
            try:
                job_tracker = ProcessingJobTracker.objects.get(processing_job=processing_job)
                progress_summary = job_tracker.get_progress_summary()
            except ProcessingJobTracker.DoesNotExist:
                # Return basic job status if no tracker exists
                progress_summary = {
                    'job_id': str(processing_job.id),
                    'file_name': processing_job.data_file.file_name,
                    'overall_progress': 0.0,
                    'current_step': 'Initializing',
                    'status_breakdown': {
                        'file_processing': processing_job.status,
                        'analytics': 'PENDING',
                        'ml_processing': 'PENDING',
                        'anomaly_detection': 'PENDING',
                    }
                }
            
            return Response({
                'success': True,
                'progress': progress_summary,
                'job_status': processing_job.status,
                'created_at': processing_job.created_at.isoformat(),
                'started_at': processing_job.started_at.isoformat() if processing_job.started_at else None,
                'completed_at': processing_job.completed_at.isoformat() if processing_job.completed_at else None,
            })
            
        except Exception as e:
            logger.error(f"Error retrieving processing progress: {e}")
            return Response({
                'error': f'Error retrieving processing progress: {str(e)}'
            }, status=500)


class DatabaseStoredComprehensiveAnalyticsView(generics.GenericAPIView):
    """Database-stored comprehensive analytics that returns the same pattern as existing endpoints"""
    
    def get(self, request, file_id, *args, **kwargs):
        """Get comprehensive analytics from database-stored results"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            import uuid
            
            # Clean file_id (remove trailing slashes) - ensure it's a string
            file_id = str(file_id).rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get the data file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Check if we have database-stored analytics results
            analytics_results = AnalyticsProcessingResult.objects.filter(
                data_file=data_file
            ).order_by('-created_at')
            
            if not analytics_results.exists():
                return Response(
                    {'error': 'No database-stored analytics found for this file. Please run processing first.'}, 
                    status=status.HTTP_404_NOT_FOUND
                )
            
            # Get the latest comprehensive analytics result
            comprehensive_result = analytics_results.filter(
                analytics_type='comprehensive_expense'
            ).first()
            
            if not comprehensive_result:
                return Response(
                    {'error': 'No comprehensive analytics found for this file. Please run comprehensive analytics processing first.'}, 
                    status=status.HTTP_404_NOT_FOUND
                )
            
            # Get risk data from ML processing results
            risk_data = self._get_risk_data_from_db(data_file)
            
            # Prepare analytics data in the same pattern as existing endpoint
            analytics_data = {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'status': data_file.status,
                    'total_records': data_file.total_records,
                    'processed_records': data_file.processed_records,
                    'failed_records': data_file.failed_records,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None
                },
                'general_stats': self._get_general_stats_from_db(comprehensive_result),
                'charts': self._get_charts_from_db(comprehensive_result),
                'summary': self._get_summary_from_db(comprehensive_result),
                'risk_data': risk_data,
                'processing_info': {
                    'analytics_id': str(comprehensive_result.id),
                    'processing_status': comprehensive_result.processing_status,
                    'processing_duration': comprehensive_result.processing_duration,
                    'created_at': comprehensive_result.created_at.isoformat(),
                    'processed_at': comprehensive_result.processed_at.isoformat() if comprehensive_result.processed_at else None,
                    'data_source': 'database'
                }
            }
            
            return Response(analytics_data)
            
        except Exception as e:
            logger.error(f"Error in DatabaseStoredComprehensiveAnalyticsView: {e}")
            return Response(
                {'error': f'Error retrieving database-stored analytics: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _get_general_stats_from_db(self, analytics_result):
        """Get general statistics from database-stored analytics"""
        try:
            return {
                'total_transactions': analytics_result.total_transactions,
                'total_amount': float(analytics_result.total_amount),
                'unique_users': analytics_result.unique_users,
                'unique_accounts': analytics_result.unique_accounts,
                'flagged_transactions': analytics_result.flagged_transactions,
                'high_risk_transactions': analytics_result.high_risk_transactions,
                'anomalies_found': analytics_result.anomalies_found,
                'duplicates_found': analytics_result.duplicates_found,
                'average_amount': float(analytics_result.total_amount) / analytics_result.total_transactions if analytics_result.total_transactions > 0 else 0,
                'data_source': 'database'
            }
        except Exception as e:
            logger.error(f"Error getting general stats from DB: {e}")
            return {}
    
    def _get_charts_from_db(self, analytics_result):
        """Get charts data from database-stored analytics"""
        try:
            charts = {}
            
            # Get expense breakdown data
            if analytics_result.expense_breakdown:
                charts['expense_breakdown'] = analytics_result.expense_breakdown
            
            # Get user patterns data
            if analytics_result.user_patterns:
                charts['user_patterns'] = analytics_result.user_patterns
            
            # Get account patterns data
            if analytics_result.account_patterns:
                charts['account_patterns'] = analytics_result.account_patterns
            
            # Get temporal patterns data
            if analytics_result.temporal_patterns:
                charts['temporal_patterns'] = analytics_result.temporal_patterns
            
            # Get chart data
            if analytics_result.chart_data:
                charts.update(analytics_result.chart_data)
            
            charts['data_source'] = 'database'
            return charts
            
        except Exception as e:
            logger.error(f"Error getting charts from DB: {e}")
            return {'data_source': 'database'}
    
    def _get_summary_from_db(self, analytics_result):
        """Get summary data from database-stored analytics"""
        try:
            summary = {
                'total_transactions': analytics_result.total_transactions,
                'total_amount': float(analytics_result.total_amount),
                'unique_users': analytics_result.unique_users,
                'unique_accounts': analytics_result.unique_accounts,
                'flagged_transactions': analytics_result.flagged_transactions,
                'high_risk_transactions': analytics_result.high_risk_transactions,
                'anomalies_found': analytics_result.anomalies_found,
                'duplicates_found': analytics_result.duplicates_found,
                'data_source': 'database'
            }
            
            # Add risk assessment if available
            if analytics_result.risk_assessment:
                summary['risk_assessment'] = analytics_result.risk_assessment
            
            return summary
            
        except Exception as e:
            logger.error(f"Error getting summary from DB: {e}")
            return {'data_source': 'database'}
    
    def _get_risk_data_from_db(self, data_file):
        """Get risk data from database-stored ML processing results"""
        try:
            # Get ML processing results for this file
            ml_results = MLModelProcessingResult.objects.filter(
                data_file=data_file,
                processing_status='COMPLETED'
            ).order_by('-created_at').first()
            
            if not ml_results:
                return {
                    'risk_stats': {},
                    'risk_charts': {},
                    'data_source': 'database',
                    'message': 'No ML processing results found'
                }
            
            # Extract risk data from ML results
            risk_data = {
                'risk_stats': {
                    'anomalies_detected': ml_results.anomalies_detected,
                    'duplicates_found': ml_results.duplicates_found,
                    'risk_score': ml_results.risk_score,
                    'confidence_score': ml_results.confidence_score,
                    'model_type': ml_results.model_type
                },
                'risk_charts': ml_results.detailed_results.get('risk_charts', {}),
                'data_source': 'database',
                'ml_processing_id': str(ml_results.id)
            }
            
            return risk_data
            
        except Exception as e:
            logger.error(f"Error getting risk data from DB: {e}")
            return {
                'risk_stats': {},
                'risk_charts': {},
                'data_source': 'database',
                'error': str(e)
            }


class DatabaseStoredDuplicateAnalysisView(generics.GenericAPIView):
    """Database-stored duplicate analysis that returns the same pattern as existing endpoint"""
    
    def get(self, request, file_id, *args, **kwargs):
        """Get duplicate analysis from database-stored results"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            import uuid
            
            # Clean file_id - ensure it's a string
            file_id = str(file_id).rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get the data file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Check if we have database-stored duplicate analysis results
            duplicate_results = AnalyticsProcessingResult.objects.filter(
                data_file=data_file,
                analytics_type='duplicate_analysis'
            ).order_by('-created_at').first()
            
            if not duplicate_results:
                return Response(
                    {'error': 'No database-stored duplicate analysis found for this file. Please run duplicate analysis processing first.'}, 
                    status=status.HTTP_404_NOT_FOUND
                )
            
            # Generate response with existing analysis data
            analysis_response = self._generate_duplicate_analysis_response(data_file, duplicate_results)
            
            return Response(analysis_response)
            
        except Exception as e:
            logger.error(f"Error in DatabaseStoredDuplicateAnalysisView: {e}")
            return Response(
                {'error': f'Error retrieving database-stored duplicate analysis: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_duplicate_analysis_response(self, data_file, duplicate_result):
        """Generate duplicate analysis response from database-stored results"""
        try:
            # Extract data from the database result
            trial_balance_data = duplicate_result.trial_balance_data
            
            # Get analysis info
            analysis_info = trial_balance_data.get('analysis_info', {})
            duplicate_list = trial_balance_data.get('duplicate_list', [])
            breakdowns = trial_balance_data.get('breakdowns', {})
            chart_data = trial_balance_data.get('chart_data', {})
            summary_table = trial_balance_data.get('summary_table', [])
            export_data = trial_balance_data.get('export_data', [])
            detailed_insights = trial_balance_data.get('detailed_insights', {})
            ml_enhancement = trial_balance_data.get('ml_enhancement', {})
            
            # Generate response in the same pattern as existing endpoint
            response = {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'client_name': data_file.client_name,
                    'company_name': data_file.company_name,
                    'fiscal_year': data_file.fiscal_year,
                    'status': data_file.status,
                    'total_records': data_file.total_records,
                    'processed_records': data_file.processed_records,
                    'failed_records': data_file.failed_records,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None
                },
                'analysis_info': analysis_info,
                'duplicate_list': duplicate_list,
                'breakdowns': breakdowns,
                'chart_data': chart_data,
                'summary_table': summary_table,
                'export_data': export_data,
                'detailed_insights': detailed_insights,
                'ml_enhancement': ml_enhancement,
                'processing_info': {
                    'analytics_id': str(duplicate_result.id),
                    'processing_status': duplicate_result.processing_status,
                    'processing_duration': duplicate_result.processing_duration,
                    'created_at': duplicate_result.created_at.isoformat(),
                    'processed_at': duplicate_result.processed_at.isoformat() if duplicate_result.processed_at else None,
                    'data_source': 'database'
                }
            }
            
            return response
            
        except Exception as e:
            logger.error(f"Error generating duplicate analysis response from DB: {e}")
            return {
                'error': f'Error generating response: {str(e)}',
                'data_source': 'database'
            }


class AnalyticsDatabaseCheckView(generics.GenericAPIView):
    """Check if analysis is being saved to DB against file_id"""
    
    def get(self, request, file_id, *args, **kwargs):
        """Check database storage status for a specific file"""
        try:
            from rest_framework import status
            from django.shortcuts import get_object_or_404
            import uuid
            
            # Clean file_id - ensure it's a string
            file_id = str(file_id).rstrip('/')
            
            # Validate UUID format
            try:
                uuid.UUID(file_id)
            except ValueError:
                return Response(
                    {'error': 'Invalid file ID format'}, 
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Get the data file
            data_file = get_object_or_404(DataFile, id=file_id)
            
            # Check analytics processing results
            analytics_results = AnalyticsProcessingResult.objects.filter(
                data_file=data_file
            ).order_by('-created_at')
            
            # Check ML processing results
            ml_results = MLModelProcessingResult.objects.filter(
                data_file=data_file
            ).order_by('-created_at')
            
            # Check processing job tracker
            job_trackers = ProcessingJobTracker.objects.filter(
                data_file=data_file
            ).order_by('-created_at')
            
            # Check file processing jobs
            processing_jobs = FileProcessingJob.objects.filter(
                data_file=data_file
            ).order_by('-created_at')
            
            # Prepare comprehensive check response
            check_response = {
                'file_info': {
                    'id': str(data_file.id),
                    'file_name': data_file.file_name,
                    'status': data_file.status,
                    'uploaded_at': data_file.uploaded_at.isoformat() if data_file.uploaded_at else None,
                    'processed_at': data_file.processed_at.isoformat() if data_file.processed_at else None
                },
                'database_storage_status': {
                    'analytics_results_count': analytics_results.count(),
                    'ml_results_count': ml_results.count(),
                    'job_trackers_count': job_trackers.count(),
                    'processing_jobs_count': processing_jobs.count(),
                    'has_database_storage': analytics_results.exists() or ml_results.exists(),
                    'is_fully_stored': analytics_results.exists() and ml_results.exists()
                },
                'analytics_results': [
                    {
                        'id': str(result.id),
                        'analytics_type': result.analytics_type,
                        'processing_status': result.processing_status,
                        'total_transactions': result.total_transactions,
                        'created_at': result.created_at.isoformat(),
                        'processed_at': result.processed_at.isoformat() if result.processed_at else None
                    }
                    for result in analytics_results[:10]  # Limit to 10 most recent
                ],
                'ml_results': [
                    {
                        'id': str(result.id),
                        'model_type': result.model_type,
                        'processing_status': result.processing_status,
                        'anomalies_detected': result.anomalies_detected,
                        'duplicates_found': result.duplicates_found,
                        'created_at': result.created_at.isoformat(),
                        'processed_at': result.processed_at.isoformat() if result.processed_at else None
                    }
                    for result in ml_results[:10]  # Limit to 10 most recent
                ],
                'job_trackers': [
                    {
                        'id': str(tracker.id),
                        'overall_progress': tracker.overall_progress,
                        'current_step': tracker.current_step,
                        'completed_steps': tracker.completed_steps,
                        'total_steps': tracker.total_steps,
                        'created_at': tracker.created_at.isoformat(),
                        'completed_at': tracker.completed_at.isoformat() if tracker.completed_at else None
                    }
                    for tracker in job_trackers[:5]  # Limit to 5 most recent
                ],
                'processing_jobs': [
                    {
                        'id': str(job.id),
                        'status': job.status,
                        'run_anomalies': job.run_anomalies,
                        'requested_anomalies': job.requested_anomalies,
                        'created_at': job.created_at.isoformat(),
                        'completed_at': job.completed_at.isoformat() if job.completed_at else None,
                        'processing_duration': job.processing_duration
                    }
                    for job in processing_jobs[:5]  # Limit to 5 most recent
                ],
                'recommendations': self._generate_recommendations(
                    analytics_results, ml_results, job_trackers, processing_jobs
                )
            }
            
            return Response(check_response)
            
        except Exception as e:
            logger.error(f"Error in AnalyticsDatabaseCheckView: {e}")
            return Response(
                {'error': f'Error checking database storage: {str(e)}'}, 
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    def _generate_recommendations(self, analytics_results, ml_results, job_trackers, processing_jobs):
        """Generate recommendations based on database storage status"""
        recommendations = []
        
        if not analytics_results.exists():
            recommendations.append({
                'type': 'warning',
                'message': 'No analytics results found in database. Run analytics processing to store results.',
                'action': 'Run analytics processing'
            })
        
        if not ml_results.exists():
            recommendations.append({
                'type': 'warning',
                'message': 'No ML processing results found in database. Run ML processing to store results.',
                'action': 'Run ML processing'
            })
        
        if not job_trackers.exists():
            recommendations.append({
                'type': 'info',
                'message': 'No job trackers found. This indicates processing may not have used the new database storage system.',
                'action': 'Use new processing system'
            })
        
        if processing_jobs.exists():
            latest_job = processing_jobs.first()
            if latest_job.status == 'PENDING':
                recommendations.append({
                    'type': 'info',
                    'message': 'Latest processing job is pending. Wait for completion or restart processing.',
                    'action': 'Wait or restart processing'
                })
            elif latest_job.status == 'FAILED':
                recommendations.append({
                    'type': 'error',
                    'message': 'Latest processing job failed. Check error logs and restart processing.',
                    'action': 'Check logs and restart'
                })
        
        if not recommendations:
            recommendations.append({
                'type': 'success',
                'message': 'Database storage is working correctly. All results are being saved to database.',
                'action': 'None required'
            })
        
        return recommendations
