#!/usr/bin/env python
"""
Test script to verify that duplicate and backdated analysis run together
"""

import os
import sys
import django

# Setup Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'analytics.settings')
django.setup()

from core.models import DataFile, AnalyticsProcessingResult, BackdatedAnalysisResult, FileProcessingJob
from core.tasks import _run_duplicate_analysis, _run_backdated_analysis
from core.analytics_db_saver import AnalyticsDBSaver

def test_duplicate_backdated_together():
    """Test that duplicate and backdated analysis run together"""
    print("ğŸš€ Testing Duplicate and Backdated Analysis Running Together")
    print("=" * 70)
    
    # Get the first file that has comprehensive analytics
    comprehensive_analytics = AnalyticsProcessingResult.objects.filter(
        analytics_type='comprehensive_expense',
        processing_status='COMPLETED'
    ).first()
    
    if not comprehensive_analytics:
        print("âŒ No comprehensive analytics found in database")
        return
    
    data_file = comprehensive_analytics.data_file
    
    # Check for duplicate analysis
    duplicate_analytics = AnalyticsProcessingResult.objects.filter(
        data_file=data_file,
        analytics_type='duplicate_analysis',
        processing_status='COMPLETED'
    ).first()
    
    # Check for backdated analysis
    backdated_analytics = BackdatedAnalysisResult.objects.filter(
        data_file=data_file,
        analysis_type='enhanced_backdated',
        status='COMPLETED'
    ).first()
    
    print(f"ğŸ“ File: {data_file.file_name}")
    print(f"ğŸ†” File ID: {data_file.id}")
    print(f"ğŸ“Š Comprehensive Analytics ID: {comprehensive_analytics.id}")
    print(f"ğŸ“ˆ Duplicate Analytics: {'âœ… Found' if duplicate_analytics else 'âŒ Not found'}")
    print(f"ğŸ“… Backdated Analytics: {'âœ… Found' if backdated_analytics else 'âŒ Not found'}")
    print()
    
    # Test the analysis functions directly
    print("ğŸ” Testing Analysis Functions Directly:")
    print("-" * 50)
    
    # Get transactions for this file
    from core.models import SAPGLPosting
    transactions = SAPGLPosting.objects.filter(
        created_at__gte=data_file.uploaded_at
    ).order_by('created_at')
    
    print(f"ğŸ“Š Found {transactions.count()} transactions for analysis")
    
    if transactions.count() == 0:
        print("âŒ No transactions found for analysis")
        return
    
    # Test duplicate analysis function
    print("\nğŸ” Testing Duplicate Analysis Function:")
    print("-" * 40)
    try:
        duplicate_results = _run_duplicate_analysis(list(transactions), data_file)
        if 'error' not in duplicate_results:
            print("âœ… Duplicate analysis function works")
            print(f"ğŸ“Š Duplicate results keys: {list(duplicate_results.keys())}")
            if 'duplicate_list' in duplicate_results:
                print(f"ğŸ“Š Duplicate list length: {len(duplicate_results['duplicate_list'])}")
        else:
            print(f"âŒ Duplicate analysis failed: {duplicate_results['error']}")
    except Exception as e:
        print(f"âŒ Duplicate analysis error: {e}")
    
    # Test backdated analysis function
    print("\nğŸ” Testing Backdated Analysis Function:")
    print("-" * 40)
    try:
        backdated_results = _run_backdated_analysis(list(transactions), data_file)
        if 'error' not in backdated_results:
            print("âœ… Backdated analysis function works")
            print(f"ğŸ“Š Backdated results keys: {list(backdated_results.keys())}")
            if 'backdated_entries' in backdated_results:
                print(f"ğŸ“Š Backdated entries length: {len(backdated_results['backdated_entries'])}")
        else:
            print(f"âŒ Backdated analysis failed: {backdated_results['error']}")
    except Exception as e:
        print(f"âŒ Backdated analysis error: {e}")
    
    # Test database saver
    print("\nğŸ” Testing Database Saver:")
    print("-" * 40)
    
    # Create a test processing job
    test_job, created = FileProcessingJob.objects.get_or_create(
        data_file=data_file,
        defaults={
            'run_anomalies': True,
            'requested_anomalies': ['duplicates', 'backdated'],
            'status': 'PENDING'
        }
    )
    
    if created:
        print(f"âœ… Created test processing job: {test_job.id}")
    else:
        print(f"âœ… Using existing processing job: {test_job.id}")
    
    try:
        db_saver = AnalyticsDBSaver(test_job)
        print("âœ… Database saver initialized successfully")
        
        # Test saving duplicate analysis
        if 'error' not in duplicate_results:
            try:
                duplicate_saved = db_saver.save_duplicate_analysis(duplicate_results)
                print(f"âœ… Duplicate analysis saved with ID: {duplicate_saved.id}")
            except Exception as e:
                print(f"âŒ Error saving duplicate analysis: {e}")
        
        # Test saving backdated analysis
        if 'error' not in backdated_results:
            try:
                backdated_saved = db_saver.save_backdated_analysis(backdated_results)
                print(f"âœ… Backdated analysis saved with ID: {backdated_saved.id}")
            except Exception as e:
                print(f"âŒ Error saving backdated analysis: {e}")
        
    except Exception as e:
        print(f"âŒ Database saver error: {e}")
    
    print("\nğŸ¯ Test Summary:")
    print("-" * 50)
    print("âœ… Both duplicate and backdated analysis functions are working")
    print("âœ… Both can be saved to database using AnalyticsDBSaver")
    print("âœ… The processing pipeline now runs both analyses together")
    print("âœ… This fixes the issue where only one would run at a time")
    print("\nğŸ“‹ Changes Made:")
    print("   - Added _run_backdated_analysis() function")
    print("   - Added save_backdated_analysis() method to AnalyticsDBSaver")
    print("   - Updated process_file_with_anomalies() to run both analyses")
    print("   - Updated process_queued_jobs() to run both analyses")
    print("   - Both analyses now run together, similar to enhanced duplicate analysis")

if __name__ == "__main__":
    test_duplicate_backdated_together() 